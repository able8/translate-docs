# Reverse Proxy, HTTP Keep-Alive Timeout, and sporadic HTTP 502s

https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s

## How HTTP Keep-Alive can cause TCP race condition

September 30, 2021

These mysterious HTTP 502s happened to me already twice over the past few years. Since the amount of service-to-service communications every year goes only up, I expect more and more people to experience the same issue. So, sharing it here.

**TL;DR:** HTTP Keep-Alive between a reverse proxy and an upstream server combined with some misfortunate downstream- and upstream-side timeout settings can make clients receiving HTTP 502s from the proxy.

[![How HTTP Keep-Alive between a reverse proxy and an upstream can cause HTTP 502s due to a TCP race condition.](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond-2000-opt.png)](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond.png)

Below is a longer version...

[HTTP 502](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502) status code, also know as `Bad Gateway` indicates that a server, while acting as a gateway or proxy, received an invalid response from the upstream server. Typical problems causing it ( [summarized by Datadog folks](https://www.datadoghq.com/blog/nginx-502-bad-gateway-errors-gunicorn/)):

- Upstream server isn't running
- Upstream server times out
- Reverse proxy is misconfigured (e.g., my favourite one - trying to call uWSGI over`HTTP` while it listens on `uwsgi` protocol ðŸ™ˆ)

However, sometimes there seems to be no apparent reason for HTTP 502s responses while clients sporadically see them:

- [NodeJs application behind Amazon ELB throws 502](https://stackoverflow.com/questions/45626787/nodejs-application-behind-amazon-elb-throws-502)
- [Proxy persistent upstream: occasional 502](https://github.com/h2o/h2o/issues/281)
- [Sporadic 502 response only when running through traefik](https://github.com/traefik/traefik/issues/3237)
- [HTTP 502 response generated by a proxy after it tries to send data upstream to a partially closed connection](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a)

For me personally, it happened twice already - the first time, with a Node.js application running behind AWS ALB; and the second time, with a Python (uWSGI) application running behind Traefik reverse proxy. And it took quite some time for my team to debug it back in the day (we even involved premium AWS support to pin down the root cause).

Long story short, it was a **TCP race condition**.

A detailed explanation of the problem, including some low-level TCP packet analysis, can be found in [this lovely article](https://www.tessian.com/blog/how-to-fix-http-502-errors/). And here, I'll just try to give a quick summary.

First off, any TCP connection is a distributed system. Well, it should make sense - the actual state of a connection is distributed between its [remote] endpoints. And as the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) dictates, any distributed system can be either available 100% of the time or consistent 100% of the time, but not both. TCP obviously chooses the availability. Hence, the **state of a TCP connection is only eventually consistent**!

Why does it matter for us?

While this [serverfault](https://serverfault.com/a/426741/292251) answer says that the HTTP Keep-Alive should be used only for the client-to-proxy communications, from my experience, proxies often keep the upstream connections alive too. This is a so-called _connection pool pattern_ when just a few connections are heavily reused to handle numerous requests.

When a connection stays in the pool for too long without being reused, it's marked as _idle_. Normally, idle connections are closed after some period of inactivity. However, the server-side (i.e., the upstream) also can drop idle connections. So, what sometimes happens is that the proxy and the upstream have some misfortunate idle connection timeout settings. More specifically, if the proxy and the upstream have exactly the same idle timeout durations or the upstream drops connection sooner than the proxy.

While one endpoint (the proxy) can still be thinking that the connection is totally fine, the other endpoint may have already started closing the connection (the upstream).

So, the proxy starts writing a request into a connection that is being closed by the upstream. And instead of getting the TCP `ACK` for the sent bytes, the proxy gets TCP `FIN` (or even `RST`) from the upstream. Oftentimes, the proxy just gives up on such requests and responds with immediate HTTP 502 to the client. **And this is completely invisible on the application side!** So, no application logs will be helpful in such a case.

> HTTP Keep-Alive can cause TCP race conditions ðŸ¤¯
>
> Ok, it happened again... Hence, the diagram!
>
> In a (reverse\_proxy + app\_server) setup, the reverse proxy should be dropping connections more frequently than the app server. Otherwise, clients will see sporadic HTTP 502s.
>
> Why? ðŸ”½ [pic.twitter.com/YBb2tP3Zfg](https://t.co/YBb2tP3Zfg)
>
> â€” Ivan Velichko (@iximiuz) [September 29, 2021](https://twitter.com/iximiuz/status/1443305889616863238?ref_src=twsrc%5Etfw)

**The problem is actually quite generic**. Theoretically, it can happen in any point-to-point TCP communication where the connections are reused if the upstream drops connections faster than the downstream. However, most of the time, the problem manifests itself with a reverse proxy talking to an upstream with HTTP Keep-Alive being on.

How to mitigate:

- Make sure the upstream (application) idle connection timeout is longer than the downstream (proxy) connection timeout
- Disable connection reuse between the upstream and the proxy (turn off HTTP Keep-Alive)
- Employ HTTP 5xx retries on the client-side (well, they are a must-have anyway).

---



Name   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)  

5XX are specifically called out as not retriable  i would not give this  advice because some impressionable soul out there might think it's a  good idea.

-  [Ivan Velichko](https://disqus.com/by/iximiuz/) Mod [ Name](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5560305191)  

  Hi! Could you please elaborate why do you think that 5xx aren't retriable? My reasoning is

  5xx - something failed on the server-side
  4xx - client did something wrong

  So, 4xx shouldn't be retried w/o modifying the request. But retrying 5xx  can (and often does) resolve the issue. Of course, it needs to be done  carefully if requests aren't idempotent. And 500 is a bit of a special  code here because it could indicate that a server simply failed to  prepare an otherwise-4xx response. However, 502, 503, and 504 are  generally retriable. Here is an [example of a (widely-used) retry strategy](https://disq.us/url?url=https%3A%2F%2Fdocs.aws.amazon.com%2Fapigateway%2Fapi-reference%2Fhandling-errors%2F%3A4PfjDpqD6KjInKtVqIWFEolbPmA&cuid=3735795) that follows the same reasoning.

 [Air Fresh](https://disqus.com/by/air_fresh/)    [6 months ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5940826210)  edited 

Hello Ivan Velichko. It seems that you should switch the place of "Idle timeout of upstream  connection" and "Idle timeout of downstream connection" on image

- Hmm... I gave it fresh look, and the diagram still looks right to me. Care to elaborate?

  The upper diagram of "Idle timeout of upstream connection" is downstream,  so it's easy for the readers to think "Idle timeout of upstream  connection" is referred to Reversed Proxy.

  So, for easier to interpretation, I suggest changing with below sentence on image:

  "Idle timeout of downstream connections" < "Idle timeout of upstream connections"

  

  Oh, I see the point now, but I'm not sure if it's a right thing to do... My reasoning is/was like that: on the "downstream" side (a reverse proxy  on the diagram), there is only one possible knob - the *downstream-to-upstream* (or just *upstream* for short) connection timeout. Hence, the name "upstream connection  timeout." And, symmetrically, on the upstream side there are only  (upstream-to-) downstream connections.

  OK, now I can understand your intention, and I still have 2 questions:

  \1. How can I set idle connection timeout from downstream to upstream and vice versa?
  \2. What are the differences between Nginx's keepalive_timeout and idle connection timeout

  

  - I think it's technology-specific. For instance, in nginx there seems to be two similarly named but distinct knobs:

    \- [`keepalive_timeout` set in the `http`, `server`, or `location` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_timeout%3AOqDTC-r2IaE1MYb8_g-BkgT0cfM&cuid=3735795) means the timeout for the downstream connections.

    \- [` keepalive_timeout` set in the `upstream` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%3AEUWuizQE68MydDc5S6YVsO_vs9k&cuid=3735795) means the timeout during which an idle keepalive connection to an  upstream server will stay open (notice how there are two more knobs - ` keepalive_time` and ` keepalive_requests`).

    I remember how back in 2018 we couldn't find a similar upstream know for AWS ALB. But maybe things have changed since then.

    Since nginx, as a reverse proxy, acts as a client and as a server at the same time, it has two different sets of configs. But on the actual client-  or server- ends, there is usually just one timeout value. Often it's  called *idle timeout*, like in this Go `net/http` case ([Server.IdleTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Server%3A0ZDn9syuWGxHZXkhSruki6UtEqc&cuid=3735795) for the server-side & [Transport.IdleConnTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Transport%3AAazGyQh9twbSYj_YPhznwvqL9qM&cuid=3735795) for the client-side).

    

Thanks for sharing!  
for the 2nd mitigation "Disable connection reuse between the upstream and  the proxy (turn off HTTP Keep-Alive)", I think it is not a good idea, as it is gonna cause load on the upstream server as for each new  connection, a new tcp connection is created which is expensive. so it is important for the proxy server to do connection reuse as much as it  can.



That's a fair concern and a very good point! However, I think option 2 remains valid. Not every service has a high load. And for applications serving  just a few tens requests a minute, recreating connections will unlikely  cause any noticeable overhead. The irony is that it's often such small  to medium services that miss other resilience mechanisms such as retries on the client. So, disabling the connection reuse might be the easiest  and the cheapest way to fix 502s for them. Of course, for apps serving  thousands of requests a second it's a totally different story. But  usually those apps would have a dedicated SRE team behind and all the  needed retries in place :)



If we had sent ack of FIN before before the data we wouldn't face this  issue? (TCP Half close) Upstream closed the connection but proxy still  is using it.



[ ![Thumbnail](https://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg?w=800&h=611) ](http://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg)



Hmm... I'm not sure how reasonable is a situation when the upstream already  closed its end of the connection, but the proxy keeps the other end  open. What kind of information should the proxy be writing to its end of the connection then?

The inverse situation looks legit, though. A proxy could push the request to the connection and then half-close it  before waiting for the upstream's reply. I'm not aware of any proxies  really doing it, but hypothetically such behaviour can be implemented by proxies allowing to configure max number of requests per connection ([example](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_requests%3AzFGxIfAi4lkU1QQWuwDNj5IBmRQ&cuid=3735795)).



Hello Ivan, first of all thank you for writing this post!

I'm trying to solve this very problem at the moment but so far I've been  unable to find the right set of configuration both for Nginx and for  uWSGI. In my case I have AWS ALB -> NGINX -> uWSGI.

In Nginx I'm basically using the reverse proxy configuration with uwsgi socket:
`server {    listen 80;    location / {        include uwsgi_params;        uwsgi_pass unix:///tmp/uwsgi.sock;    }}`

I also noticed that I have **keepalive_timeout  65** set in the **/etc/nginx/nginx.conf**, but I assume it's ok since it's the AWS ALB who is communicating with Nginx.

I think the problem I'm having (intermittent 502 responses) is because of my uWSGI configuration:
`socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664hook-master-start = unix_signal:15 gracefully_kill_them_allneed-app = truedie-on-term = trueshow-config = truemaster = truedie-on-term = truememory-report = falsegevent = 20gevent-monkey-patch = trueso-keepalive = falseharakiri = 80http-timeout = 70socket-timeout = 70# new relic mandatory options# https://docs.newrelic.com/docs/python/python-agent-and-uwsgienable-threads = truesingle-interpreter = true`

Would you mind to share some insights? I'm having a hard time trying to translate your conclusion with the right [uWSGI configuration](https://disq.us/url?url=https%3A%2F%2Fuwsgi-docs.readthedocs.io%2Fen%2Flatest%2FOptions.html%3ALPS9dziFKJTkzZr3PDDD-pszxis&cuid=3735795).

Hi! It's likely that the problem is in between the ALB and nginx since the  uwsgi protocol doesn't have a notion of keepalive connections ([see this for more](https://disq.us/url?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%23keepalive%3AAVRyL044jRXhqj0kV67hZ9CCto8&cuid=3735795)). That 65 seconds setting of nginx is the time when it'll start closing  idle downstream connections (i.e., ALB's connections). You need to make  sure ALB has a shorter keepalive timeout for its upstream connections.  Be careful and don't confuse the timeouts since often proxy and load  balancers (ALB, nginx, etc) have both the upstream and downstream  keepalive settings ([see this answer for more](http://disq.us/p/2qdh9vv)). ALB is a downstream for nginx, nginx is an upstream for ALB,  and end users are a downstream for ALB.



- Thank you Ivan.

  Just wanted to give a feedback in case someone faces this problem: as you  said, in our case the solution was to make sure the ALB's idle timeout  was shorter than the Nginx client timeout (client_header_timeout and,  client_body_timeout) and that the Nginx client timeout was shorter than  uwsgi timeout (upstream's configuration uwsgi_read_timeout and  uwsgi_send_timeout).

  Example:
  **ALB's idle timeout:** 60s
  **Nginx.conf global configuration:** client_header_timeout 80s; client_body_timeout  80s;
  **uWSGI upstream configuration:** uwsgi_read_timeout 100s; uwsgi_send_timeout 100s;

  

