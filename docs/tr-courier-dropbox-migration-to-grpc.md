# Courier: Dropbox migration to gRPC

# Courierï¼šDropbox è¿ç§»åˆ° gRPC

By Ruslan NigmatullinÂ and Alexey Ivanov â€¢ Jan 08, 2019


1. [The road to gRPC](http://dropbox.tech#the-road-to-grpc)
2. [What Courier brings to gRPC](http://dropbox.tech#what-courier-brings-to-grpc)
3. [Performance optimizations](http://dropbox.tech#performance-optimizations)
4. [Implementation details](http://dropbox.tech#implementation-details)
5. [Migration process](http://dropbox.tech#migration-process)
6. [Lessons learned](http://dropbox.tech#lessons-learned)
7. [Future Work](http://dropbox.tech#future-work)
8. [We are hiring!](http://dropbox.tech#we-are-hiring)

---

1. [gRPCä¹‹è·¯](http://dropbox.tech#the-road-to-grpc)
2. [Courier ä¸º gRPC å¸¦æ¥äº†ä»€ä¹ˆ](http://dropbox.tech#what-courier-brings-to-grpc)
3. [æ€§èƒ½ä¼˜åŒ–](http://dropbox.tech#performance-optimizations)
4. [å®ç°ç»†èŠ‚](http://dropbox.tech#implementation-details)
5. [è¿ç§»æµç¨‹](http://dropbox.tech#migration-process)
6. [ç»éªŒæ•™è®­](http://dropbox.tech#lessons-learned)
7. [æœªæ¥å·¥ä½œ](http://dropbox.tech#future-work)
8. [æˆ‘ä»¬æ­£åœ¨æ‹›è˜ï¼](http://dropbox.tech#we-are-hiring)

Dropbox runs hundreds of services, written in different languages, which exchange millions of requests per second. At the core of our Service Oriented Architecture is Courier, our gRPC-based Remote Procedure Call (RPC) framework. While developing Courier, we learned a lot about extending gRPC, optimizing performance for scale, and providing a bridge from our legacy RPC system.

Dropbox è¿è¡Œæ•°ç™¾ä¸ªç”¨ä¸åŒè¯­è¨€ç¼–å†™çš„æœåŠ¡ï¼Œæ¯ç§’äº¤æ¢æ•°ç™¾ä¸‡ä¸ªè¯·æ±‚ã€‚æˆ‘ä»¬é¢å‘æœåŠ¡æ¶æ„çš„æ ¸å¿ƒæ˜¯ Courierï¼Œè¿™æ˜¯æˆ‘ä»¬åŸºäº gRPC çš„è¿œç¨‹è¿‡ç¨‹è°ƒç”¨ (RPC) æ¡†æ¶ã€‚åœ¨å¼€å‘ Courier çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å­¦åˆ°äº†å¾ˆå¤šå…³äºæ‰©å±• gRPCã€ä¼˜åŒ–è§„æ¨¡æ€§èƒ½ä»¥åŠä¸ºæˆ‘ä»¬çš„æ—§ RPC ç³»ç»Ÿæä¾›æ¡¥æ¢çš„çŸ¥è¯†ã€‚

_Note: this post shows code generation examples in Python and Go. We also support Rust and Java._

_æ³¨æ„ï¼šè¿™ç¯‡æ–‡ç« å±•ç¤ºäº† Python å’Œ Go ä¸­çš„ä»£ç ç”Ÿæˆç¤ºä¾‹ã€‚æˆ‘ä»¬ä¹Ÿæ”¯æŒ Rust å’Œ Javaã€‚_

## The road to gRPC

## gRPC ä¹‹è·¯

Courier is not Dropboxâ€™s first RPC framework. Even before we started to break our Python monolith into services in earnest, we needed a solid foundation for inter-service communication. Especially since the choice of the RPC framework has profound reliability implications.

Courier ä¸æ˜¯ Dropbox çš„ç¬¬ä¸€ä¸ª RPC æ¡†æ¶ã€‚ç”šè‡³åœ¨æˆ‘ä»¬å¼€å§‹è®¤çœŸåœ°å°† Python å•ä½“åˆ†è§£ä¸ºæœåŠ¡ä¹‹å‰ï¼Œæˆ‘ä»¬å°±éœ€è¦ä¸ºæœåŠ¡é—´é€šä¿¡æ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚ç‰¹åˆ«æ˜¯å› ä¸º RPC æ¡†æ¶çš„é€‰æ‹©å…·æœ‰æ·±è¿œçš„å¯é æ€§å½±å“ã€‚

Previously, Dropbox experimented with multiple RPC frameworks. At first, we started with a custom protocol for manual serialization and de-serialization. Some services like our [Scribe-based log pipeline](https://blogs.dropbox.com/tech/2015/05/how-to-write-a-better-scribe/) used [Apache Thrift](https://github.com/apache/thrift). But our main RPC framework (legacy RPC) was an HTTP/1.1-based protocol with protobuf-encoded messages.

æ­¤å‰ï¼ŒDropbox è¯•éªŒäº†å¤šä¸ª RPC æ¡†æ¶ã€‚èµ·åˆï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªç”¨äºæ‰‹åŠ¨åºåˆ—åŒ–å’Œååºåˆ—åŒ–çš„è‡ªå®šä¹‰åè®®å¼€å§‹ã€‚ä¸€äº›æœåŠ¡ï¼Œæ¯”å¦‚æˆ‘ä»¬çš„ [åŸºäº Scribe çš„æ—¥å¿—ç®¡é“](https://blogs.dropbox.com/tech/2015/05/how-to-write-a-better-scribe/) ä½¿ç”¨äº† [Apache Thrift](https:/ /github.com/apache/thriftï¼‰ã€‚ä½†æ˜¯æˆ‘ä»¬çš„ä¸»è¦ RPC æ¡†æ¶ï¼ˆæ—§å¼ RPC)æ˜¯ä¸€ä¸ªåŸºäº HTTP/1.1 çš„åè®®ï¼Œå¸¦æœ‰ protobuf ç¼–ç çš„æ¶ˆæ¯ã€‚

For our new framework, there were several choices. We could evolve the legacy RPC framework to incorporate Swagger (now [OpenAPI](https://github.com/OAI/OpenAPI-Specification)). Or we could [create a new standard](https://xkcd.com/927/). We also considered building on top of both Thrift and gRPC.

å¯¹äºæˆ‘ä»¬çš„æ–°æ¡†æ¶ï¼Œæœ‰å¤šç§é€‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥å‘å±•é—ç•™çš„ RPC æ¡†æ¶ä»¥åˆå¹¶ Swaggerï¼ˆç°åœ¨ [OpenAPI](https://github.com/OAI/OpenAPI-Specification))ã€‚æˆ–è€…æˆ‘ä»¬å¯ä»¥[åˆ›å»ºä¸€ä¸ªæ–°æ ‡å‡†](https://xkcd.com/927/)ã€‚æˆ‘ä»¬è¿˜è€ƒè™‘åœ¨ Thrift å’Œ gRPC ä¹‹ä¸Šæ„å»ºã€‚

We settled on gRPC primarily because it allowed us to bring forward our existing protobufs. For our use cases, multiplexing HTTP/2 transport and bi-directional streaming were also attractive.

æˆ‘ä»¬é€‰æ‹© gRPC ä¸»è¦æ˜¯å› ä¸ºå®ƒå…è®¸æˆ‘ä»¬æå‡ºæˆ‘ä»¬ç°æœ‰çš„ protobufã€‚å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œå¤šè·¯å¤ç”¨ HTTP/2 ä¼ è¾“å’ŒåŒå‘æµä¹Ÿå¾ˆæœ‰å¸å¼•åŠ›ã€‚

> Note that if [fbthrift](https://github.com/facebook/fbthrift) had existed at the time, we may have taken a closer look at Thrift based solutions.

> è¯·æ³¨æ„ï¼Œå¦‚æœ [fbthrift](https://github.com/facebook/fbthrift) å½“æ—¶å·²ç»å­˜åœ¨ï¼Œæˆ‘ä»¬å¯èƒ½å·²ç»ä»”ç»†ç ”ç©¶äº†åŸºäº Thrift çš„è§£å†³æ–¹æ¡ˆã€‚

## What Courier brings to gRPC

## Courier ä¸º gRPC å¸¦æ¥äº†ä»€ä¹ˆ

Courier is not a different RPC protocolâ€”itâ€™s just how Dropbox integrated gRPC with our existing infrastructure. For example, it needs to work with our specific versions of authentication, authorization, and service discovery. It also needs to integrate with our stats, event logging, and tracing tools. The result of all that work is what we call Courier.

Courier å¹¶ä¸æ˜¯ä¸€ç§ä¸åŒçš„ RPC åè®®â€”â€”å®ƒåªæ˜¯ Dropbox å°† gRPC ä¸æˆ‘ä»¬ç°æœ‰çš„åŸºç¡€è®¾æ–½é›†æˆçš„æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå®ƒéœ€è¦ä¸æˆ‘ä»¬ç‰¹å®šç‰ˆæœ¬çš„èº«ä»½éªŒè¯ã€æˆæƒå’ŒæœåŠ¡å‘ç°é…åˆä½¿ç”¨ã€‚å®ƒè¿˜éœ€è¦ä¸æˆ‘ä»¬çš„ç»Ÿè®¡ä¿¡æ¯ã€äº‹ä»¶æ—¥å¿—è®°å½•å’Œè·Ÿè¸ªå·¥å…·é›†æˆã€‚æ‰€æœ‰è¿™äº›å·¥ä½œçš„ç»“æœå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ Courierã€‚

> While we support using [Bandaid](https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/) as a gRPC proxy for a few specific use cases, the majority of our services communicate with each other with no proxy, to minimize the effect of the RPC on serving latency.

> è™½ç„¶æˆ‘ä»¬æ”¯æŒä½¿ç”¨ [Bandaid](https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/) ä½œä¸ºä¸€äº›ç‰¹å®šç”¨ä¾‹çš„ gRPC ä»£ç†ï¼Œä½†æˆ‘ä»¬çš„å¤§å¤šæ•°æœåŠ¡åœ¨æ²¡æœ‰ä»£ç†çš„æƒ…å†µä¸‹ç›¸äº’é€šä¿¡ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘ RPC å¯¹æœåŠ¡å»¶è¿Ÿçš„å½±å“ã€‚

We want to minimize the amount of boilerplate we write. Since Courier is our common framework for service development, it incorporates features which all services need. Most of these features are enabled by default, and can be controlled by command-line arguments. Some of them can also be toggled dynamically via a feature flag.

æˆ‘ä»¬å¸Œæœ›å°½é‡å‡å°‘æˆ‘ä»¬ç¼–å†™çš„æ ·æ¿æ•°é‡ã€‚ç”±äº Courier æ˜¯æˆ‘ä»¬ç”¨äºæœåŠ¡å¼€å‘çš„é€šç”¨æ¡†æ¶ï¼Œå› æ­¤å®ƒåŒ…å«äº†æ‰€æœ‰æœåŠ¡æ‰€éœ€çš„åŠŸèƒ½ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™äº›åŠŸèƒ½ä¸­çš„å¤§å¤šæ•°éƒ½æ˜¯å¯ç”¨çš„ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¿›è¡Œæ§åˆ¶ã€‚å…¶ä¸­ä¸€äº›è¿˜å¯ä»¥é€šè¿‡åŠŸèƒ½æ ‡å¿—åŠ¨æ€åˆ‡æ¢ã€‚

### Security: service identity and TLS mutual authentication

### å®‰å…¨ï¼šæœåŠ¡èº«ä»½å’Œ TLS ç›¸äº’è®¤è¯

Courier implements our standard service identity mechanism. All our servers and clients have their own TLS certificates, which are issued by our internal Certificate Authority. Each one has an identity, encoded in the certificate. This identity is then used for mutual authentication, where the server verifies the client, and the client verifies the server. 

Courier å®ç°äº†æˆ‘ä»¬çš„æ ‡å‡†æœåŠ¡æ ‡è¯†æœºåˆ¶ã€‚æˆ‘ä»¬æ‰€æœ‰çš„æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯éƒ½æœ‰è‡ªå·±çš„ TLS è¯ä¹¦ï¼Œç”±æˆ‘ä»¬çš„å†…éƒ¨è¯ä¹¦é¢å‘æœºæ„é¢å‘ã€‚æ¯ä¸ªäººéƒ½æœ‰ä¸€ä¸ªèº«ä»½ï¼Œç¼–ç åœ¨è¯ä¹¦ä¸­ã€‚ç„¶åå°†æ­¤èº«ä»½ç”¨äºç›¸äº’èº«ä»½éªŒè¯ï¼Œå…¶ä¸­æœåŠ¡å™¨éªŒè¯å®¢æˆ·ç«¯ï¼Œå®¢æˆ·ç«¯éªŒè¯æœåŠ¡å™¨ã€‚

> On the TLS side, where we control both ends of the communication, we enforce quite restrictive defaults. Encryption with [PFS](https://scotthelme.co.uk/perfect-forward-secrecy/) is mandatory for all internal RPCs. The TLS version is pinned to 1.2+. We also restrict symmetric/asymmetric algorithms to a secure subset, with `ECDHE-ECDSA-AES128-GCM-SHA256` being preferred.

> åœ¨ TLS æ–¹é¢ï¼Œæˆ‘ä»¬æ§åˆ¶é€šä¿¡çš„ä¸¤ç«¯ï¼Œæˆ‘ä»¬å¼ºåˆ¶æ‰§è¡Œéå¸¸ä¸¥æ ¼çš„é»˜è®¤å€¼ã€‚æ‰€æœ‰å†…éƒ¨ RPC éƒ½å¿…é¡»ä½¿ç”¨ [PFS](https://scotthelme.co.uk/perfect-forward-secrecy/) è¿›è¡ŒåŠ å¯†ã€‚ TLS ç‰ˆæœ¬å›ºå®šä¸º 1.2+ã€‚æˆ‘ä»¬è¿˜å°†å¯¹ç§°/éå¯¹ç§°ç®—æ³•é™åˆ¶ä¸ºå®‰å…¨å­é›†ï¼Œé¦–é€‰â€œECDHE-ECDSA-AES128-GCM-SHA256â€ã€‚

After identity is confirmed and the request is decrypted, the server verifies that the client has proper permissions. Access Control Lists (ACLs) and rate limits can be set on both services and individual methods. They can also be updated via our distributed config filesystem (AFS). This allows service owners to shed load in a matter of seconds, without needing to restart processes. Subscribing to notifications and handling configuration updates is taken care of by the Courier framework.

åœ¨ç¡®è®¤èº«ä»½å¹¶è§£å¯†è¯·æ±‚åï¼ŒæœåŠ¡å™¨éªŒè¯å®¢æˆ·ç«¯æ˜¯å¦å…·æœ‰é€‚å½“çš„æƒé™ã€‚å¯ä»¥åœ¨æœåŠ¡å’Œä¸ªåˆ«æ–¹æ³•ä¸Šè®¾ç½®è®¿é—®æ§åˆ¶åˆ—è¡¨ (ACL) å’Œé€Ÿç‡é™åˆ¶ã€‚å®ƒä»¬ä¹Ÿå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„åˆ†å¸ƒå¼é…ç½®æ–‡ä»¶ç³»ç»Ÿ (AFS) è¿›è¡Œæ›´æ–°ã€‚è¿™å…è®¸æœåŠ¡æ‰€æœ‰è€…åœ¨å‡ ç§’é’Ÿå†…å‡è½»è´Ÿè½½ï¼Œè€Œæ— éœ€é‡æ–°å¯åŠ¨è¿›ç¨‹ã€‚ Courier æ¡†æ¶è´Ÿè´£è®¢é˜…é€šçŸ¥å’Œå¤„ç†é…ç½®æ›´æ–°ã€‚

> Service â€œIdentityâ€ is the global identifier for ACLs, rate limits, stats, and more. As a side bonus, itâ€™s also cryptographically secure.

> æœåŠ¡â€œèº«ä»½â€æ˜¯ ACLã€é€Ÿç‡é™åˆ¶ã€ç»Ÿè®¡ä¿¡æ¯ç­‰çš„å…¨å±€æ ‡è¯†ç¬¦ã€‚ä½œä¸ºé™„å¸¦å¥–åŠ±ï¼Œå®ƒä¹Ÿæ˜¯åŠ å¯†å®‰å…¨çš„ã€‚

Here is an example of Courier ACL/ratelimit configuration definition from our [Optical Character Recognition (OCR) service](https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text-from-billions-of-images/):

ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„ [å…‰å­¦å­—ç¬¦è¯†åˆ« (OCR) æœåŠ¡](https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text) çš„ Courier ACL/ratelimit é…ç½®å®šä¹‰ç¤ºä¾‹-æ¥è‡ªæ•°åäº¿å¼ å›¾ç‰‡/)ï¼š

```
limits:
dropbox_engine_ocr:
    # All RPC methods.
    default:
      max_concurrency: 32
      queue_timeout_ms: 1000

      rate_acls:
        # OCR clients are unlimited.
        ocr: -1
        # Nobody else gets to talk to us.
        authenticated: 0
        unauthenticated: 0

```


![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/02-screenshot2018-12-0317.31.03.png)

> We are considering adopting the SPIFFE Verifiable Identity Document (SVID), which is part of [Secure Production Identity Framework for Everyone](https://spiffe.io/)(SPIFFE). This would make our RPC framework compatible with various open source projects.

> æˆ‘ä»¬æ­£åœ¨è€ƒè™‘é‡‡ç”¨ SPIFFE Verifiable Identity Document (SVID)ï¼Œå®ƒæ˜¯ [Secure Production Identity Framework for Everyone](https://spiffe.io/)(SPIFFE) çš„ä¸€éƒ¨åˆ†ã€‚è¿™å°†ä½¿æˆ‘ä»¬çš„ RPC æ¡†æ¶ä¸å„ç§å¼€æºé¡¹ç›®å…¼å®¹ã€‚

### Observability: stats and tracing

### å¯è§‚å¯Ÿæ€§ï¼šç»Ÿè®¡å’Œè·Ÿè¸ª

Using just an identity, you can easily locate standard logs, stats, traces, and other useful information about a Courier service.

ä»…ä½¿ç”¨èº«ä»½ï¼Œæ‚¨å°±å¯ä»¥è½»æ¾æ‰¾åˆ°æœ‰å…³ Courier æœåŠ¡çš„æ ‡å‡†æ—¥å¿—ã€ç»Ÿè®¡ä¿¡æ¯ã€è·Ÿè¸ªå’Œå…¶ä»–æœ‰ç”¨ä¿¡æ¯ã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/03-screenshot2018-12-0518.03.17.png)

Our code generation adds per-service and per-method stats for both clients and servers. Server stats are broken down by the client identity. Out of the box, we have granular attribution of load, errors, and latency for any Courier service.

æˆ‘ä»¬çš„ä»£ç ç”Ÿæˆä¸ºå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨æ·»åŠ äº†æ¯ä¸ªæœåŠ¡å’Œæ¯ä¸ªæ–¹æ³•çš„ç»Ÿè®¡ä¿¡æ¯ã€‚æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯æŒ‰å®¢æˆ·ç«¯èº«ä»½è¿›è¡Œç»†åˆ†ã€‚å¼€ç®±å³ç”¨ï¼Œæˆ‘ä»¬å¯¹ä»»ä½• Courier æœåŠ¡çš„è´Ÿè½½ã€é”™è¯¯å’Œå»¶è¿Ÿéƒ½æœ‰è¯¦ç»†çš„å½’å› ã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/gw1uztwk.png)

Courier stats include client-side availability and latency, as well as server-side request rates and queue sizes. We also have various break-downs like per-method latency histograms or per-client TLS handshakes.

Courier ç»Ÿè®¡ä¿¡æ¯åŒ…æ‹¬å®¢æˆ·ç«¯å¯ç”¨æ€§å’Œå»¶è¿Ÿï¼Œä»¥åŠæœåŠ¡å™¨ç«¯è¯·æ±‚ç‡å’Œé˜Ÿåˆ—å¤§å°ã€‚æˆ‘ä»¬è¿˜æœ‰å„ç§æ•…éšœï¼Œä¾‹å¦‚æ¯ä¸ªæ–¹æ³•çš„å»¶è¿Ÿç›´æ–¹å›¾æˆ–æ¯ä¸ªå®¢æˆ·ç«¯çš„ TLS æ¡æ‰‹ã€‚

> One of the benefits of having our own code generation is that we can initialize these data structures statically, including histograms and tracing spans. This minimizes the performance impact.

> æ‹¥æœ‰è‡ªå·±çš„ä»£ç ç”Ÿæˆçš„å¥½å¤„ä¹‹ä¸€æ˜¯æˆ‘ä»¬å¯ä»¥é™æ€åˆå§‹åŒ–è¿™äº›æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬ç›´æ–¹å›¾å’Œè·Ÿè¸ªè·¨åº¦ã€‚è¿™æœ€å¤§é™åº¦åœ°å‡å°‘äº†æ€§èƒ½å½±å“ã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/05-screenshot2018-12-0516.44.06.png)

Our legacy RPC only propagated `request_id` across API boundaries. This allowed joining logs from different services. In Courier, weâ€™ve introduced an API based on a subset of the [OpenTracing](https://opentracing.io/) specification. We wrote our own client libraries, while the server-side is built on top of Cassandra and [Jaeger](https://github.com/jaegertracing/jaeger). The details of how we made this tracing system performant warrant a dedicated blog post.

æˆ‘ä»¬çš„æ—§ç‰ˆ RPC ä»…è·¨ API è¾¹ç•Œä¼ æ’­ `request_id`ã€‚è¿™å…è®¸åŠ å…¥æ¥è‡ªä¸åŒæœåŠ¡çš„æ—¥å¿—ã€‚åœ¨ Courier ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäº [OpenTracing](https://opentracing.io/) è§„èŒƒå­é›†çš„ APIã€‚æˆ‘ä»¬ç¼–å†™äº†è‡ªå·±çš„å®¢æˆ·ç«¯åº“ï¼Œè€ŒæœåŠ¡å™¨ç«¯æ„å»ºåœ¨ Cassandra å’Œ [Jaeger](https://github.com/jaegertracing/jaeger) ä¹‹ä¸Šã€‚æˆ‘ä»¬å¦‚ä½•ä½¿æ­¤è·Ÿè¸ªç³»ç»Ÿå…·æœ‰é«˜æ€§èƒ½çš„è¯¦ç»†ä¿¡æ¯éœ€è¦ä¸“é—¨çš„åšå®¢æ–‡ç« ã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/06-screenshot2018-12-0516.35.14.png)

Tracing also gives us the ability to generate a runtime service dependency graph. This helps engineers to understand all the transitive dependencies of a service. It can also potentially be used as a post-deploy check for avoiding unintentional dependencies.

è·Ÿè¸ªè¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆè¿è¡Œæ—¶æœåŠ¡ä¾èµ–å…³ç³»å›¾ã€‚è¿™æœ‰åŠ©äºå·¥ç¨‹å¸ˆäº†è§£æœåŠ¡çš„æ‰€æœ‰ä¼ é€’ä¾èµ–å…³ç³»ã€‚å®ƒè¿˜å¯ä»¥æ½œåœ¨åœ°ç”¨ä½œéƒ¨ç½²åæ£€æŸ¥ï¼Œä»¥é¿å…æ— æ„çš„ä¾èµ–å…³ç³»ã€‚

### Reliability: deadlines and circuit-breaking

### å¯é æ€§ï¼šæˆªæ­¢æ—¥æœŸå’Œæ–­è·¯

Courier provides a centralized location for language specific implementations of functionality common to all clients, such as timeouts. Over time, we have added many capabilities at this layer, often as action items from postmortems. 

Courier ä¸ºæ‰€æœ‰å®¢æˆ·ç«¯é€šç”¨çš„åŠŸèƒ½çš„è¯­è¨€ç‰¹å®šå®ç°æä¾›äº†ä¸€ä¸ªé›†ä¸­ä½ç½®ï¼Œä¾‹å¦‚è¶…æ—¶ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸€å±‚æ·»åŠ äº†è®¸å¤šåŠŸèƒ½ï¼Œé€šå¸¸ä½œä¸ºäº‹ååˆ†æçš„æ“ä½œé¡¹ã€‚

**Deadlines** Every [gRPC](https://grpc.io/blog/deadlines) [request includes a](https://grpc.io/blog/deadlines) [deadline](https://grpc.io/blog/deadlines), indicating how long the client will wait for a reply. Since Courier stubs automatically propagate known metadata, the deadline travels with the request even across API boundaries. Within a process, deadlines are converted into a native representation. For example, in Go they are represented by a `context.Context` result from the `WithDeadline` method.

**æˆªæ­¢æ—¥æœŸ** æ¯ä¸ª [gRPC](https://grpc.io/blog/deadlines)[è¯·æ±‚åŒ…æ‹¬](https://grpc.io/blog/deadlines) [deadline](https://grpc.io/blog/deadlines)ï¼Œè¡¨ç¤ºå®¢æˆ·ç«¯ç­‰å¾…å›å¤çš„æ—¶é—´ã€‚ç”±äº Courier å­˜æ ¹ä¼šè‡ªåŠ¨ä¼ æ’­å·²çŸ¥å…ƒæ•°æ®ï¼Œå› æ­¤å³ä½¿è·¨è¶Š API è¾¹ç•Œï¼Œæˆªæ­¢æ—¥æœŸä¹Ÿä¼šéšç€è¯·æ±‚è€Œå˜åŒ–ã€‚åœ¨æµç¨‹ä¸­ï¼Œæˆªæ­¢æ—¥æœŸè¢«è½¬æ¢ä¸ºæœ¬åœ°è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œåœ¨ Go ä¸­ï¼Œå®ƒä»¬ç”± `WithDeadline` æ–¹æ³•çš„ `context.Context` ç»“æœè¡¨ç¤ºã€‚

In practice, we have fixed whole classes of reliability problems by forcing engineers to define deadlines in their service definitions.

åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼ºåˆ¶å·¥ç¨‹å¸ˆåœ¨ä»–ä»¬çš„æœåŠ¡å®šä¹‰ä¸­å®šä¹‰æˆªæ­¢æ—¥æœŸæ¥ä¿®å¤æ•´ç±»å¯é æ€§é—®é¢˜ã€‚

> This context can travel even outside of the RPC layer! For example, our legacy MySQL ORM serializes the RPC context along with the deadline into a comment in the SQL query. Our SQLProxy can parse these comments and `KILL` queries when the deadline is exceeded. As a side benefit, we have per-request attribution when debugging database queries.

> è¿™ä¸ªä¸Šä¸‹æ–‡ç”šè‡³å¯ä»¥åœ¨ RPC å±‚ä¹‹å¤–ä¼ æ’­ï¼ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„æ—§ MySQL ORM å°† RPC ä¸Šä¸‹æ–‡è¿åŒæˆªæ­¢æ—¥æœŸåºåˆ—åŒ–ä¸º SQL æŸ¥è¯¢ä¸­çš„æ³¨é‡Šã€‚æˆ‘ä»¬çš„ SQLProxy å¯ä»¥åœ¨è¶…è¿‡æˆªæ­¢æ—¥æœŸæ—¶è§£æè¿™äº›è¯„è®ºå’Œ `KILL` æŸ¥è¯¢ã€‚ä½œä¸ºé™„å¸¦çš„å¥½å¤„ï¼Œæˆ‘ä»¬åœ¨è°ƒè¯•æ•°æ®åº“æŸ¥è¯¢æ—¶æœ‰æ¯ä¸ªè¯·æ±‚çš„å±æ€§ã€‚

**Circuit-breaking** Another common problem that our legacy RPC clients have to solve is implementing custom exponential backoff and jitter on retries. This is often necessary to prevent cascading overloads from one service to another.

**ç”µè·¯ä¸­æ–­** æˆ‘ä»¬çš„ä¼ ç»Ÿ RPC å®¢æˆ·ç«¯å¿…é¡»è§£å†³çš„å¦ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯åœ¨é‡è¯•æ—¶å®ç°è‡ªå®šä¹‰æŒ‡æ•°é€€é¿å’ŒæŠ–åŠ¨ã€‚è¿™é€šå¸¸æ˜¯å¿…è¦çš„ï¼Œä»¥é˜²æ­¢ä»ä¸€é¡¹æœåŠ¡åˆ°å¦ä¸€é¡¹æœåŠ¡çš„çº§è”è¿‡è½½ã€‚

In Courier, we wanted to solve circuit-breaking in a more generic way. We started by introducing a LIFO queue between the listener and the workpool.

åœ¨ Courier ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä»¥æ›´é€šç”¨çš„æ–¹å¼è§£å†³æ–­è·¯é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨ä¾¦å¬å™¨å’Œå·¥ä½œæ± ä¹‹é—´å¼•å…¥äº†ä¸€ä¸ª LIFO é˜Ÿåˆ—ã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/07-screenshot2018-12-0521.54.58.png)

In the case of a service overload, this LIFO queue acts as an automatic circuit breaker. The queue is not only bounded by size, but critically, itâ€™s also **bounded by time**. A request can only spend so long in the queue.

åœ¨æœåŠ¡è¿‡è½½çš„æƒ…å†µä¸‹ï¼Œæ­¤ LIFO é˜Ÿåˆ—å……å½“è‡ªåŠ¨æ–­è·¯å™¨ã€‚é˜Ÿåˆ—ä¸ä»…å—å¤§å°é™åˆ¶ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä¹Ÿ**å—æ—¶é—´é™åˆ¶**ã€‚ä¸€ä¸ªè¯·æ±‚åªèƒ½åœ¨é˜Ÿåˆ—ä¸­èŠ±è´¹è¿™ä¹ˆé•¿æ—¶é—´ã€‚

> LIFO has the downside of request reordering. If you want to preserve ordering, you can use [CoDel](https://queue.acm.org/detail.cfm?id=2209336). It also has circuit breaking properties, but wonâ€™t mess with the order of requests.

> LIFO æœ‰è¯·æ±‚é‡æ–°æ’åºçš„ç¼ºç‚¹ã€‚å¦‚æœæ‚¨æƒ³ä¿ç•™é¡ºåºï¼Œå¯ä»¥ä½¿ç”¨ [CoDel](https://queue.acm.org/detail.cfm?id=2209336)ã€‚å®ƒè¿˜å…·æœ‰æ–­è·¯ç‰¹æ€§ï¼Œä½†ä¸ä¼šæ‰°ä¹±è¯·æ±‚çš„é¡ºåºã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/08-screenshot2018-12-0521.54.48.png)

### Introspection: debug endpoints

### å†…çœï¼šè°ƒè¯•ç«¯ç‚¹

Even though debug endpoints are not part of Courier itself, they are widely adopted across Dropbox. They are too useful to not mention! Here are a couple of examples of useful introspections.

å°½ç®¡è°ƒè¯•ç«¯ç‚¹ä¸æ˜¯ Courier æœ¬èº«çš„ä¸€éƒ¨åˆ†ï¼Œä½†å®ƒä»¬åœ¨ Dropbox ä¸­è¢«å¹¿æ³›é‡‡ç”¨ã€‚å®ƒä»¬å¤ªæœ‰ç”¨äº†ï¼Œä¸èƒ½ä¸æï¼ä»¥ä¸‹æ˜¯ä¸€äº›æœ‰ç”¨çš„å†…çœç¤ºä¾‹ã€‚

> For security reasons, you may want to expose these on a separate port (possibly only on a loopback interface) or even a Unix socket (so access can be additionally controlled with Unix file permissions.) You should also strongly consider using mutual TLS authentication there by asking developers to present their certs to access debug endpoints (esp. non-readonly ones.)

> å‡ºäºå®‰å…¨åŸå› ï¼Œæ‚¨å¯èƒ½å¸Œæœ›åœ¨å•ç‹¬çš„ç«¯å£ï¼ˆå¯èƒ½ä»…åœ¨ç¯å›æ¥å£ä¸Šï¼‰æˆ–ä»€è‡³ Unix å¥—æ¥å­—ä¸Šå…¬å¼€è¿™äº›ï¼ˆå› æ­¤å¯ä»¥ä½¿ç”¨ Unix æ–‡ä»¶æƒé™é¢å¤–æ§åˆ¶è®¿é—®ã€‚ï¼‰æ‚¨è¿˜åº”è¯¥å¼ºçƒˆè€ƒè™‘ä½¿ç”¨ç›¸äº’ TLS èº«ä»½éªŒè¯é€šè¿‡è¦æ±‚å¼€å‘äººå‘˜æä¾›ä»–ä»¬çš„è¯ä¹¦æ¥è®¿é—®è°ƒè¯•ç«¯ç‚¹ï¼ˆç‰¹åˆ«æ˜¯éåªè¯»ç«¯ç‚¹ï¼‰ã€‚

**Runtime** Having the ability to get an insight into the runtime state is a very useful debug feature, e.g. [heap and CPU profiles could be exposed as HTTP or gRPC endpoints](https://golang.org/pkg/net/http/pprof/).

**è¿è¡Œæ—¶** èƒ½å¤Ÿæ·±å…¥äº†è§£è¿è¡Œæ—¶çŠ¶æ€æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„è°ƒè¯•åŠŸèƒ½ï¼Œä¾‹å¦‚[å †å’Œ CPU é…ç½®æ–‡ä»¶å¯ä»¥ä½œä¸º HTTP æˆ– gRPC ç«¯ç‚¹å…¬å¼€](https://golang.org/pkg/net/http/pprof/)ã€‚

> We are planning on using this during the canary verification procedure to automate CPU/memory diffs between old and new code versions.

> æˆ‘ä»¬è®¡åˆ’åœ¨é‡‘ä¸é›€éªŒè¯è¿‡ç¨‹ä¸­ä½¿ç”¨å®ƒæ¥è‡ªåŠ¨åŒ–æ–°æ—§ä»£ç ç‰ˆæœ¬ä¹‹é—´çš„ CPU/å†…å­˜å·®å¼‚ã€‚

These debug endpoints can allow modification of runtime state, e.g. a golang-based service can allow dynamically setting the [GCPercent](https://golang.org/pkg/runtime/debug/#SetGCPercent).

è¿™äº›è°ƒè¯•ç«¯ç‚¹å¯ä»¥å…è®¸ä¿®æ”¹è¿è¡Œæ—¶çŠ¶æ€ï¼Œä¾‹å¦‚åŸºäº golang çš„æœåŠ¡å¯ä»¥å…è®¸åŠ¨æ€è®¾ç½® [GCPercent](https://golang.org/pkg/runtime/debug/#SetGCPercent)ã€‚

**Library** For a library author being able to automatically export some library-specific data as an RPC-endpoint may be quite useful. Good examples here is that [malloc library can dump its internal stats](http://jemalloc.net/jemalloc.3.html#malloc_stats_print_opts). Another example is a read/write debug endpoint to change the logging level of a service on the fly.

**åº“** å¯¹äºåº“ä½œè€…è€Œè¨€ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å¯¼å‡ºæŸäº›åº“ç‰¹å®šæ•°æ®ä½œä¸º RPC ç«¯ç‚¹å¯èƒ½éå¸¸æœ‰ç”¨ã€‚è¿™é‡Œçš„å¥½ä¾‹å­æ˜¯ [malloc åº“å¯ä»¥è½¬å‚¨å…¶å†…éƒ¨ç»Ÿè®¡ä¿¡æ¯](http://jemalloc.net/jemalloc.3.html#malloc_stats_print_opts)ã€‚å¦ä¸€ä¸ªç¤ºä¾‹æ˜¯è¯»/å†™è°ƒè¯•ç«¯ç‚¹ï¼Œç”¨äºåŠ¨æ€æ›´æ”¹æœåŠ¡çš„æ—¥å¿—è®°å½•çº§åˆ«ã€‚

**RPC** It is given that troubleshooting encrypted and binary-encoded protocols will be a bit complicated, therefore putting in as much instrumentation as performance allows in the RPC layer itself is the right thing to do. One example of such an introspection API is a recent [channelz proposal for the gRPC](https://github.com/grpc/proposal/blob/master/A14-channelz.md).

**RPC** å‡å®šå¯¹åŠ å¯†å’ŒäºŒè¿›åˆ¶ç¼–ç åè®®è¿›è¡Œæ•…éšœæ’é™¤ä¼šæœ‰ç‚¹å¤æ‚ï¼Œå› æ­¤åœ¨ RPC å±‚æœ¬èº«ä¸­å°½å¯èƒ½å¤šåœ°è¿›è¡Œæ€§èƒ½æµ‹è¯•æ˜¯æ­£ç¡®çš„åšæ³•ã€‚è¿™ç§å†…çœ API çš„ä¸€ä¸ªä¾‹å­æ˜¯æœ€è¿‘çš„ [é’ˆå¯¹ gRPC çš„ channelz ææ¡ˆ](https://github.com/grpc/proposal/blob/master/A14-channelz.md)ã€‚

**Application** Being able to view application-level parameters can also be useful. A good example is a generalized application info endpoint with build/source hash, command line, etc. This can be used by the orchestration system to verify the consistency of a service deployment.

**åº”ç”¨ç¨‹åº** èƒ½å¤ŸæŸ¥çœ‹åº”ç”¨ç¨‹åºçº§å‚æ•°ä¹Ÿå¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯å…·æœ‰æ„å»º/æºå“ˆå¸Œã€å‘½ä»¤è¡Œç­‰çš„é€šç”¨åº”ç”¨ç¨‹åºä¿¡æ¯ç«¯ç‚¹ã€‚ç¼–æ’ç³»ç»Ÿå¯ä»¥ä½¿ç”¨å®ƒæ¥éªŒè¯æœåŠ¡éƒ¨ç½²çš„ä¸€è‡´æ€§ã€‚

## Performance optimizations

## æ€§èƒ½ä¼˜åŒ–

We discovered a handful of Dropbox specific performance bottlenecks when rolling out gRPC at scale.

åœ¨å¤§è§„æ¨¡æ¨å‡º gRPC æ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº› Dropbox ç‰¹å®šçš„æ€§èƒ½ç“¶é¢ˆã€‚

### TLS handshake overhead 

### TLS æ¡æ‰‹å¼€é”€

With a service that handles lots of connections, the cumulative CPU overhead of TLS handshakes can become non-negligible. This is especially true during mass service restarts.

å¯¹äºå¤„ç†å¤§é‡è¿æ¥çš„æœåŠ¡ï¼ŒTLS æ¡æ‰‹çš„ç´¯ç§¯ CPU å¼€é”€å¯èƒ½å˜å¾—ä¸å¯å¿½ç•¥ã€‚åœ¨å¤§é‡æœåŠ¡é‡æ–°å¯åŠ¨æœŸé—´å°¤å…¶å¦‚æ­¤ã€‚

We switched from RSA 2048 keypairs to ECDSA P-256 to get better performance for signing operations. Here are BoringSSL performance examples (note that RSA is still faster for signature verification):

æˆ‘ä»¬ä» RSA 2048 å¯†é’¥å¯¹åˆ‡æ¢åˆ° ECDSA P-256 ä»¥è·å¾—æ›´å¥½çš„ç­¾åæ“ä½œæ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯ BoringSSL æ€§èƒ½ç¤ºä¾‹ï¼ˆè¯·æ³¨æ„ï¼ŒRSA åœ¨ç­¾åéªŒè¯æ–¹é¢ä»ç„¶æ›´å¿«ï¼‰ï¼š

RSA:

```
ğ›Œ ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'RSA 2048'
Did ... RSA 2048 signing operations in ..............  (1527.9 ops/sec)
Did ... RSA 2048 verify (same key) operations in .... (37066.4 ops/sec)
Did ... RSA 2048 verify (fresh key) operations in ... (25887.6 ops/sec)

```


ECDSA:

```
ğ›Œ ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'ECDSA P-256'
Did ... ECDSA P-256 signing operations in ... (40410.9 ops/sec)
Did ... ECDSA P-256 verify operations in .... (17037.5 ops/sec)

```


> Since RSA 2048 verification is ~3x faster than ECDSA P-256 one, from a performance perspective, you may consider using RSA for your root/leaf certs. From a security perspective though itâ€™s a bit more complicated since youâ€™ll be chaining different security primitives and therefore resulting security properties will be the minimum of all of them. For the same performance reasons you should also think twice before using RSA 4096 (and higher) certs for your root/leaf certs.

> ç”±äº RSA 2048 éªŒè¯æ¯” ECDSA P-256 éªŒè¯å¿«çº¦ 3 å€ï¼Œå› æ­¤ä»æ€§èƒ½è§’åº¦æ¥çœ‹ï¼Œæ‚¨å¯ä»¥è€ƒè™‘å°† RSA ç”¨äºæ ¹/å¶è¯ä¹¦ã€‚ä»å®‰å…¨çš„è§’åº¦æ¥çœ‹ï¼Œè™½ç„¶å®ƒæœ‰ç‚¹å¤æ‚ï¼Œå› ä¸ºæ‚¨å°†é“¾æ¥ä¸åŒçš„å®‰å…¨åŸè¯­ï¼Œå› æ­¤äº§ç”Ÿçš„å®‰å…¨å±æ€§å°†æ˜¯æ‰€æœ‰è¿™äº›ä¸­çš„æœ€å°å€¼ã€‚å‡ºäºç›¸åŒçš„æ€§èƒ½åŸå› ï¼Œæ‚¨åœ¨ä¸ºæ ¹/å¶è¯ä¹¦ä½¿ç”¨ RSA 4096ï¼ˆåŠæ›´é«˜ç‰ˆæœ¬ï¼‰è¯ä¹¦ä¹‹å‰ä¹Ÿåº”è¯¥ä¸‰æ€è€Œåè¡Œã€‚

We also found that TLS library choice (and compilation flags) matter a lot for both performance and security. For example, here is a comparison of MacOS X Mojaveâ€™s LibreSSL build vs homebrewed OpenSSL on the same hardware:

æˆ‘ä»¬è¿˜å‘ç° TLS åº“çš„é€‰æ‹©ï¼ˆå’Œç¼–è¯‘æ ‡å¿—ï¼‰å¯¹æ€§èƒ½å’Œå®‰å…¨æ€§éƒ½å¾ˆé‡è¦ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯åœ¨ç›¸åŒç¡¬ä»¶ä¸Š MacOS X Mojave çš„ LibreSSL æ„å»ºä¸è‡ªåˆ¶ OpenSSL çš„æ¯”è¾ƒï¼š

LibreSSL 2.6.4:

```
ğ›Œ ~ openssl speed rsa2048
LibreSSL 2.6.4
...
                  sign    verify    sign/s verify/s
rsa 2048 bits 0.032491s 0.001505s     30.8    664.3

```

OpenSSL 1.1.1aï¼š

```
ğ›Œ ~ openssl speed rsa2048
OpenSSL 1.1.1a  20 Nov 2018
...
                  sign    verify    sign/s verify/s
rsa 2048 bits 0.000992s 0.000029s   1208.0  34454.8

```


But the fastest way to do a TLS handshake is to not do it at all! [Weâ€™ve modified gRPC-core and gRPC-python](https://github.com/grpc/grpc/issues/14425) to support session resumption, which made service rollout way less CPU intensive.

ä½†æ˜¯è¿›è¡Œ TLS æ¡æ‰‹çš„æœ€å¿«æ–¹æ³•æ˜¯æ ¹æœ¬ä¸è¿™æ ·åšï¼ [æˆ‘ä»¬ä¿®æ”¹äº† gRPC-core å’Œ gRPC-python](https://github.com/grpc/grpc/issues/14425) ä»¥æ”¯æŒä¼šè¯æ¢å¤ï¼Œè¿™ä½¿å¾—æœåŠ¡æ¨å‡ºçš„ CPU å¯†é›†åº¦é™ä½ã€‚

### Encryption is not expensive

### åŠ å¯†å¹¶ä¸æ˜‚è´µ

It is a common misconception that encryption is expensive. Symmetric encryption is actually blazingly fast on modern hardware. A desktop-grade processor is able to encrypt and authenticate data at 40Gbps rate on a single core:

è®¤ä¸ºåŠ å¯†å¾ˆæ˜‚è´µæ˜¯ä¸€ç§å¸¸è§çš„è¯¯è§£ã€‚å¯¹ç§°åŠ å¯†åœ¨ç°ä»£ç¡¬ä»¶ä¸Šå®é™…ä¸Šéå¸¸å¿«ã€‚æ¡Œé¢çº§å¤„ç†å™¨èƒ½å¤Ÿåœ¨å•æ ¸ä¸Šä»¥ 40Gbps çš„é€Ÿç‡åŠ å¯†å’ŒéªŒè¯æ•°æ®ï¼š

```
ğ›Œ ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'AES'
Did ... AES-128-GCM (8192 bytes) seal operations in ... 4534.4 MB/s

```


Nevertheless, we did end up having to tune gRPC for our [50Gb/s storage boxes](https://blogs.dropbox.com/tech/2018/06/extending-magic-pocket-innovation-with-the-first-petabyte-scale-smr-drive-deployment/). We learned that when the encryption speed is comparable to the memory copy speed, reducing the number of `memcpy` operations was critical. In addition, we also made [some of the changes to gRPC itself](https://github.com/grpc/grpc/issues/14058).

å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬æœ€ç»ˆè¿˜æ˜¯ä¸å¾—ä¸ä¸ºæˆ‘ä»¬çš„ [50Gb/s å­˜å‚¨ç›’](https://blogs.dropbox.com/tech/2018/06/extending-magic-pocket-innovation-with-the-first-) è°ƒæ•´ gRPC PB-scale-smr-drive-deployment/)ã€‚æˆ‘ä»¬äº†è§£åˆ°ï¼Œå½“åŠ å¯†é€Ÿåº¦ä¸å†…å­˜å¤åˆ¶é€Ÿåº¦ç›¸å½“æ—¶ï¼Œå‡å°‘â€œmemcpyâ€æ“ä½œçš„æ•°é‡è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ [gRPC æœ¬èº«è¿›è¡Œäº†ä¸€äº›æ›´æ”¹](https://github.com/grpc/grpc/issues/14058)ã€‚

> Authenticated and encrypted protocols have caught many tricky hardware issues. For example, processor, DMA, and network data corruptions. Even if you are not using gRPC, using TLS for internal communication is always a good idea.

> ç»è¿‡èº«ä»½éªŒè¯å’ŒåŠ å¯†çš„åè®®å·²ç»è§£å†³äº†è®¸å¤šæ£˜æ‰‹çš„ç¡¬ä»¶é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¤„ç†å™¨ã€DMA å’Œç½‘ç»œæ•°æ®æŸåã€‚å³ä½¿æ‚¨ä¸ä½¿ç”¨ gRPCï¼Œä½¿ç”¨ TLS è¿›è¡Œå†…éƒ¨é€šä¿¡ä¹Ÿå§‹ç»ˆæ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

### High Bandwidth-Delay product links 

### é«˜å¸¦å®½å»¶è¿Ÿäº§å“é“¾æ¥

Dropbox has [multiple data centers connected through a backbone network](https://blogs.dropbox.com/tech/2017/09/infrastructure-update-evolution-of-the-dropbox-backbone-network/). Sometimes nodes from different regions need to communicate with each other over RPC, e.g. for the purposes of replication. When using TCP the kernel is responsible for limiting the amount of data inflight for a given connection (within the limits of `/proc/sys/net/ipv4/tcp_{r,w}mem`), though since gRPC is HTTP/2 -based it also has its own flow control on top of TCP. [The upper bound for the BDP is hardcoded in](https://github.com/grpc/grpc-go/issues/2400) [grpc-go to 16Mb](https://github.com/grpc/grpc-go/issues/2400), which can become a bottleneck for a single high BDP connection.

Dropbox æœ‰[é€šè¿‡éª¨å¹²ç½‘è¿æ¥çš„å¤šä¸ªæ•°æ®ä¸­å¿ƒ](https://blogs.dropbox.com/tech/2017/09/infrastructure-update-evolution-of-the-dropbox-backbone-network/)ã€‚æœ‰æ—¶æ¥è‡ªä¸åŒåŒºåŸŸçš„èŠ‚ç‚¹éœ€è¦é€šè¿‡ RPC ç›¸äº’é€šä¿¡ï¼Œä¾‹å¦‚ç”¨äºå¤åˆ¶çš„ç›®çš„ã€‚å½“ä½¿ç”¨ TCP æ—¶ï¼Œå†…æ ¸è´Ÿè´£é™åˆ¶ç»™å®šè¿æ¥çš„ä¼ è¾“æ•°æ®é‡ï¼ˆåœ¨`/proc/sys/net/ipv4/tcp_{r,w}mem` çš„é™åˆ¶å†…)ï¼Œå°½ç®¡å› ä¸º gRPC æ˜¯ HTTP/2åŸºäºå®ƒåœ¨ TCP ä¹‹ä¸Šä¹Ÿæœ‰è‡ªå·±çš„æµé‡æ§åˆ¶ã€‚ [BDP çš„ä¸Šé™è¢«ç¡¬ç¼–ç åœ¨](https://github.com/grpc/grpc-go/issues/2400) [grpc-go to 16Mb](https://github.com/grpc/grpc-go/issues/2400)ï¼Œè¿™å¯èƒ½æˆä¸ºå•ä¸ªé«˜ BDP è¿æ¥çš„ç“¶é¢ˆã€‚

### Golangâ€™s net.Server vs grpc.Server

### Golang çš„ net.Server ä¸ grpc.Server

In our Go code we initially supported both HTTP/1.1 and gRPC using the same [net.Server](https://golang.org/pkg/net/http/#Server). This was logical from the code maintenance perspective but had suboptimal performance. Splitting HTTP/1.1 and gRPC paths to be processed by separate servers and switching gRPC to [grpc.Server](https://godoc.org/google.golang.org/grpc#Server) greatly improved throughput and memory usage of our Courier services.

åœ¨æˆ‘ä»¬çš„ Go ä»£ç ä¸­ï¼Œæˆ‘ä»¬æœ€åˆä½¿ç”¨ç›¸åŒçš„ [net.Server](https://golang.org/pkg/net/http/#Server) æ”¯æŒ HTTP/1.1 å’Œ gRPCã€‚ä»ä»£ç ç»´æŠ¤çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯åˆä¹é€»è¾‘çš„ï¼Œä½†æ€§èƒ½æ¬ ä½³ã€‚æ‹†åˆ† HTTP/1.1 å’Œ gRPC è·¯å¾„ç”±å•ç‹¬çš„æœåŠ¡å™¨å¤„ç†å¹¶å°† gRPC åˆ‡æ¢åˆ° [grpc.Server](https://godoc.org/google.golang.org/grpc#Server) å¤§å¤§æé«˜äº†æˆ‘ä»¬ Courier çš„ååé‡å’Œå†…å­˜ä½¿ç”¨ç‡æœåŠ¡ã€‚

### golang/protobuf vs gogo/protobuf

Marshaling and unmarshaling can be expensive when you switch to gRPC. For our Go code, weâ€™ve switched to [gogo/protobuf](https://github.com/gogo/protobuf) which noticeably decreased CPU usage on our busiest Courier servers.

å½“æ‚¨åˆ‡æ¢åˆ° gRPC æ—¶ï¼Œç¼–ç»„å’Œè§£ç»„å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚å¯¹äºæˆ‘ä»¬çš„ Go ä»£ç ï¼Œæˆ‘ä»¬å·²åˆ‡æ¢åˆ° [gogo/protobuf](https://github.com/gogo/protobuf)ï¼Œè¿™æ˜¾ç€é™ä½äº†æˆ‘ä»¬æœ€ç¹å¿™çš„ Courier æœåŠ¡å™¨ä¸Šçš„ CPU ä½¿ç”¨ç‡ã€‚

> As always,
>  [there are some caveats around using gogo/protobuf](https://jbrandhorst.com/post/gogoproto/), but if you stick to a sane subset of functionality you should be fine.

> ä¸€å¦‚æ—¢å¾€ï¼Œ
> [ä½¿ç”¨ gogo/protobuf æ—¶æœ‰ä¸€äº›æ³¨æ„äº‹é¡¹](https://jbrandhorst.com/post/gogoproto/)ï¼Œä½†å¦‚æœæ‚¨åšæŒä½¿ç”¨åˆç†çš„åŠŸèƒ½å­é›†ï¼Œæ‚¨åº”è¯¥æ²¡é—®é¢˜ã€‚

## Implementation details

## å®ç°ç»†èŠ‚

Starting from here, we are going to dig way deeper into the guts of Courier, looking at protobuf schemas and stub examples from different languages. For all the examples below we are going to use our `Test` service (the service we use in Courierâ€™s integration tests).

ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŒ–æ˜ Courier çš„å†…éƒ¨ç»“æ„ï¼ŒæŸ¥çœ‹æ¥è‡ªä¸åŒè¯­è¨€çš„ protobuf æ¨¡å¼å’Œå­˜æ ¹ç¤ºä¾‹ã€‚å¯¹äºä¸‹é¢çš„æ‰€æœ‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„â€œTestâ€æœåŠ¡ï¼ˆæˆ‘ä»¬åœ¨ Courier çš„é›†æˆæµ‹è¯•ä¸­ä½¿ç”¨çš„æœåŠ¡ï¼‰ã€‚

### Service description

###  æœåŠ¡è¯´æ˜

Letâ€™s look at the snippet from the `Test` service definition:

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹â€œTestâ€æœåŠ¡å®šä¹‰ä¸­çš„ç‰‡æ®µï¼š

```
service Test {
    option (rpc_core.service_default_deadline_ms) = 1000;

    rpc UnaryUnary(TestRequest) returns (TestResponse) {
        option (rpc_core.method_default_deadline_ms) = 5000;
    }

    rpc UnaryStream(TestRequest) returns (stream TestResponse) {
        option (rpc_core.method_no_deadline) = true;
    }
    ...
}

```


As was mentioned in the reliability section above, deadlines are mandatory for all Courier methods. They can be set for the whole service with the following protobuf option:

æ­£å¦‚ä¸Šé¢å¯é æ€§éƒ¨åˆ†æ‰€è¿°ï¼Œæ‰€æœ‰ Courier æ–¹æ³•éƒ½å¿…é¡»æœ‰æˆªæ­¢æ—¥æœŸã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ protobuf é€‰é¡¹ä¸ºæ•´ä¸ªæœåŠ¡è®¾ç½®å®ƒä»¬ï¼š

```
option (rpc_core.service_default_deadline_ms) = 1000;

```


Each method can also set its own deadline, overriding the service-wide one (if present).

æ¯ç§æ–¹æ³•è¿˜å¯ä»¥è®¾ç½®è‡ªå·±çš„æˆªæ­¢æ—¥æœŸï¼Œè¦†ç›–æœåŠ¡èŒƒå›´çš„æˆªæ­¢æ—¥æœŸï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚

```
option (rpc_core.method_default_deadline_ms) = 5000;

```


In rare cases where deadline doesnâ€™t really make sense (such as a method to watch some resource), the developer is allowed to explicitly disable it:

åœ¨æˆªæ­¢æ—¥æœŸæ²¡æœ‰çœŸæ­£æ„ä¹‰çš„æå°‘æ•°æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚è§‚å¯ŸæŸäº›èµ„æºçš„æ–¹æ³•ï¼‰ï¼Œå¼€å‘äººå‘˜å¯ä»¥æ˜ç¡®ç¦ç”¨å®ƒï¼š

```
option (rpc_core.method_no_deadline) = true;

```


The real service definition is also expected to have extensive API documentation, sometimes even along with usage examples.

çœŸæ­£çš„æœåŠ¡å®šä¹‰ä¹Ÿåº”è¯¥æœ‰å¤§é‡çš„ API æ–‡æ¡£ï¼Œæœ‰æ—¶ç”šè‡³è¿˜æœ‰ä½¿ç”¨ç¤ºä¾‹ã€‚

### Stub generation

### å­˜æ ¹ç”Ÿæˆ

Courier generates its own stubs instead of relying on interceptors (except for the Java case, where the interceptor API is powerful enough) mainly because it gives us more flexibility. Letâ€™s compare our stubs to the default ones using Golang as an example.

Courier ç”Ÿæˆè‡ªå·±çš„å­˜æ ¹è€Œä¸æ˜¯ä¾èµ–æ‹¦æˆªå™¨ï¼ˆJava æƒ…å†µé™¤å¤–ï¼Œå…¶ä¸­æ‹¦æˆªå™¨ API è¶³å¤Ÿå¼ºå¤§ï¼‰ä¸»è¦æ˜¯å› ä¸ºå®ƒä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚è®©æˆ‘ä»¬ä»¥ Golang ä¸ºä¾‹ï¼Œå°†æˆ‘ä»¬çš„å­˜æ ¹ä¸é»˜è®¤å­˜æ ¹è¿›è¡Œæ¯”è¾ƒã€‚

This is what default gRPC server stubs look like:

è¿™æ˜¯é»˜è®¤çš„ gRPC æœåŠ¡å™¨å­˜æ ¹çš„æ ·å­ï¼š

```
func _Test_UnaryUnary_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
        in := new(TestRequest)
        if err := dec(in);err != nil {
                return nil, err
        }
        if interceptor == nil {
                return srv.(TestServer).UnaryUnary(ctx, in)
        }
        info := &grpc.UnaryServerInfo{
                Server:     srv,
                FullMethod: "/test.Test/UnaryUnary",
        }
        handler := func(ctx context.Context, req interface{}) (interface{}, error) {
                return srv.(TestServer).UnaryUnary(ctx, req.(*TestRequest))
        }
        return interceptor(ctx, in, info, handler)
}

```


Here, all the processing happens inline: decoding the protobuf, running interceptors, and calling the `UnaryUnary` handler itself.

åœ¨è¿™é‡Œï¼Œæ‰€æœ‰å¤„ç†éƒ½å†…è”è¿›è¡Œï¼šè§£ç  protobufï¼Œè¿è¡Œæ‹¦æˆªå™¨ï¼Œå¹¶è°ƒç”¨ `UnaryUnary` å¤„ç†ç¨‹åºæœ¬èº«ã€‚

Now letâ€™s look at Courier stubs:

ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ Courier å­˜æ ¹ï¼š

```
func _Test_UnaryUnary_dbxHandler(
        srv interface{},
        ctx context.Context,
        dec func(interface{}) error,
        interceptor grpc.UnaryServerInterceptor) (
        interface{},
        error) {

        defer processor.PanicHandler()

        impl := srv.(*dbxTestServerImpl)
        metadata := impl.testUnaryUnaryMetadata

        ctx = metadata.SetupContext(ctx)
        clientId = client_info.ClientId(ctx)
        stats := metadata.StatsMap.GetOrCreatePerClientStats(clientId)
        stats.TotalCount.Inc()

        req := &processor.UnaryUnaryRequest{
                Srv:            srv,
                Ctx:            ctx,
                Dec:            dec,
                Interceptor:    interceptor,
                RpcStats:       stats,
                Metadata:       metadata,
                FullMethodPath: "/test.Test/UnaryUnary",
                Req:            &test.TestRequest{},
                Handler:        impl._UnaryUnary_internalHandler,
                ClientId:       clientId,
                EnqueueTime:    time.Now(),
        }

        metadata.WorkPool.Process(req).Wait()
        return req.Resp, req.Err
}

```


Thatâ€™s a lot of code, so letâ€™s go over it line by line.

è¿™æ˜¯å¾ˆå¤šä»£ç ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¸€è¡Œä¸€è¡Œåœ°æµè§ˆä¸€éã€‚

First, we defer the panic handler that is responsible for automatic error collection. This allows us to send all uncaught exceptions to centralized storage for later aggregation and reporting:

é¦–å…ˆï¼Œæˆ‘ä»¬æ¨è¿Ÿè´Ÿè´£è‡ªåŠ¨é”™è¯¯æ”¶é›†çš„ææ…Œå¤„ç†ç¨‹åºã€‚è¿™å…è®¸æˆ‘ä»¬å°†æ‰€æœ‰æœªæ•è·çš„å¼‚å¸¸å‘é€åˆ°é›†ä¸­å­˜å‚¨ï¼Œä»¥ä¾¿ä»¥åèšåˆå’ŒæŠ¥å‘Šï¼š

```
defer processor.PanicHandler()
```


> One more reason for setting up a custom panic handler is to ensure that we abort application on panic. Default golang/net HTTP handler behavior is to ignore it and continue serving new requests (with potentially corrupted and inconsistent state).

> è®¾ç½®è‡ªå®šä¹‰ææ…Œå¤„ç†ç¨‹åºçš„å¦ä¸€ä¸ªåŸå› æ˜¯ç¡®ä¿æˆ‘ä»¬åœ¨ææ…Œæ—¶ä¸­æ­¢åº”ç”¨ç¨‹åºã€‚é»˜è®¤çš„ golang/net HTTP å¤„ç†ç¨‹åºè¡Œä¸ºæ˜¯å¿½ç•¥å®ƒå¹¶ç»§ç»­æä¾›æ–°è¯·æ±‚ï¼ˆå¯èƒ½å­˜åœ¨æŸåå’Œä¸ä¸€è‡´çš„çŠ¶æ€ï¼‰ã€‚

Then we propagate context by overriding its values from the metadata of the incoming request:

ç„¶åæˆ‘ä»¬é€šè¿‡è¦†ç›–ä¼ å…¥è¯·æ±‚çš„å…ƒæ•°æ®ä¸­çš„å€¼æ¥ä¼ æ’­ä¸Šä¸‹æ–‡ï¼š

```
ctx = metadata.SetupContext(ctx)
clientId = client_info.ClientId(ctx)

```


We also create (and cache for efficiency purposes) the per-client stats on the server side for more granular attribution:

æˆ‘ä»¬è¿˜åœ¨æœåŠ¡å™¨ç«¯åˆ›å»ºï¼ˆå¹¶å‡ºäºæ•ˆç‡ç›®çš„ç¼“å­˜ï¼‰æ¯ä¸ªå®¢æˆ·ç«¯çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥è·å¾—æ›´ç²¾ç»†çš„å½’å› ï¼š

```
stats := metadata.StatsMap.GetOrCreatePerClientStats(clientId)

```


> This dynamically creates a per-client (i.e. per-TLS identity) stats in runtime. We also have per-method stats for each service and, since the stub generator has access to all the methods during the code generation time, we can statically pre-create these to avoid runtime overhead.

> è¿™ä¼šåœ¨è¿è¡Œæ—¶åŠ¨æ€åˆ›å»ºæ¯ä¸ªå®¢æˆ·ç«¯ï¼ˆå³æ¯ä¸ª TLS èº«ä»½ï¼‰çš„ç»Ÿè®¡ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜æœ‰æ¯ä¸ªæœåŠ¡çš„æ¯ä¸ªæ–¹æ³•çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶ä¸”ç”±äºå­˜æ ¹ç”Ÿæˆå™¨å¯ä»¥åœ¨ä»£ç ç”ŸæˆæœŸé—´è®¿é—®æ‰€æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥é™æ€åœ°é¢„åˆ›å»ºè¿™äº›ä»¥é¿å…è¿è¡Œæ—¶å¼€é”€ã€‚

Then we create the request structure, pass it to the work pool, and wait for the completion:

ç„¶åæˆ‘ä»¬åˆ›å»ºè¯·æ±‚ç»“æ„ï¼Œä¼ é€’ç»™å·¥ä½œæ± ï¼Œç­‰å¾…å®Œæˆï¼š

```
req := &processor.UnaryUnaryRequest{
        Srv:            srv,
        Ctx:            ctx,
        Dec:            dec,
        Interceptor:    interceptor,
        RpcStats:       stats,
        Metadata:       metadata,
        ...
}
metadata.WorkPool.Process(req).Wait()

```


Note that almost no work has been done by this point: no protobuf decoding, no interceptor execution, etc. ACL enforcement, prioritization, and rate-limiting happens inside the workpool before any of that is done.

è¯·æ³¨æ„ï¼Œæ­¤æ—¶å‡ ä¹æ²¡æœ‰å®Œæˆä»»ä½•å·¥ä½œï¼šæ²¡æœ‰ protobuf è§£ç ã€æ²¡æœ‰æ‹¦æˆªå™¨æ‰§è¡Œç­‰ã€‚ACL å®æ–½ã€ä¼˜å…ˆçº§åˆ’åˆ†å’Œé€Ÿç‡é™åˆ¶åœ¨ä»»ä½•è¿™äº›å®Œæˆä¹‹å‰åœ¨å·¥ä½œæ± å†…å‘ç”Ÿã€‚

> Note that the
>  [golang gRPC library supports](https://godoc.org/google.golang.org/grpc/tap)[the](https://godoc.org/google.golang.org/grpc/tap) [Tap interface](https://godoc.org/google.golang.org/grpc/tap), which allows very early request interception. This provides infrastructure for building efficient rate-limiters with minimal overhead.

> è¯·æ³¨æ„ï¼Œ
> [golang gRPC åº“æ”¯æŒ](https://godoc.org/google.golang.org/grpc/tap)[the](https://godoc.org/google.golang.org/grpc/tap) [Tap interface](https://godoc.org/google.golang.org/grpc/tap)ï¼Œå®ƒå…è®¸éå¸¸æ—©çš„è¯·æ±‚æ‹¦æˆªã€‚è¿™ä¸ºä»¥æœ€å°çš„å¼€é”€æ„å»ºé«˜æ•ˆçš„é€Ÿç‡é™åˆ¶å™¨æä¾›äº†åŸºç¡€è®¾æ–½ã€‚

### App-specific error codes

### ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„é”™è¯¯ä»£ç 

Our stub generator also allows developers to define app-specific error codes through custom options:

æˆ‘ä»¬çš„å­˜æ ¹ç”Ÿæˆå™¨è¿˜å…è®¸å¼€å‘äººå‘˜é€šè¿‡è‡ªå®šä¹‰é€‰é¡¹å®šä¹‰ç‰¹å®šäºåº”ç”¨ç¨‹åºçš„é”™è¯¯ä»£ç ï¼š

```
enum ErrorCode {
option (rpc_core.rpc_error) = true;

UNKNOWN = 0;
NOT_FOUND = 1 [(rpc_core.grpc_code)="NOT_FOUND"];
ALREADY_EXISTS = 2 [(rpc_core.grpc_code)="ALREADY_EXISTS"];
...
STALE_READ = 7 [(rpc_core.grpc_code)="UNAVAILABLE"];
SHUTTING_DOWN = 8 [(rpc_core.grpc_code)="CANCELLED"];
}

```


Within the same service, both gRPC and app errors are propagated, while between API boundaries all errors are replaced with UNKNOWN. This avoids the problem of accidental error proxying between different services, potentially changing their semantic meaning.

åœ¨åŒä¸€ä¸ªæœåŠ¡ä¸­ï¼ŒgRPC å’Œåº”ç”¨ç¨‹åºé”™è¯¯éƒ½ä¼šä¼ æ’­ï¼Œè€Œåœ¨ API è¾¹ç•Œä¹‹é—´ï¼Œæ‰€æœ‰é”™è¯¯éƒ½è¢«æ›¿æ¢ä¸º UNKNOWNã€‚è¿™é¿å…äº†ä¸åŒæœåŠ¡ä¹‹é—´çš„æ„å¤–é”™è¯¯ä»£ç†é—®é¢˜ï¼Œå¯èƒ½ä¼šæ”¹å˜å®ƒä»¬çš„è¯­ä¹‰ã€‚

### Python-specific changes

### ç‰¹å®šäº Python çš„æ›´æ”¹

Our Python stubs add an explicit context parameter to all Courier handlers, e.g.:

æˆ‘ä»¬çš„ Python å­˜æ ¹ä¸ºæ‰€æœ‰ Courier å¤„ç†ç¨‹åºæ·»åŠ äº†ä¸€ä¸ªæ˜ç¡®çš„ä¸Šä¸‹æ–‡å‚æ•°ï¼Œä¾‹å¦‚ï¼š

```
from dropbox.context import Context
from dropbox.proto.test.service_pb2 import (
        TestRequest,
        TestResponse,
)
from typing_extensions import Protocol

class TestCourierClient(Protocol):
    def UnaryUnary(
            self,
            ctx,      # type: Context
            request,  # type: TestRequest
            ):
        # type: (...) -> TestResponse
        ...

```


At first, it looked a bit strange, but after some time developers got used to the explicit `ctx` just as they got used to `self`.

èµ·åˆï¼Œå®ƒçœ‹èµ·æ¥æœ‰ç‚¹å¥‡æ€ªï¼Œä½†ä¸€æ®µæ—¶é—´åï¼Œå¼€å‘äººå‘˜ä¹ æƒ¯äº†æ˜¾å¼çš„ `ctx`ï¼Œå°±åƒä»–ä»¬ä¹ æƒ¯äº† `self` ä¸€æ ·ã€‚

Note that our stubs are also fully mypy-typed which pays off in full during large-scale refactoring. It also integrates nicely with some IDEs like PyCharm.

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„å­˜æ ¹ä¹Ÿæ˜¯å®Œå…¨ mypy ç±»å‹çš„ï¼Œè¿™åœ¨å¤§è§„æ¨¡é‡æ„æœŸé—´ä¼šå®Œå…¨å¾—åˆ°å›æŠ¥ã€‚å®ƒè¿˜ä¸ PyCharm ç­‰ä¸€äº› IDE å¾ˆå¥½åœ°é›†æˆã€‚

Continuing the static typing trend, we also add mypy annotations to protos themselves:

å»¶ç»­é™æ€ç±»å‹çš„è¶‹åŠ¿ï¼Œæˆ‘ä»¬è¿˜å‘ protos æœ¬èº«æ·»åŠ äº† mypy æ³¨é‡Šï¼š

```
class TestMessage(Message):
    field: int

    def __init__(self,
        field : Optional[int] = ...,
        ) -> None: ...
    @staticmethod
    def FromString(s: bytes) -> TestMessage: ...

```


These annotations prevent many common bugs, such as assigning `None` to a `string` field in Python.

è¿™äº›æ³¨è§£é˜²æ­¢äº†è®¸å¤šå¸¸è§çš„é”™è¯¯ï¼Œä¾‹å¦‚åœ¨ Python ä¸­å°† `None` åˆ†é…ç»™ `string` å­—æ®µã€‚

This code is opensourced at [dropbox/mypy-protobuf](https://github.com/dropbox/mypy-protobuf).

æ­¤ä»£ç åœ¨ [dropbox/mypy-protobuf](https://github.com/dropbox/mypy-protobuf) ä¸Šå¼€æºã€‚

## Migration process 

## è¿ç§»è¿‡ç¨‹

Writing a new RPC stack is by no means an easy task, but in terms of operational complexity it still canâ€™t be compared to the process of infra-wide migration to it. To assure the success of this project, weâ€™ve tried to make it easier for the developers to migrate from legacy RPC to Courier. Since the migration by itself is a very error-prone process, weâ€™ve decided to go with a multi-step process.

ç¼–å†™ä¸€ä¸ªæ–°çš„ RPC å †æ ˆç»ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹ï¼Œä½†åœ¨æ“ä½œå¤æ‚æ€§æ–¹é¢ä»ç„¶æ— æ³•ä¸å‘å®ƒè¿›è¡ŒèŒƒå›´å†…è¿ç§»çš„è¿‡ç¨‹ç›¸æå¹¶è®ºã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸ªé¡¹ç›®çš„æˆåŠŸï¼Œæˆ‘ä»¬è¯•å›¾è®©å¼€å‘äººå‘˜æ›´å®¹æ˜“ä»ä¼ ç»Ÿçš„ RPC è¿ç§»åˆ° Courierã€‚ç”±äºè¿ç§»æœ¬èº«æ˜¯ä¸€ä¸ªéå¸¸å®¹æ˜“å‡ºé”™çš„è¿‡ç¨‹ï¼Œå› æ­¤æˆ‘ä»¬å†³å®šé‡‡ç”¨å¤šæ­¥éª¤è¿‡ç¨‹ã€‚

### Step 0: Freeze the legacy RPC

### æ­¥éª¤ 0ï¼šå†»ç»“æ—§ç‰ˆ RPC

Before we did anything, we froze the legacy RPC feature set so itâ€™s no longer a moving target. This also gave people an incentive to move to Courier, since all new features like tracing and streaming were only available to services using Courier.

åœ¨æˆ‘ä»¬åšä»»ä½•äº‹æƒ…ä¹‹å‰ï¼Œæˆ‘ä»¬å†»ç»“äº†é—ç•™çš„ RPC åŠŸèƒ½é›†ï¼Œå› æ­¤å®ƒä¸å†æ˜¯ä¸€ä¸ªç§»åŠ¨ç›®æ ‡ã€‚è¿™ä¹Ÿæ¿€å‘äº†äººä»¬è½¬å‘ Courier çš„åŠ¨åŠ›ï¼Œå› ä¸ºæ‰€æœ‰æ–°åŠŸèƒ½ï¼ˆå¦‚è·Ÿè¸ªå’Œæµåª’ä½“ï¼‰ä»…é€‚ç”¨äºä½¿ç”¨ Courier çš„æœåŠ¡ã€‚

### Step 1: A common interface for the legacy RPC and Courier

### ç¬¬ 1 æ­¥ï¼šæ—§ç‰ˆ RPC å’Œ Courier çš„é€šç”¨æ¥å£

We started by defining a common interface for both legacy RPC and Courier. Our code generation was responsible for producing both versions of the stubs that satisfy this interface:

æˆ‘ä»¬é¦–å…ˆä¸ºæ—§ç‰ˆ RPC å’Œ Courier å®šä¹‰äº†ä¸€ä¸ªé€šç”¨æ¥å£ã€‚æˆ‘ä»¬çš„ä»£ç ç”Ÿæˆè´Ÿè´£ç”Ÿæˆæ»¡è¶³æ­¤æ¥å£çš„å­˜æ ¹çš„ä¸¤ä¸ªç‰ˆæœ¬ï¼š

```
type TestServer interface {
UnaryUnary(
      ctx context.Context,
      req *test.TestRequest) (
      *test.TestResponse,
      error)
...
}

```


### Step 2: Migration to the new interface

### ç¬¬ 2 æ­¥ï¼šè¿ç§»åˆ°æ–°ç•Œé¢

Then we started switching each service to the new interface but continued using legacy RPC. This was often a huge diff touching all the methods in the service and its clients. Since this is the most error-prone step, we wanted to de-risk it as much as possible by changing one variable at a time.

ç„¶åæˆ‘ä»¬å¼€å§‹å°†æ¯ä¸ªæœåŠ¡åˆ‡æ¢åˆ°æ–°æ¥å£ï¼Œä½†ç»§ç»­ä½¿ç”¨æ—§ç‰ˆ RPCã€‚è¿™é€šå¸¸æ˜¯æ¶‰åŠæœåŠ¡åŠå…¶å®¢æˆ·ä¸­æ‰€æœ‰æ–¹æ³•çš„å·¨å¤§å·®å¼‚ã€‚ç”±äºè¿™æ˜¯æœ€å®¹æ˜“å‡ºé”™çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡ä¸€æ¬¡æ›´æ”¹ä¸€ä¸ªå˜é‡æ¥å°½å¯èƒ½é™ä½é£é™©ã€‚

> Low profile services with a small number of methods and [spare error budget](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) can do the migration in a single step and ignore this warning.

> å…·æœ‰å°‘é‡æ–¹æ³•çš„ä½è°ƒæœåŠ¡å’Œ[å¤‡ç”¨é”™è¯¯é¢„ç®—](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) å¯ä»¥ä¸€æ­¥å®Œæˆè¿ç§»å¹¶å¿½ç•¥æ­¤è­¦å‘Šã€‚

### Step 3: Switch clients to use Courier RPC

### ç¬¬ 3 æ­¥ï¼šåˆ‡æ¢å®¢æˆ·ç«¯ä»¥ä½¿ç”¨ Courier RPC

As part of the Courier migration, we also started running both legacy and Courier servers in the same binary on different ports. Now changing the RPC implementation is a one-line diff to the client:

ä½œä¸º Courier è¿ç§»çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¿˜å¼€å§‹åœ¨ä¸åŒç«¯å£ä¸Šä»¥ç›¸åŒçš„äºŒè¿›åˆ¶æ–‡ä»¶è¿è¡Œæ—§æœåŠ¡å™¨å’Œ Courier æœåŠ¡å™¨ã€‚ç°åœ¨æ›´æ”¹ RPC å®ç°æ˜¯å¯¹å®¢æˆ·ç«¯çš„ä¸€è¡Œå·®å¼‚ï¼š

```
class MyClient(object):
def __init__(self):
-   self.client = LegacyRPCClient('myservice')
+   self.client = CourierRPCClient('myservice')

```


Note that using that model we can migrate one client at a time, starting with ones that have lower SLAs like batch processing and other async jobs.

è¯·æ³¨æ„ï¼Œä½¿ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ¬¡è¿ç§»ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œä»å…·æœ‰è¾ƒä½ SLA çš„å®¢æˆ·ç«¯å¼€å§‹ï¼Œä¾‹å¦‚æ‰¹å¤„ç†å’Œå…¶ä»–å¼‚æ­¥ä½œä¸šã€‚

### Step 4: Clean up

### ç¬¬ 4 æ­¥ï¼šæ¸…ç†

After all service clients have migrated it is time to prove that legacy RPC is not used anymore (this can be done statically by code inspection and at runtime looking at legacy server stats.) After this step is done developers can proceed to clean up and remove old code.

åœ¨æ‰€æœ‰æœåŠ¡å®¢æˆ·ç«¯è¿ç§»åï¼Œæ˜¯æ—¶å€™è¯æ˜ä¸å†ä½¿ç”¨æ—§ç‰ˆ RPCï¼ˆè¿™å¯ä»¥é€šè¿‡ä»£ç æ£€æŸ¥é™æ€å®Œæˆï¼Œå¹¶åœ¨è¿è¡Œæ—¶æŸ¥çœ‹æ—§ç‰ˆæœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯ã€‚ï¼‰å®Œæˆæ­¤æ­¥éª¤åï¼Œå¼€å‘äººå‘˜å¯ä»¥ç»§ç»­æ¸…ç†å’Œåˆ é™¤æ—§ä»£ç ã€‚

## Lessons learned

##  å¾—åˆ°æ•™è®­

At the end of the day, what Courier brings to the table is a unified RPC framework that speeds up service development, simplifies operations, and improves Dropbox reliability.

å½’æ ¹ç»“åº•ï¼ŒCourier å¸¦æ¥çš„æ˜¯ç»Ÿä¸€çš„ RPC æ¡†æ¶ï¼Œå®ƒå¯ä»¥åŠ å¿«æœåŠ¡å¼€å‘ã€ç®€åŒ–æ“ä½œå¹¶æé«˜ Dropbox çš„å¯é æ€§ã€‚

Here are the main lessons weâ€™ve learned during the Courier development and deployment:

ä»¥ä¸‹æ˜¯æˆ‘ä»¬åœ¨ Courier å¼€å‘å’Œéƒ¨ç½²è¿‡ç¨‹ä¸­å­¦åˆ°çš„ä¸»è¦ç»éªŒæ•™è®­ï¼š

1. Observability is a feature. Having all the metrics and breakdowns out-of-the-box is invaluable during troubleshooting.
2. Standardization and uniformity are important. They lower cognitive load, and simplify operations and code maintenance.
3. Try to minimize the amount of boilerplate code developers need to write. Codegen is your friend here.
4. Make migration as easy as possible. Migration will likely take way more time than the development itself. Also, migration is only finished after cleanup is performed.
5. RPC framework can be a place to add infrastructure-wide reliability improvements, e.g. mandatory deadlines, overload protection, etc. Common reliability issues can be identified by aggregating incident reports on a quarterly basis.

1. å¯è§‚å¯Ÿæ€§æ˜¯ä¸€ä¸ªç‰¹å¾ã€‚åœ¨æ•…éšœæ’é™¤æœŸé—´ï¼Œå¼€ç®±å³ç”¨çš„æ‰€æœ‰æŒ‡æ ‡å’Œæ•…éšœæ˜¯éå¸¸å®è´µçš„ã€‚
2. æ ‡å‡†åŒ–å’Œç»Ÿä¸€æ€§å¾ˆé‡è¦ã€‚å®ƒä»¬é™ä½äº†è®¤çŸ¥è´Ÿæ‹…ï¼Œå¹¶ç®€åŒ–äº†æ“ä½œå’Œä»£ç ç»´æŠ¤ã€‚
3. å°½é‡å‡å°‘å¼€å‘äººå‘˜éœ€è¦ç¼–å†™çš„æ ·æ¿ä»£ç é‡ã€‚ Codegen æ˜¯æ‚¨çš„æœ‹å‹ã€‚
4. ä½¿è¿ç§»å°½å¯èƒ½å®¹æ˜“ã€‚è¿ç§»å¯èƒ½æ¯”å¼€å‘æœ¬èº«èŠ±è´¹æ›´å¤šçš„æ—¶é—´ã€‚æ­¤å¤–ï¼Œè¿ç§»ä»…åœ¨æ‰§è¡Œæ¸…ç†åå®Œæˆã€‚
5. RPC æ¡†æ¶å¯ä»¥ä½œä¸ºæ·»åŠ åŸºç¡€è®¾æ–½èŒƒå›´å¯é æ€§æ”¹è¿›çš„åœ°æ–¹ï¼Œä¾‹å¦‚å¼ºåˆ¶æœŸé™ã€è¿‡è½½ä¿æŠ¤ç­‰ã€‚å¯ä»¥é€šè¿‡æŒ‰å­£åº¦æ±‡æ€»äº‹ä»¶æŠ¥å‘Šæ¥ç¡®å®šå¸¸è§çš„å¯é æ€§é—®é¢˜ã€‚

## Future Work

##  æœªæ¥çš„å·¥ä½œ

Courier, as well as gRPC itself, is a moving target so letâ€™s wrap up with the Runtime team and Reliability teamsâ€™ roadmaps. 

Courier ä»¥åŠ gRPC æœ¬èº«éƒ½æ˜¯ä¸€ä¸ªä¸æ–­å˜åŒ–çš„ç›®æ ‡ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æ€»ç»“ä¸€ä¸‹è¿è¡Œæ—¶å›¢é˜Ÿå’Œå¯é æ€§å›¢é˜Ÿçš„è·¯çº¿å›¾ã€‚

In relatively near future we wanted to add a proper resolver API to Pythonâ€™s gRPC code, switch to C++ bindings in Python/Rust, and add full circuit breaking and fault injection support. Later next year we are planning on looking into [ALTS and moving TLS handshake to a separate process](https://cloud.google.com/security/encryption-in-transit/application-layer-transport-security/resources/alts-whitepaper.pdf) (possibly even outside of the services' container.)

åœ¨ç›¸å¯¹ä¸ä¹…çš„å°†æ¥ï¼Œæˆ‘ä»¬å¸Œæœ›å‘ Python çš„ gRPC ä»£ç æ·»åŠ é€‚å½“çš„è§£æå™¨ APIï¼Œåœ¨ Python/Rust ä¸­åˆ‡æ¢åˆ° C++ ç»‘å®šï¼Œå¹¶æ·»åŠ å®Œæ•´çš„æ–­è·¯å’Œæ•…éšœæ³¨å…¥æ”¯æŒã€‚æ˜å¹´æ™šäº›æ—¶å€™ï¼Œæˆ‘ä»¬è®¡åˆ’ç ”ç©¶ [ALTS å¹¶å°† TLS æ¡æ‰‹ç§»è‡³å•ç‹¬çš„è¿›ç¨‹](https://cloud.google.com/security/encryption-in-transit/application-layer-transport-security/resources/alts-whitepaper.pdf)ï¼ˆç”šè‡³å¯èƒ½åœ¨æœåŠ¡çš„å®¹å™¨ä¹‹å¤–ã€‚)

## We are hiring!

##  æˆ‘ä»¬æ­£åœ¨æ‹›è˜ï¼

Do you like runtime-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and comfy small teams in both Mountain View and San Francisco.

ä½ å–œæ¬¢è¿è¡Œæ—¶ç›¸å…³çš„ä¸œè¥¿å—ï¼Ÿ Dropbox æ‹¥æœ‰éå¸ƒå…¨çƒçš„è¾¹ç¼˜ç½‘ç»œã€TB çº§çš„æµé‡ã€æ¯ç§’æ•°ç™¾ä¸‡çš„è¯·æ±‚ï¼Œä»¥åŠåœ¨å±±æ™¯åŸå’Œæ—§é‡‘å±±çš„èˆ’é€‚å°å›¢é˜Ÿã€‚

![](http://dropbox.tech/cms/content/dam/dropbox/tech-blog/en-us/2019/01/09-screenshot2018-10-0318.04.58.png)

[Traffic/Runtime/Reliability teams are hiring both SWEs and SREs](https://www.dropbox.com/jobs/listing/1233364?gh_src=f80311fa1) to work on TCP/IP packet processors and load balancers, HTTP/gRPC proxies, and our internal service mesh runtime: Courier/gRPC, Service Discovery, and AFS. Not your thing? We're also hiring for [a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world](https://www.dropbox.com/jobs/teams/engineering?gh_src=f80311fa1#open-positions).

[æµé‡/è¿è¡Œæ—¶/å¯é æ€§å›¢é˜Ÿæ­£åœ¨æ‹›è˜ SWE å’Œ SRE](https://www.dropbox.com/jobs/listing/1233364?gh_src=f80311fa1) ä»äº‹ TCP/IP æ•°æ®åŒ…å¤„ç†å™¨å’Œè´Ÿè½½å‡è¡¡å™¨ã€HTTP/gRPCä»£ç†å’Œæˆ‘ä»¬çš„å†…éƒ¨æœåŠ¡ç½‘æ ¼è¿è¡Œæ—¶ï¼šCourier/gRPCã€æœåŠ¡å‘ç°å’Œ AFSã€‚ä¸æ˜¯ä½ çš„ä¸œè¥¿ï¼Ÿæˆ‘ä»¬è¿˜åœ¨æ‹›è˜ [æ—§é‡‘å±±ã€çº½çº¦ã€è¥¿é›…å›¾ã€ç‰¹æ‹‰ç»´å¤«å’Œä¸–ç•Œå„åœ°å…¶ä»–åŠäº‹å¤„çš„å„ç§å·¥ç¨‹èŒä½](https://www.dropbox.com/jobs/teams/engineering?gh_src=f80311fa1#open-positions)ã€‚

### Acknowledgments

### è‡´è°¢

**Contributors:** Ashwin Amit, Can Berk Guder, Dave Zbarsky, Giang Nguyen, Mehrdad Afshari, Patrick Lee, Ross Delinger, Ruslan Nigmatullin, Russ Allbery, Santosh Ananthakrishnan.

**è´¡çŒ®è€…ï¼š** Ashwin Amitã€Can Berk Guderã€Dave Zbarskyã€Giang Nguyenã€Mehrdad Afshariã€Patrick Leeã€Ross Delingerã€Ruslan Nigmatullinã€Russ Allberyã€Santosh Ananthakrishnanã€‚

We are also very grateful to the gRPC team for their support. 

æˆ‘ä»¬ä¹Ÿéå¸¸æ„Ÿè°¢ gRPC å›¢é˜Ÿçš„æ”¯æŒã€‚

