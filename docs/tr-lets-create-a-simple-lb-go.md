# Let's Create a Simple Load Balancer With Go

# è®©æˆ‘ä»¬ç”¨ Go åˆ›å»ºä¸€ä¸ªç®€å•çš„è´Ÿè½½å‡è¡¡å™¨

Posted 8. November 2019.   **10 min read.**

Load Balancers plays a key role in Web Architecture. They allow distributing load among a set of backends. This makes services more scalable. Also since there are multiple backends configured the service become  highly available as load balancer can pick up a working server in case  of a failure.

è´Ÿè½½å‡è¡¡å™¨åœ¨ Web æ¶æ„ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚å®ƒä»¬å…è®¸åœ¨ä¸€ç»„åç«¯ä¹‹é—´åˆ†é…è´Ÿè½½ã€‚è¿™ä½¿å¾—æœåŠ¡æ›´å…·å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œç”±äºé…ç½®äº†å¤šä¸ªåç«¯ï¼ŒæœåŠ¡å˜å¾—é«˜åº¦å¯ç”¨ï¼Œå› ä¸ºè´Ÿè½½å‡è¡¡å™¨å¯ä»¥åœ¨å‡ºç°æ•…éšœæ—¶é€‰æ‹©å·¥ä½œæœåŠ¡å™¨ã€‚

After playing with professional Load Balancers like [NGINX](https://www.nginx.com/) I tried creating a simple Load Balancer for fun. I implemented it using [Golang](https://golang.org/). Go is a modern language which supports concurrency as a first-class  citizen. Go has a rich standard library which allows writing high-performance  applications with fewer lines of codes. It also produces a statically  linked single binary for easy distributions.

åœ¨ç©è¿‡åƒ [NGINX](https://www.nginx.com/) è¿™æ ·çš„ä¸“ä¸šè´Ÿè½½å‡è¡¡å™¨ä¹‹åï¼Œæˆ‘å°è¯•åˆ›å»ºä¸€ä¸ªç®€å•çš„è´Ÿè½½å‡è¡¡å™¨æ¥è·å¾—ä¹è¶£ã€‚æˆ‘ä½¿ç”¨ [Golang](https://golang.org/) å®ç°äº†å®ƒã€‚ Go æ˜¯ä¸€ç§ç°ä»£è¯­è¨€ï¼Œå®ƒä½œä¸ºä¸€ç­‰å…¬æ°‘æ”¯æŒå¹¶å‘ã€‚ Go æœ‰ä¸€ä¸ªä¸°å¯Œçš„æ ‡å‡†åº“ï¼Œå®ƒå…è®¸ç”¨æ›´å°‘çš„ä»£ç è¡Œç¼–å†™é«˜æ€§èƒ½åº”ç”¨ç¨‹åºã€‚å®ƒè¿˜ç”Ÿæˆä¸€ä¸ªé™æ€é“¾æ¥çš„å•ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä»¥ä¾¿äºåˆ†å‘ã€‚

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#how-does-our-simple-load-balancer-work)How does our simple load balancer work

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#how-does-our-simple-load-balancer-work)æˆ‘ä»¬çš„ç®€å•è´Ÿè½½å‡è¡¡å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„

Load Balancers have different strategies for distributing the load across a set of backends.

è´Ÿè½½å‡è¡¡å™¨æœ‰ä¸åŒçš„ç­–ç•¥æ¥è·¨ä¸€ç»„åç«¯åˆ†é…è´Ÿè½½ã€‚

For example,

ä¾‹å¦‚ï¼Œ

- **Round Robin** - Distribute load equally, assumes all backends have the same processing power
- **Weighted Round Robin** - Additional weights can be given considering the backend's processing power
- **Least Connections** - Load is distributed to the servers with least active connections

- **Round Robin** - å¹³å‡åˆ†é…è´Ÿè½½ï¼Œå‡è®¾æ‰€æœ‰åç«¯å…·æœ‰ç›¸åŒçš„å¤„ç†èƒ½åŠ›
- **åŠ æƒå¾ªç¯** - è€ƒè™‘åˆ°åç«¯çš„å¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥ç»™äºˆé¢å¤–çš„æƒé‡
- **æœ€å°‘è¿æ¥** - è´Ÿè½½åˆ†é…åˆ°æ´»åŠ¨è¿æ¥æœ€å°‘çš„æœåŠ¡å™¨

For our simple load balancer, we would try implementing the simplest one among these methods, **Round Robin**.

å¯¹äºæˆ‘ä»¬çš„ç®€å•è´Ÿè½½å‡è¡¡å™¨ï¼Œæˆ‘ä»¬å°†å°è¯•å®ç°è¿™äº›æ–¹æ³•ä¸­æœ€ç®€å•çš„ä¸€ç§ï¼Œ**Round Robin**ã€‚



![A Round Robin Load Balancer](https://kasvith.me/assets/static/lb-archi.0b1c2c4.b3e35c7510dc44451088756d14739161.png)A Round Robin Load Balancer



## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#round-robin-selection)Round Robin Selection

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#round-robin-selection)å¾ªç¯é€‰æ‹©

Round Robin is simple in terms. It gives equal opportunities for workers to perform tasks in turns.

Round Robin çš„æœ¯è¯­å¾ˆç®€å•ã€‚å®ƒä¸ºå·¥äººæä¾›äº†è½®æµæ‰§è¡Œä»»åŠ¡çš„å¹³ç­‰æœºä¼šã€‚



![Round Robin Selection on incoming requests](https://kasvith.me/assets/static/lb-rr.a901b4a.3b34a78610b7c1e2d22b85f0419700d2.png)Round Robin Selection on incoming requests



As shown in the figure about this happens cyclically. But we can't *directly* use that aren't we?

å¦‚å›¾æ‰€ç¤ºï¼Œè¿™ç§æƒ…å†µæ˜¯å‘¨æœŸæ€§å‘ç”Ÿçš„ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸èƒ½*ç›´æ¥*ä½¿ç”¨ï¼Œä¸æ˜¯å—ï¼Ÿ

**What if a backend is down?** We probably don't want to route traffic there. So this cannot be directly used unless we put some conditions on it. We need to **route traffic only to backends which are up and running**.

**å¦‚æœåç«¯å‡ºç°æ•…éšœæ€ä¹ˆåŠï¼Ÿ** æˆ‘ä»¬å¯èƒ½ä¸æƒ³å°†æµé‡è·¯ç”±åˆ°é‚£é‡Œã€‚æ‰€ä»¥è¿™ä¸ªä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œé™¤éæˆ‘ä»¬ç»™å®ƒåŠ ä¸€äº›æ¡ä»¶ã€‚æˆ‘ä»¬éœ€è¦**ä»…å°†æµé‡è·¯ç”±åˆ°å·²å¯åŠ¨å¹¶æ­£åœ¨è¿è¡Œçš„åç«¯**ã€‚

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#lets-define-some-structs)Lets define some structs

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#lets-define-some-structs)è®©æˆ‘ä»¬å®šä¹‰ä¸€äº›ç»“æ„

After revising the plan, we know now we want a way to track all the details about a Backend. We need to track whether it's alive or dead and also keep track of the Url as well.

ä¿®æ”¹è®¡åˆ’åï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è·Ÿè¸ªæœ‰å…³åç«¯çš„æ‰€æœ‰è¯¦ç»†ä¿¡æ¯ã€‚æˆ‘ä»¬éœ€è¦è·Ÿè¸ªå®ƒæ˜¯æ´»ç€è¿˜æ˜¯æ­»äº†ï¼Œè¿˜éœ€è¦è·Ÿè¸ª Urlã€‚

We can simply define a struct like this to hold our backends.

æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å®šä¹‰ä¸€ä¸ªåƒè¿™æ ·çš„ç»“æ„æ¥ä¿å­˜æˆ‘ä»¬çš„åç«¯ã€‚

```go
type Backend struct {
  URL          *url.URL
  Alive        bool
  mux          sync.RWMutex
  ReverseProxy *httputil.ReverseProxy
}
```


Don't worry **I will reason about the fields in the `Backend`**.

åˆ«æ‹…å¿ƒ **æˆ‘ä¼šæ¨ç† `Backend` ä¸­çš„å­—æ®µ**ã€‚

Now we need a way to track all the backends in our load balancer, for that we can simply use a Slice. And also a counter variable. We can define it as **`ServerPool`**

ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è·Ÿè¸ªè´Ÿè½½å‡è¡¡å™¨ä¸­çš„æ‰€æœ‰åç«¯ï¼Œä¸ºæ­¤æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨ Sliceã€‚è¿˜æœ‰ä¸€ä¸ªè®¡æ•°å™¨å˜é‡ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶å®šä¹‰ä¸º **`ServerPool`**

```go
type ServerPool struct {
  backends []*Backend
  current  uint64
}
```


## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#use-of-the-reverseproxy)Use of the ReverseProxy

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#use-of-the-reverseproxy)ReverseProxyçš„ä½¿ç”¨

As we already identified, the sole purpose of the load balancer is to route traffic to different backends and return the results to the  original client.

æ­£å¦‚æˆ‘ä»¬å·²ç»ç¡®å®šçš„ï¼Œè´Ÿè½½å‡è¡¡å™¨çš„å”¯ä¸€ç›®çš„æ˜¯å°†æµé‡è·¯ç”±åˆ°ä¸åŒçš„åç«¯å¹¶å°†ç»“æœè¿”å›ç»™åŸå§‹å®¢æˆ·ç«¯ã€‚

According to Go's documentation,

æ ¹æ® Go çš„æ–‡æ¡£ï¼Œ

> ReverseProxy is an HTTP Handler that takes an incoming request and  sends it to another server, proxying the response back to the client.

> ReverseProxy æ˜¯ä¸€ä¸ª HTTP å¤„ç†ç¨‹åºï¼Œå®ƒæ¥æ”¶ä¼ å…¥çš„è¯·æ±‚å¹¶å°†å…¶å‘é€åˆ°å¦ä¸€å°æœåŠ¡å™¨ï¼Œå°†å“åº”ä»£ç†å›å®¢æˆ·ç«¯ã€‚

**Which is exactly what we want**. There is no need to reinvent the wheel. We can simply relay our original requests through the `ReverseProxy`.

**è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„**ã€‚æ²¡æœ‰å¿…è¦é‡æ–°å‘æ˜è½®å­ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°é€šè¿‡ `ReverseProxy` ä¸­ç»§æˆ‘ä»¬çš„åŸå§‹è¯·æ±‚ã€‚

```go
u, _ := url.Parse("http://localhost:8080")
rp := httputil.NewSingleHostReverseProxy(u)
  
// initialize your server and add this as handler
http.HandlerFunc(rp.ServeHTTP)
```


With `httputil.NewSingleHostReverseProxy(url)` we can initialize a reverse proxy which would relay requests to the passed `url`. In the above example, all the requests are now passed to localhost:8080 and the results are sent back to the original client. You can find more examples here.

ä½¿ç”¨`httputil.NewSingleHostReverseProxy(url)`ï¼Œæˆ‘ä»¬å¯ä»¥åˆå§‹åŒ–ä¸€ä¸ªåå‘ä»£ç†ï¼Œå®ƒå°†è¯·æ±‚ä¸­ç»§åˆ°ä¼ é€’çš„`url`ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç°åœ¨æ‰€æœ‰çš„è¯·æ±‚éƒ½è¢«ä¼ é€’åˆ° localhost:8080 å¹¶å°†ç»“æœå‘é€å›åŸå§‹å®¢æˆ·ç«¯ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

If we take a look at ServeHTTP method signature, it has the signature of an HTTP handler, that's why we could pass it to the `HandlerFunc` in `http`.

å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ ServeHTTP æ–¹æ³•ç­¾åï¼Œå®ƒæœ‰ä¸€ä¸ª HTTP å¤„ç†ç¨‹åºçš„ç­¾åï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥å°†å®ƒä¼ é€’ç»™ `http` ä¸­çš„ `HandlerFunc`ã€‚

You can find more examples in [docs](https://golang.org/pkg/net/http/httputil/#ReverseProxy). 

æ‚¨å¯ä»¥åœ¨ [docs](https://golang.org/pkg/net/http/httputil/#ReverseProxy) ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚

For our simple load balancer we could initiate the `ReverseProxy` with the associated `URL` in the `Backend`, so that `ReverseProxy` will route our requests to the `URL`.

å¯¹äºæˆ‘ä»¬çš„ç®€å•è´Ÿè½½å‡è¡¡å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨â€œåç«¯â€ä¸­ä½¿ç”¨å…³è”çš„â€œURLâ€å¯åŠ¨â€œReverseProxyâ€ï¼Œä»¥ä¾¿â€œReverseProxyâ€å°†æˆ‘ä»¬çš„è¯·æ±‚è·¯ç”±åˆ°â€œURLâ€ã€‚

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#selection-process)Selection Process

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#selection-process)é€‰æ‹©æµç¨‹

We need to **skip dead backends** during the next pick. But to do anything we need a way to count.

æˆ‘ä»¬éœ€è¦åœ¨ä¸‹ä¸€æ¬¡é€‰æ‹©æœŸé—´**è·³è¿‡æ­»åç«¯**ã€‚ä½†æ˜¯è¦åšä»»ä½•äº‹æƒ…ï¼Œæˆ‘ä»¬éƒ½éœ€è¦ä¸€ç§è®¡ç®—æ–¹æ³•ã€‚

Multiple clients will connect to the load balancer and when each of  them requests a next peer to pass the traffic on race conditions could  occur. To prevent it we could lock the `ServerPool` with a `mutex`. But that would be an overkill, besides we don't want to lock the ServerPool at all. We just want to increase the counter by one

å¤šä¸ªå®¢æˆ·ç«¯å°†è¿æ¥åˆ°è´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶ä¸”å½“æ¯ä¸ªå®¢æˆ·ç«¯è¯·æ±‚ä¸‹ä¸€ä¸ªå¯¹ç­‰æ–¹åœ¨ç«äº‰æ¡ä»¶ä¸‹ä¼ é€’æµé‡æ—¶ï¼Œå¯èƒ½ä¼šå‘ç”Ÿã€‚ä¸ºäº†é˜²æ­¢å®ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ª `mutex` é”å®š `ServerPool`ã€‚ä½†è¿™å°†æ˜¯ä¸€ç§çŸ«æ‰è¿‡æ­£ï¼Œæ­¤å¤–æˆ‘ä»¬æ ¹æœ¬ä¸æƒ³é”å®š ServerPoolã€‚æˆ‘ä»¬åªæƒ³å°†è®¡æ•°å™¨åŠ ä¸€

To meet that requirement, the ideal solution is to make this increment atomically. And Go supports that well via `atomic` package.

ä¸ºäº†æ»¡è¶³è¯¥è¦æ±‚ï¼Œç†æƒ³çš„è§£å†³æ–¹æ¡ˆæ˜¯åŸå­åœ°è¿›è¡Œæ­¤å¢é‡ã€‚ Go é€šè¿‡ `atomic` åŒ…å¾ˆå¥½åœ°æ”¯æŒè¿™ä¸€ç‚¹ã€‚

```go
func (s *ServerPool) NextIndex() int {
  return int(atomic.AddUint64(&s.current, uint64(1)) % uint64(len(s.backends)))
}
```


In here, we are increasing the current value by one atomically and  returns the index by modding with the length of the slice. Which means the value always will be between 0 and length of the slice. In the end, we are interested in a particular index, not the total  count.

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥åŸå­æ–¹å¼å°†å½“å‰å€¼å¢åŠ  1ï¼Œå¹¶é€šè¿‡ä¿®æ”¹åˆ‡ç‰‡çš„é•¿åº¦æ¥è¿”å›ç´¢å¼•ã€‚è¿™æ„å‘³ç€è¯¥å€¼å§‹ç»ˆä»‹äº 0 å’Œåˆ‡ç‰‡é•¿åº¦ä¹‹é—´ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ç‰¹å®šç´¢å¼•æ„Ÿå…´è¶£ï¼Œè€Œä¸æ˜¯æ€»æ•°ã€‚

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#picking-up-an-alive-backend)Picking up an alive backend.

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#picking-up-an-alive-backend)é€‰æ‹©ä¸€ä¸ªæ´»ç€çš„åç«¯ã€‚

We already know that our requests are routed in a cycle for each backend. All we have to skip dead ones, that's it.

æˆ‘ä»¬å·²ç»çŸ¥é“æˆ‘ä»¬çš„è¯·æ±‚åœ¨æ¯ä¸ªåç«¯çš„å¾ªç¯ä¸­è·¯ç”±ã€‚æˆ‘ä»¬æ‰€è¦åšçš„å°±æ˜¯è·³è¿‡æ­»è€…ï¼Œä»…æ­¤è€Œå·²ã€‚

`GetNext()` always return a value  that's capped between 0 and the length of the slice. At any point, we  get a next peer and if it's not alive we would have to search through  the slice in a cycle.



`GetNext()` æ€»æ˜¯è¿”å›ä¸€ä¸ªä¸Šé™åœ¨ 0 å’Œåˆ‡ç‰‡é•¿åº¦ä¹‹é—´çš„å€¼ã€‚åœ¨ä»»ä½•æ—¶å€™ï¼Œæˆ‘ä»¬éƒ½ä¼šå¾—åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå¦‚æœå®ƒä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸åœ¨ä¸€ä¸ªå¾ªç¯ä¸­æœç´¢åˆ‡ç‰‡ã€‚



![Traverse the slice as a cycle](https://kasvith.me/assets/static/lb-slice-traverse.93ea44d.9c17a7fc56f9bf6c9e4583c29127aa55.png)Traverse the slice as a cycle



As shown in the figure above, we want to traverse from next to the entire list, which can be done simply by traversing `next + length` But to pick an index, we want to cap it between slice length. It can be easily done with modding operation.

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬è¦ä» next åˆ°æ•´ä¸ªåˆ—è¡¨éå†ï¼Œå¯ä»¥ç®€å•åœ°é€šè¿‡éå† `next + length` æ¥å®Œæˆï¼Œä½†æ˜¯è¦é€‰æ‹©ä¸€ä¸ªç´¢å¼•ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å®ƒé™åˆ¶åœ¨åˆ‡ç‰‡é•¿åº¦ä¹‹é—´ã€‚å®ƒå¯ä»¥é€šè¿‡ä¿®æ”¹æ“ä½œè½»æ¾å®Œæˆã€‚

After we find a working backend through the search, we mark it as the current one.

åœ¨æˆ‘ä»¬é€šè¿‡æœç´¢æ‰¾åˆ°ä¸€ä¸ªå¯ç”¨çš„åç«¯åï¼Œæˆ‘ä»¬å°†å…¶æ ‡è®°ä¸ºå½“å‰åç«¯ã€‚

Below you can see the code for the above operation.

æ‚¨å¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ä¸Šè¿°æ“ä½œçš„ä»£ç ã€‚

```go
// GetNextPeer returns next active peer to take a connection
func (s *ServerPool) GetNextPeer() *Backend {
  // loop entire backends to find out an Alive backend
  next := s.NextIndex()
  l := len(s.backends) + next // start from next and move a full cycle
  for i := next;i < l;i++ {
    idx := i % len(s.backends) // take an index by modding with length
    // if we have an alive backend, use it and store if its not the original one
    if s.backends[idx].IsAlive() {
      if i != next {
        atomic.StoreUint64(&s.current, uint64(idx)) // mark the current one
      }
      return s.backends[idx]
    }
  }
  return nil
}
```


## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#avoid-race-conditions-in-backend-struct)Avoid Race Conditions in Backend struct

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#avoid-race-conditions-in-backend-struct)é¿å…åç«¯ç»“æ„ä¸­çš„ç«äº‰æ¡ä»¶

There is a serious issue we need to consider. Our `Backend` structure has a variable which could be modified or accessed by different goroutines same time.

æˆ‘ä»¬éœ€è¦è€ƒè™‘ä¸€ä¸ªä¸¥é‡çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ `Backend` ç»“æ„æœ‰ä¸€ä¸ªå˜é‡ï¼Œå¯ä»¥åŒæ—¶è¢«ä¸åŒçš„ goroutine ä¿®æ”¹æˆ–è®¿é—®ã€‚

We know there would be more goroutines reading from this rather than writing to it. So we have picked `RWMutex` to serialize the access to the `Alive`.

æˆ‘ä»¬çŸ¥é“ä¼šæœ‰æ›´å¤šçš„ goroutine ä»ä¸­è¯»å–è€Œä¸æ˜¯å†™å…¥ã€‚æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©äº† `RWMutex` æ¥åºåˆ—åŒ–å¯¹ `Alive` çš„è®¿é—®ã€‚

```go
// SetAlive for this backend
func (b *Backend) SetAlive(alive bool) {
  b.mux.Lock()
  b.Alive = alive
  b.mux.Unlock()
}

// IsAlive returns true when backend is alive
func (b *Backend) IsAlive() (alive bool) {
  b.mux.RLock()
  alive = b.Alive
  b.mux.RUnlock()
  return
}
```


## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#lets-load-balance-requests)Lets load balance requests

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#lets-load-balance-requests)è®©è´Ÿè½½å‡è¡¡è¯·æ±‚

With all the background we created, we can formulate the following simple method to load balance our requests. It will only fail when our all backends are offline.

æœ‰äº†æˆ‘ä»¬åˆ›å»ºçš„æ‰€æœ‰èƒŒæ™¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ¶å®šä»¥ä¸‹ç®€å•çš„æ–¹æ³•æ¥å¹³è¡¡æˆ‘ä»¬çš„è¯·æ±‚ã€‚åªæœ‰å½“æˆ‘ä»¬æ‰€æœ‰çš„åç«¯éƒ½ç¦»çº¿æ—¶å®ƒæ‰ä¼šå¤±è´¥ã€‚

```go
// lb load balances the incoming request
func lb(w http.ResponseWriter, r *http.Request) {
  peer := serverPool.GetNextPeer()
  if peer != nil {
    peer.ReverseProxy.ServeHTTP(w, r)
    return
  }
  http.Error(w, "Service not available", http.StatusServiceUnavailable)
}
```


This method can be simply passed as a `HandlerFunc` to the http server.

è¿™ä¸ªæ–¹æ³•å¯ä»¥ç®€å•åœ°ä½œä¸º `HandlerFunc` ä¼ é€’ç»™ http æœåŠ¡å™¨ã€‚

```go
server := http.Server{
  Addr:    fmt.Sprintf(":%d", port),
  Handler: http.HandlerFunc(lb),
}
```




## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#route-traffic-only-to-healthy-backends)Route traffic only to healthy backends

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#route-traffic-only-to-healthy-backends)ä»…å°†æµé‡è·¯ç”±åˆ°å¥åº·çš„åç«¯

Our current `lb` has a serious  issue. We don't know if a backend is healthy or not. To know this we  have to try out a backend and check whether it is alive.

æˆ‘ä»¬å½“å‰çš„â€œlbâ€æœ‰ä¸€ä¸ªä¸¥é‡çš„é—®é¢˜ã€‚æˆ‘ä»¬ä¸çŸ¥é“åç«¯æ˜¯å¦å¥åº·ã€‚è¦çŸ¥é“è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¿…é¡»å°è¯•ä¸€ä¸ªåç«¯å¹¶æ£€æŸ¥å®ƒæ˜¯å¦è¿˜æ´»ç€ã€‚

We can do this in two ways,

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼åšåˆ°è¿™ä¸€ç‚¹

- **Active**: While performing the current request, we find the selected backend is unresponsive, mark it as down.
- **Passive**: We can ping backends on fixed intervals and check status

- **Active**ï¼šåœ¨æ‰§è¡Œå½“å‰è¯·æ±‚æ—¶ï¼Œæˆ‘ä»¬å‘ç°æ‰€é€‰åç«¯æ²¡æœ‰å“åº”ï¼Œå°†å…¶æ ‡è®°ä¸ºå…³é—­ã€‚
- **è¢«åŠ¨**ï¼šæˆ‘ä»¬å¯ä»¥åœ¨å›ºå®šçš„æ—¶é—´é—´éš” ping åç«¯å¹¶æ£€æŸ¥çŠ¶æ€

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#actively-checking-for-healthy-backends)Actively checking for healthy backends

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#actively-checking-for-healthy-backends)ä¸»åŠ¨æ£€æŸ¥åç«¯æ˜¯å¦å¥åº·

`ReverseProxy` triggers a callback function, `ErrorHandler` on any error. We can use that to detect any failure. Here is the implementation

`ReverseProxy` ä¼šåœ¨ä»»ä½•é”™è¯¯æ—¶è§¦å‘å›è°ƒå‡½æ•° `ErrorHandler`ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥æ£€æµ‹ä»»ä½•æ•…éšœã€‚è¿™æ˜¯å®ç°

```go
proxy.ErrorHandler = func(writer http.ResponseWriter, request *http.Request, e error) {
  log.Printf("[%s] %s\n", serverUrl.Host, e.Error())
  retries := GetRetryFromContext(request)
  if retries < 3 {
    select {
      case <-time.After(10 * time.Millisecond):
        ctx := context.WithValue(request.Context(), Retry, retries+1)
        proxy.ServeHTTP(writer, request.WithContext(ctx))
      }
      return
    }

  // after 3 retries, mark this backend as down
  serverPool.MarkBackendStatus(serverUrl, false)

  // if the same request routing for few attempts with different backends, increase the count
  attempts := GetAttemptsFromContext(request)
  log.Printf("%s(%s) Attempting retry %d\n", request.RemoteAddr, request.URL.Path, attempts)
  ctx := context.WithValue(request.Context(), Attempts, attempts+1)
  lb(writer, request.WithContext(ctx))
}
```


In here we leverage the power of closures to design this error  handler. It allows us to capture outer variables like server url into  our method. It will check for existing retry count and if it is less than 3, we  again send the same request to the same backend. The reason behind this is due to temporary errors the server may reject  your requests and it may be available after a short delay(possibly the  server ran out of sockets to accept more clients). So we have put a  timer to delay the retry for around 10 milliseconds. We increases the  retry count with every request.

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ©ç”¨é—­åŒ…çš„åŠ›é‡æ¥è®¾è®¡è¿™ä¸ªé”™è¯¯å¤„ç†ç¨‹åºã€‚å®ƒå…è®¸æˆ‘ä»¬å°†æœåŠ¡å™¨ url ç­‰å¤–éƒ¨å˜é‡æ•è·åˆ°æˆ‘ä»¬çš„æ–¹æ³•ä¸­ã€‚å®ƒå°†æ£€æŸ¥ç°æœ‰çš„é‡è¯•è®¡æ•°ï¼Œå¦‚æœå®ƒå°äº 3ï¼Œæˆ‘ä»¬å†æ¬¡å°†ç›¸åŒçš„è¯·æ±‚å‘é€åˆ°åŒä¸€ä¸ªåç«¯ã€‚è¿™èƒŒåçš„åŸå› æ˜¯ç”±äºæœåŠ¡å™¨å¯èƒ½ä¼šæ‹’ç»æ‚¨çš„è¯·æ±‚çš„ä¸´æ—¶é”™è¯¯ï¼Œå¹¶ä¸”å¯èƒ½ä¼šåœ¨çŸ­æš‚å»¶è¿Ÿåå¯ç”¨ï¼ˆå¯èƒ½æœåŠ¡å™¨ç”¨å°½äº†å¥—æ¥å­—ä»¥æ¥å—æ›´å¤šå®¢æˆ·ç«¯ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªè®¡æ—¶å™¨æ¥å»¶è¿Ÿé‡è¯•å¤§çº¦ 10 æ¯«ç§’ã€‚æˆ‘ä»¬å¢åŠ æ¯ä¸ªè¯·æ±‚çš„é‡è¯•æ¬¡æ•°ã€‚

After every retry failed, we mark this backend as down.

æ¯æ¬¡é‡è¯•å¤±è´¥åï¼Œæˆ‘ä»¬å°†æ­¤åç«¯æ ‡è®°ä¸ºå…³é—­ã€‚

Next thing we want to do is attempting a new backend to the same  request. We do it by keeping a count of the attempts using the context  package. After increasing the attempt count, we pass it back to `lb` to pick a new peer to process the request.

æˆ‘ä»¬è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯å°è¯•å¯¹åŒä¸€è¯·æ±‚ä½¿ç”¨æ–°çš„åç«¯ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ context åŒ…è®°å½•å°è¯•æ¬¡æ•°æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚åœ¨å¢åŠ å°è¯•æ¬¡æ•°åï¼Œæˆ‘ä»¬å°†å®ƒä¼ å›ç»™ `lb` ä»¥é€‰æ‹©ä¸€ä¸ªæ–°çš„å¯¹ç­‰ç‚¹æ¥å¤„ç†è¯·æ±‚ã€‚

Now we can't do this indefinitely, thus we need to check from `lb` whether the maximum attempts already taken before processing the request further.

ç°åœ¨æˆ‘ä»¬ä¸èƒ½æ— é™æœŸåœ°è¿™æ ·åšï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä»â€œlbâ€æ£€æŸ¥åœ¨è¿›ä¸€æ­¥å¤„ç†è¯·æ±‚ä¹‹å‰æ˜¯å¦å·²ç»è¿›è¡Œäº†æœ€å¤§å°è¯•ã€‚

We can simply get the attempt count from the request and if it has exceeded the max count, eliminate the request.

æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä»è¯·æ±‚ä¸­è·å–å°è¯•æ¬¡æ•°ï¼Œå¦‚æœè¶…è¿‡æœ€å¤§æ¬¡æ•°ï¼Œåˆ™æ¶ˆé™¤è¯·æ±‚ã€‚

```go
// lb load balances the incoming request
func lb(w http.ResponseWriter, r *http.Request) {
  attempts := GetAttemptsFromContext(r)
  if attempts > 3 {
    log.Printf("%s(%s) Max attempts reached, terminating\n", r.RemoteAddr, r.URL.Path)
    http.Error(w, "Service not available", http.StatusServiceUnavailable)
    return
  }

  peer := serverPool.GetNextPeer()
  if peer != nil {
    peer.ReverseProxy.ServeHTTP(w, r)
    return
  }
  http.Error(w, "Service not available", http.StatusServiceUnavailable)
}
```


This implementation is recursive.

è¿™ä¸ªå®ç°æ˜¯é€’å½’çš„ã€‚

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#use-of-context)Use of context

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#use-of-context)ä½¿ç”¨ä¸Šä¸‹æ–‡

`context` package allows you to store useful data in an Http request. We heavily utilized this to track request specific data such as Attempt count and Retry count.

`context` åŒ…å…è®¸æ‚¨åœ¨ Http è¯·æ±‚ä¸­å­˜å‚¨æœ‰ç”¨çš„æ•°æ®ã€‚æˆ‘ä»¬å¤§é‡ä½¿ç”¨å®ƒæ¥è·Ÿè¸ªè¯·æ±‚ç‰¹å®šæ•°æ®ï¼Œä¾‹å¦‚å°è¯•è®¡æ•°å’Œé‡è¯•è®¡æ•°ã€‚

First, we need to specify keys for the context. It is recommended to  use non-colliding integer keys rather than strings. Go provides `iota` keyword to implement constants incrementally, each containing a unique value. That is a perfect solution defining integer keys.

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸ºä¸Šä¸‹æ–‡æŒ‡å®šé”®ã€‚å»ºè®®ä½¿ç”¨éå†²çªæ•´æ•°é”®è€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚ Go æä¾›äº† `iota` å…³é”®å­—æ¥å¢é‡åœ°å®ç°å¸¸é‡ï¼Œæ¯ä¸ªå¸¸é‡éƒ½åŒ…å«ä¸€ä¸ªå”¯ä¸€çš„å€¼ã€‚è¿™æ˜¯å®šä¹‰æ•´æ•°é”®çš„å®Œç¾è§£å†³æ–¹æ¡ˆã€‚

```go
const (
  Attempts int = iota
  Retry
)
```


Then we can retrieve the value as usually we do with a HashMap like  follows. The default return value may depend on the use case.

ç„¶åæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨ HashMap æ£€ç´¢å€¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚é»˜è®¤è¿”å›å€¼å¯èƒ½å–å†³äºç”¨ä¾‹ã€‚

```go
// GetAttemptsFromContext returns the attempts for request
func GetRetryFromContext(r *http.Request) int {
  if retry, ok := r.Context().Value(Retry).(int);ok {
    return retry
  }
  return 0
}
```


## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#passive-health-checks)Passive health checks 

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#passive-health-checks)è¢«åŠ¨å¥åº·æ£€æŸ¥

Passive health checks allow to recover dead backends or identify  them. We ping the backends with fixed intervals to check their status.

è¢«åŠ¨å¥åº·æ£€æŸ¥å…è®¸æ¢å¤æ­»åç«¯æˆ–è¯†åˆ«å®ƒä»¬ã€‚æˆ‘ä»¬ä»¥å›ºå®šçš„æ—¶é—´é—´éš” ping åç«¯ä»¥æ£€æŸ¥å®ƒä»¬çš„çŠ¶æ€ã€‚

To ping, we try to establish a TCP connection. If the backend  responses, we mark it as alive. This method can be changed to call a  specific endpoint like `/status` if you like. Make sure to close the connection once it established to reduce the additional load in the server. Otherwise, it will try to maintain the connection and it would run out of resources eventually.

ä¸ºäº† pingï¼Œæˆ‘ä»¬å°è¯•å»ºç«‹ TCP è¿æ¥ã€‚å¦‚æœåç«¯å“åº”ï¼Œæˆ‘ä»¬å°†å…¶æ ‡è®°ä¸ºæ´»ç€ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥æ›´æ”¹æ­¤æ–¹æ³•ä»¥è°ƒç”¨ç‰¹å®šç«¯ç‚¹ï¼Œä¾‹å¦‚ `/status`ã€‚ç¡®ä¿åœ¨å»ºç«‹è¿æ¥åå…³é—­è¿æ¥ä»¥å‡å°‘æœåŠ¡å™¨ä¸­çš„é¢å¤–è´Ÿè½½ã€‚å¦åˆ™ï¼Œå®ƒä¼šå°è¯•ä¿æŒè¿æ¥ï¼Œæœ€ç»ˆä¼šè€—å°½èµ„æºã€‚

```go
// isAlive checks whether a backend is Alive by establishing a TCP connection
func isBackendAlive(u *url.URL) bool {
  timeout := 2 * time.Second
  conn, err := net.DialTimeout("tcp", u.Host, timeout)
  if err != nil {
    log.Println("Site unreachable, error: ", err)
    return false
  }
  _ = conn.Close() // close it, we dont need to maintain this connection
  return true
}
```


Now we can iterate the servers and mark their status like follows,

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿­ä»£æœåŠ¡å™¨å¹¶æ ‡è®°å®ƒä»¬çš„çŠ¶æ€ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œ

```go
// HealthCheck pings the backends and update the status
func (s *ServerPool) HealthCheck() {
  for _, b := range s.backends {
    status := "up"
    alive := isBackendAlive(b.URL)
    b.SetAlive(alive)
    if !alive {
      status = "down"
    }
    log.Printf("%s [%s]\n", b.URL, status)
  }
}
```


To run this periodically we can start a timer in Go. Once a timer created it allows you to listen for the event using a channel.

ä¸ºäº†å®šæœŸè¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ Go ä¸­å¯åŠ¨ä¸€ä¸ªè®¡æ—¶å™¨ã€‚åˆ›å»ºè®¡æ—¶å™¨åï¼Œå®ƒå…è®¸æ‚¨ä½¿ç”¨é€šé“ä¾¦å¬äº‹ä»¶ã€‚

```go
// healthCheck runs a routine for check status of the backends every 20 secs
func healthCheck() {
  t := time.NewTicker(time.Second * 20)
  for {
    select {
    case <-t.C:
      log.Println("Starting health check...")
      serverPool.HealthCheck()
      log.Println("Health check completed")
    }
  }
}
```


In the above snippet, `<-t.C` channel will return a value per 20s. `select` allows to detect this event. `select` waits until at least one case statement could be executed if there is no `default` case.

åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œ`<-t.C` é€šé“å°†æ¯ 20 ç§’è¿”å›ä¸€ä¸ªå€¼ã€‚ `select` å…è®¸æ£€æµ‹è¿™ä¸ªäº‹ä»¶ã€‚å¦‚æœæ²¡æœ‰ `default` caseï¼Œ`select` ä¼šç­‰å¾…è‡³å°‘ä¸€ä¸ª case è¯­å¥å¯ä»¥è¢«æ‰§è¡Œã€‚

Finally, run this in a separate goroutine.

æœ€åï¼Œåœ¨å•ç‹¬çš„ goroutine ä¸­è¿è¡Œå®ƒã€‚

```go
go healthCheck()
```


## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#conclusion)Conclusion

## [#](https://kasvith.me/posts/lets-create-a-simple-lb-go/#conclusion)ç»“è®º

We covered a lot of stuff in this article.

æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ä»‹ç»äº†å¾ˆå¤šå†…å®¹ã€‚

- Round Robin Selection
- ReverseProxy from the standard library
- Mutexes
- Atomic Operations
- Closures
- Callbacks
- Select Operation

- å¾ªç¯é€‰æ‹©
- æ¥è‡ªæ ‡å‡†åº“çš„ ReverseProxy
- äº’æ–¥ä½“
- åŸå­æ“ä½œ
- å…³é—­
- å›è°ƒ
- é€‰æ‹©æ“ä½œ

There is a lot we can do to improve our tiny load balancer.

æˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šäº‹æƒ…æ¥æ”¹è¿›æˆ‘ä»¬çš„å¾®å‹è´Ÿè½½å‡è¡¡å™¨ã€‚

For example,

ä¾‹å¦‚ï¼Œ

- Use a heap for sort out alive backends to reduce search surface
- Collect statistics
- Implement weighted round-robin/least connections
- Add support for a configuration file

- ä½¿ç”¨å †æ•´ç†æ´»ç€çš„åç«¯ä»¥å‡å°‘æœç´¢é¢
- æ”¶é›†ç»Ÿè®¡æ•°æ®
- å®æ–½åŠ æƒå¾ªç¯/æœ€å°‘è¿æ¥
- æ·»åŠ å¯¹é…ç½®æ–‡ä»¶çš„æ”¯æŒ

etc.

ç­‰ç­‰ã€‚

You can find the source code to repository [here](https://github.com/kasvith/simplelb/).

æ‚¨å¯ä»¥åœ¨ [æ­¤å¤„](https://github.com/kasvith/simplelb/) ä¸­æ‰¾åˆ°å­˜å‚¨åº“çš„æºä»£ç ã€‚

Thank you for reading this article ğŸ˜„ 

è°¢è°¢ä½ é˜…è¯»è¿™ç¯‡æ–‡ç« ğŸ˜„

