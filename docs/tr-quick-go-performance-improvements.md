# Simple techniques to optimise Go programs

# ä¼˜åŒ– Go ç¨‹åºçš„ç®€å•æŠ€å·§

I'm very interested in performance. I'm not sure I can explain the underlying reasons for it. I'm easily frustrated at slow services and programs, and it seems like [I'm not alone.](http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html)

æˆ‘å¯¹è¡¨æ¼”å¾ˆæ„Ÿå…´è¶£ã€‚æˆ‘ä¸ç¡®å®šæˆ‘èƒ½è§£é‡Šå®ƒçš„æ ¹æœ¬åŸå› ã€‚æˆ‘å¾ˆå®¹æ˜“å¯¹ç¼“æ…¢çš„æœåŠ¡å’Œç¨‹åºæ„Ÿåˆ°æ²®ä¸§ï¼Œè€Œä¸”ä¼¼ä¹ [æˆ‘å¹¶ä¸å­¤å•ã€‚](http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html)

> _In A/B tests, we tried delaying the page in increments of 100 milliseconds and found that even very small delays would result in substantial and costly drops in revenue._ \- Greg Linden, Amazon.com

> _åœ¨ A/B æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å°è¯•ä»¥ 100 æ¯«ç§’ä¸ºå¢é‡å»¶è¿Ÿé¡µé¢ï¼Œå‘ç°å³ä½¿æ˜¯éå¸¸å°çš„å»¶è¿Ÿä¹Ÿä¼šå¯¼è‡´æ”¶å…¥å¤§å¹…ä¸‹é™ä¸”ä»£ä»·é«˜æ˜‚ã€‚_ \- Greg Lindenï¼ŒAmazon.com

From my experience, poor performance manifests in one of two ways:

æ ¹æ®æˆ‘çš„ç»éªŒï¼Œæ€§èƒ½ä¸ä½³è¡¨ç°ä¸ºä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼š

- Operations that performed well at small scale, but become unviable as the number of users grows. These are usually O(N) or O(NÂ²) operations. When your user base is small, these perform just fine, and are often done in order to get a product to market. As your use base grows, you see more [pathological examples](https://theoutline.com/post/4147/in-twitters-early-days-only-one-celebrity-could-tweet-at-a-time?zd=1&zi=ivqvd4py) that you weren't expecting, and your service grinds to a halt.
- Many individual sources of small optimisation - AKA 'death by a thousand crufts'.

- åœ¨å°èŒƒå›´å†…è¡¨ç°è‰¯å¥½çš„æ“ä½œï¼Œä½†éšç€ç”¨æˆ·æ•°é‡çš„å¢é•¿å˜å¾—ä¸å¯è¡Œã€‚è¿™äº›é€šå¸¸æ˜¯ O(N) æˆ– O(NÂ²) æ“ä½œã€‚å½“æ‚¨çš„ç”¨æˆ·ç¾¤å¾ˆå°æ—¶ï¼Œè¿™äº›è¡¨ç°å¾ˆå¥½ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯ä¸ºäº†å°†äº§å“æ¨å‘å¸‚åœºã€‚éšç€æ‚¨ä½¿ç”¨åŸºç¡€çš„å¢é•¿ï¼Œæ‚¨ä¼šçœ‹åˆ°æ›´å¤š[ç—…ç†ç¤ºä¾‹](https://theoutline.com/post/4147/in-twitters-early-days-only-one-celebrity-could-tweet-at-a-time?zd=1&zi=ivqvd4py)ï¼Œè¿™æ˜¯æ‚¨æ²¡æƒ³åˆ°çš„ï¼Œæ‚¨çš„æœåŠ¡ä¼šåœæ­¢ã€‚
- å°ä¼˜åŒ–çš„è®¸å¤šä¸ªäººæ¥æº - åˆåâ€œåƒç¯‡ä¸€å¾‹çš„æ­»äº¡â€ã€‚

I've spent most of my career either doing data science with Python, or building services in Go; I have far more experience optimising the latter. Go is usually not the bottleneck in the services I write - programs are often IO bound as they talk to the database. However, in batch machine learning pipelines - like I built in my previous role - your program is often CPU bound. When your Go program is using excessive CPU, and the excessive usage is having a negative impact, there's various strategies you can use to mitigate that.

æˆ‘èŒä¸šç”Ÿæ¶¯çš„å¤§éƒ¨åˆ†æ—¶é—´è¦ä¹ˆç”¨ Python è¿›è¡Œæ•°æ®ç§‘å­¦ï¼Œè¦ä¹ˆç”¨ Go æ„å»ºæœåŠ¡ï¼›æˆ‘æœ‰æ›´å¤šä¼˜åŒ–åè€…çš„ç»éªŒã€‚ Go é€šå¸¸ä¸æ˜¯æˆ‘ç¼–å†™çš„æœåŠ¡çš„ç“¶é¢ˆâ€”â€”ç¨‹åºåœ¨ä¸æ•°æ®åº“äº¤è°ˆæ—¶é€šå¸¸æ˜¯ IO ç»‘å®šçš„ã€‚ä½†æ˜¯ï¼Œåœ¨æ‰¹å¤„ç†æœºå™¨å­¦ä¹ ç®¡é“ä¸­ - å°±åƒæˆ‘åœ¨ä¹‹å‰çš„è§’è‰²ä¸­æ„å»ºçš„é‚£æ · - æ‚¨çš„ç¨‹åºé€šå¸¸å— CPU é™åˆ¶ã€‚å½“æ‚¨çš„ Go ç¨‹åºä½¿ç”¨è¿‡å¤šçš„ CPUï¼Œå¹¶ä¸”è¿‡åº¦ä½¿ç”¨ä¼šäº§ç”Ÿè´Ÿé¢å½±å“æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å¤šç§ç­–ç•¥æ¥å‡è½»è¿™ç§å½±å“ã€‚

This post explains some techniques you can use to significantly improve the performance of your program with little effort. I am deliberately ignoring techniques that require significant effort, or large changes to program structure.

è¿™ç¯‡æ–‡ç« è§£é‡Šäº†ä¸€äº›æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›æŠ€æœ¯è½»æ¾æ˜¾ç€æé«˜ç¨‹åºçš„æ€§èƒ½ã€‚æˆ‘æ•…æ„å¿½ç•¥éœ€è¦å¤§é‡åŠªåŠ›æˆ–å¯¹ç¨‹åºç»“æ„è¿›è¡Œå¤§é‡æ›´æ”¹çš„æŠ€æœ¯ã€‚

## Before you start

##  åœ¨ä½ å¼€å§‹ä¹‹å‰

Before you make any changes to your program, invest time in creating a proper baseline to compare against. If you don't do this, you'll be searching around in the dark, wondering if your changes are having any improvement. Write benchmarks first, and grab [profiles](https://blog.golang.org/profiling-go-programs) for use in pprof. In the best case, this will be a [Go benchmark](https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go): this allows easy usage of pprof, and memory allocation profiling. You should also use [`benchcmp`](https://godoc.org/golang.org/x/tools/cmd/benchcmp): a helpful tool for comparing the difference in performance between two benchmarks.

åœ¨å¯¹ç¨‹åºè¿›è¡Œä»»ä½•æ›´æ”¹ä¹‹å‰ï¼Œè¯·èŠ±æ—¶é—´åˆ›å»ºé€‚å½“çš„åŸºçº¿ä»¥è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœæ‚¨ä¸è¿™æ ·åšï¼Œæ‚¨å°†åœ¨é»‘æš—ä¸­å››å¤„å¯»æ‰¾ï¼Œæƒ³çŸ¥é“æ‚¨çš„æ›´æ”¹æ˜¯å¦æœ‰ä»»ä½•æ”¹è¿›ã€‚å…ˆå†™benchmarksï¼Œç„¶åæŠ“å–[profiles](https://blog.golang.org/profiling-go-programs) ç”¨äºpprofã€‚åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œè¿™å°†æ˜¯ä¸€ä¸ª [Go åŸºå‡†æµ‹è¯•](https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go)ï¼šè¿™æ ·å¯ä»¥è½»æ¾ä½¿ç”¨ pprofï¼Œå’Œå†…å­˜åˆ†é…åˆ†æã€‚æ‚¨è¿˜åº”è¯¥ä½¿ç”¨ [`benchcmp`](https://godoc.org/golang.org/x/tools/cmd/benchcmp)ï¼šä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚

If your code is not easily benchmarked, just start with something that you can time. You can profile your code manually using [`runtime/pprof`](https://golang.org/pkg/runtime/pprof/).

å¦‚æœæ‚¨çš„ä»£ç ä¸å®¹æ˜“è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œè¯·ä»å¯ä»¥è®¡æ—¶çš„å†…å®¹å¼€å§‹ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ [`runtime/pprof`](https://golang.org/pkg/runtime/pprof/) æ‰‹åŠ¨åˆ†ææ‚¨çš„ä»£ç ã€‚

Let's get started!

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### Use `sync.Pool` to re-use previously allocated objects

### ä½¿ç”¨`sync.Pool` æ¥é‡ç”¨ä¹‹å‰åˆ†é…çš„å¯¹è±¡

[`sync.Pool`](https://golang.org/pkg/sync/#Pool) implements a [free-list](https://en.wikipedia.org/wiki/Free_list). This allows you to re-use structures that you've previously allocated. This amortises the allocation of an object over many usages, reducing the work the garbage collector has to do. The API is very simple: implement a function that allocates a new instance of your object. It should return a pointer type.

[`sync.Pool`](https://golang.org/pkg/sync/#Pool) å®ç°äº†ä¸€ä¸ª [free-list](https://en.wikipedia.org/wiki/Free_list)ã€‚è¿™å…è®¸æ‚¨é‡ç”¨æ‚¨ä¹‹å‰åˆ†é…çš„ç»“æ„ã€‚è¿™å¯ä»¥åœ¨å¤šæ¬¡ä½¿ç”¨ä¸­åˆ†æ‘Šå¯¹è±¡çš„åˆ†é…ï¼Œå‡å°‘åƒåœ¾æ”¶é›†å™¨å¿…é¡»åšçš„å·¥ä½œã€‚ API éå¸¸ç®€å•ï¼šå®ç°ä¸€ä¸ªå‡½æ•°æ¥åˆ†é…å¯¹è±¡çš„æ–°å®ä¾‹ã€‚å®ƒåº”è¯¥è¿”å›ä¸€ä¸ªæŒ‡é’ˆç±»å‹ã€‚

```
var bufpool = sync.Pool{
    New: func() interface{} {
        buf := make([]byte, 512)
        return &buf
    }}

```

After this, you can `Get()` objects from the pool, `Put()` ting them back after you are done.

åœ¨æ­¤ä¹‹åï¼Œæ‚¨å¯ä»¥ä»æ± ä¸­`Get()` å¯¹è±¡ï¼Œå®Œæˆå`Put()` å°†å®ƒä»¬æ”¾å›åŸå¤„ã€‚

```
// sync.Pool returns a interface{}: you must cast it to the underlying type
// before you use it.
bp := bufpool.Get().(*[]byte)
b := *bp
defer func() {
    *bp = b
    bufpool.Put(bp)
}()

// Now, go do interesting things with your byte buffer.
buf := bytes.NewBuffer(b)

```

Some caveats apply. Before Go 1.13, the pool was cleared out every time a garbage collection occured. This might be detrimental to performance in programs that allocate a lot. In 1.13, it [seems that more objects will survive GC's](https://go-review.googlesource.com/c/go/+/162919/).

ä¸€äº›è­¦å‘Šé€‚ç”¨ã€‚åœ¨ Go 1.13 ä¹‹å‰ï¼Œæ¯æ¬¡å‘ç”Ÿåƒåœ¾æ”¶é›†æ—¶éƒ½ä¼šæ¸…é™¤æ± ã€‚è¿™å¯èƒ½ä¼šæŸå®³åˆ†é…å¾ˆå¤šçš„ç¨‹åºçš„æ€§èƒ½ã€‚åœ¨ 1.13 ä¸­ï¼Œ[ä¼¼ä¹æ›´å¤šçš„å¯¹è±¡å°†åœ¨ GC ä¸­å¹¸å­˜ä¸‹æ¥](https://go-review.googlesource.com/c/go/+/162919/)ã€‚

âš ï¸ **You must zero out the fields of the struct before putting the object back in the pool**.

âš ï¸ **åœ¨å°†å¯¹è±¡æ”¾å›æ± ä¹‹å‰ï¼Œæ‚¨å¿…é¡»å°†ç»“æ„ä½“çš„å­—æ®µæ¸…é›¶**ã€‚

If you don't do this, you can obtain a 'dirty' object from the pool that contains data from previous use. This can be a severe security risk!

å¦‚æœæ‚¨ä¸è¿™æ ·åšï¼Œæ‚¨å¯ä»¥ä»åŒ…å«ä»¥å‰ä½¿ç”¨è¿‡çš„æ•°æ®çš„æ± ä¸­è·å–ä¸€ä¸ªâ€œè„â€å¯¹è±¡ã€‚è¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¸¥é‡çš„å®‰å…¨é£é™©ï¼

```go
type AuthenticationResponse {
    Token string
    UserID string
}

rsp := authPool.Get().(*AuthenticationResponse)
defer authPool.Put(rsp)

// If we don't hit this if statement, we might return data from other users!ğŸ˜±
if blah {
    rsp.UserID = "user-1"
    rsp.Token = "super-secret
}

return rsp

```

The safe way to ensure you always zero memory is to do so explicitly:

ç¡®ä¿å§‹ç»ˆä¸ºé›¶å†…å­˜çš„å®‰å…¨æ–¹æ³•æ˜¯æ˜ç¡®è¿™æ ·åšï¼š

```go
// reset resets all fields of the AuthenticationResponse before pooling it.
func (a* AuthenticationResponse) reset() {
    a.Token = ""
    a.UserID = ""
}

rsp := authPool.Get().(*AuthenticationResponse)
defer func() {
    rsp.reset()
    authPool.Put(rsp)
}()

```

The only case in which this is not an issue is when you use _exactly_ the memory that you wrote to. For example:

è¿™ä¸æ˜¯é—®é¢˜çš„å”¯ä¸€æƒ…å†µæ˜¯å½“æ‚¨_å®Œå…¨_ä½¿ç”¨æ‚¨å†™å…¥çš„å†…å­˜æ—¶ã€‚ä¾‹å¦‚ï¼š

```go
var (
    r io.Reader
    w io.Writer
)

// Obtain a buffer from the pool.
buf := *bufPool.Get().(*[]byte)
defer bufPool.Put(&buf)

// We only write to w exactly what we read from r, and no more.ğŸ˜Œ
nr, er := r.Read(buf)
if nr > 0 {
    nw, ew := w.Write(buf[0:nr])
}

```

_Edit: a previous version of this blog post did not specify that the `New()` function should return a pointer type. Thanks, [kevinconaway](https://news.ycombinator.com/item?id=20214506)!_

_Editï¼šæ­¤åšå®¢æ–‡ç« çš„å…ˆå‰ç‰ˆæœ¬æœªæŒ‡å®š `New()` å‡½æ•°åº”è¿”å›æŒ‡é’ˆç±»å‹ã€‚è°¢è°¢ï¼Œ[kevinconaway](https://news.ycombinator.com/item?id=20214506)ï¼_

### Avoid using structures containing pointers as map keys for large maps

### é¿å…ä½¿ç”¨åŒ…å«æŒ‡é’ˆçš„ç»“æ„ä½œä¸ºå¤§å‹æ˜ å°„çš„æ˜ å°„é”®

Phew - that was quite a mouthful. Sorry about that. Much has been said (a lot by my former colleague, [Phil Pearl](https://twitter.com/philpearl)) on Go's performance with [large heap sizes](https://syslog.ravelin.com/further-dangers-of-large-heaps-in-go-7a267b57d487). During a garbage collection, the runtime scans objects containing pointers, and chases them. If you have a very large `map[string]int`, the GC has to check every string within the map, every GC, as strings contain pointers.

å‘¼â€”â€”çœŸæ˜¯ä¸€å£ã€‚å¯¹äºé‚£ä¸ªå¾ˆæŠ±æ­‰ã€‚å…³äº Go çš„ [å¤§å †å¤§å°](https://syslog.ravelin.com/further-å¤§å †çš„å±é™©7a267b57d487)ã€‚åœ¨åƒåœ¾å›æ”¶æœŸé—´ï¼Œè¿è¡Œæ—¶ä¼šæ‰«æåŒ…å«æŒ‡é’ˆçš„å¯¹è±¡ï¼Œå¹¶è¿½è¸ªå®ƒä»¬ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªéå¸¸å¤§çš„ `map[string]int`ï¼ŒGC å¿…é¡»æ£€æŸ¥æ˜ å°„ä¸­çš„æ¯ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯ä¸ª GCï¼Œå› ä¸ºå­—ç¬¦ä¸²åŒ…å«æŒ‡é’ˆã€‚

In this example, we write 10 million elements to a `map[string]int`, and time the garbage collections. We allocate our map at the package scope to ensure it is heap allocated.

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°† 1000 ä¸‡ä¸ªå…ƒç´ å†™å…¥ `map[string]int`ï¼Œå¹¶å¯¹åƒåœ¾æ”¶é›†è®¡æ—¶ã€‚æˆ‘ä»¬åœ¨åŒ…èŒƒå›´å†…åˆ†é…æˆ‘ä»¬çš„æ˜ å°„ä»¥ç¡®ä¿å®ƒæ˜¯å †åˆ†é…çš„ã€‚

```go
package main

import (
    "fmt"
    "runtime"
    "strconv"
    "time"
)

const (
    numElements = 10000000
)

var foo = map[string]int{}

func timeGC() {
    t := time.Now()
    runtime.GC()
    fmt.Printf("gc took: %s\n", time.Since(t))
}

func main() {
    for i := 0;i < numElements;i++ {
        foo[strconv.Itoa(i)] = i
    }

    for {
        timeGC()
        time.Sleep(1 * time.Second)
    }
}

```

Running this program, we see the following:

è¿è¡Œè¿™ä¸ªç¨‹åºï¼Œæˆ‘ä»¬çœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š

```
ğŸ inthash â†’ go install && inthash
gc took: 98.726321ms
gc took: 105.524633ms
gc took: 102.829451ms
gc took: 102.71908ms
gc took: 103.084104ms
gc took: 104.821989ms

```

That's quite a while in computer land! ğŸ˜°

è¿™åœ¨è®¡ç®—æœºé¢†åŸŸå·²ç»å¾ˆé•¿æ—¶é—´äº†ï¼ ğŸ˜°

What could we do to improve it? Removing pointers wherever possible seems like a good idea - we'll reduce the number of pointers that the garbage collector has to chase. [Strings contain pointers](https://www.reddit.com/r/golang/comments/4ologg/why_is_byte_used_as_a_string_type/d4e6gy8/); so let's implement this as a `map[int]int`.

æˆ‘ä»¬å¯ä»¥åšäº›ä»€ä¹ˆæ¥æ”¹å–„å®ƒï¼Ÿå°½å¯èƒ½åˆ é™¤æŒ‡é’ˆä¼¼ä¹æ˜¯ä¸ªå¥½ä¸»æ„â€”â€”æˆ‘ä»¬å°†å‡å°‘åƒåœ¾æ”¶é›†å™¨å¿…é¡»è¿½é€çš„æŒ‡é’ˆæ•°é‡ã€‚ [å­—ç¬¦ä¸²åŒ…å«æŒ‡é’ˆ](https://www.reddit.com/r/golang/comments/4ologg/why_is_byte_used_as_a_string_type/d4e6gy8/);æ‰€ä»¥è®©æˆ‘ä»¬å°†å…¶å®ç°ä¸ºä¸€ä¸ª `map[int]int`ã€‚

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

const (
    numElements = 10000000
)

var foo = map[int]int{}

func timeGC() {
    t := time.Now()
    runtime.GC()
    fmt.Printf("gc took: %s\n", time.Since(t))
}

func main() {
    for i := 0;i < numElements;i++ {
        foo[i] = i
    }

    for {
        timeGC()
        time.Sleep(1 * time.Second)
    }
}

```

Running the program again, we get the following:

å†æ¬¡è¿è¡Œç¨‹åºï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```
ğŸ inthash â†’ go install && inthash
gc took: 3.608993ms
gc took: 3.926913ms
gc took: 3.955706ms
gc took: 4.063795ms
gc took: 3.91519ms
gc took: 3.75226ms

```

Much better. We've chopped 97% off garbage collection timings. In a production use case, you'd need to hash the strings to an integer before inserting ino the map.

å¥½å¤šäº†ã€‚æˆ‘ä»¬å·²ç»å°†åƒåœ¾æ”¶é›†æ—¶é—´ç¼©çŸ­äº† 97%ã€‚åœ¨ç”Ÿäº§ç”¨ä¾‹ä¸­ï¼Œæ‚¨éœ€è¦åœ¨æ’å…¥åœ°å›¾ä¹‹å‰å°†å­—ç¬¦ä¸²æ•£åˆ—ä¸ºæ•´æ•°ã€‚

â„¹ï¸ There's plenty more you can do to evade the GC. If you allocate giant arrays of pointerless structs, ints or bytes, [the GC will not scan it](https://medium.com/@rf_14423/did-the-big-allocations-of-ram-contain-pointers-directly-or-indirectly-actual-pointers-strings-76ed28c0bc92): meaning you pay no GC overhead. Such techniques usually require substantial re-working of the program, so we won't delve any further into them today.

â„¹ï¸ ä½ å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…æ¥é€ƒé¿ GCã€‚å¦‚æœä½ åˆ†é…å·¨å¤§çš„æ— æŒ‡é’ˆç»“æ„ã€æ•´æ•°æˆ–å­—èŠ‚æ•°ç»„ï¼Œ[GC ä¸ä¼šæ‰«æå®ƒ](https://medium.com/@rf_14423/did-the-big-allocations-of-ram-contain-pointers-directly-or-indirectly-actual-pointers-strings-76ed28c0bc92)ï¼šæ„å‘³ç€æ‚¨æ— éœ€æ”¯ä»˜ GC å¼€é”€ã€‚æ­¤ç±»æŠ€æœ¯é€šå¸¸éœ€è¦å¯¹ç¨‹åºè¿›è¡Œå¤§é‡é‡æ–°è®¾è®¡ï¼Œå› æ­¤æˆ‘ä»¬ä»Šå¤©ä¸ä¼šæ·±å…¥ç ”ç©¶å®ƒä»¬ã€‚

âš ï¸ As with all optimisation, your mileage may vary. See a [Twitter thread from Damian Gryski](https://twitter.com/dgryski/status/1140685755578118144) for an interesting example where removing strings from a large map in favour of a smarter data structure actually _increased_ memory. In general, you should read everything he puts out.

âš ï¸ä¸æ‰€æœ‰ä¼˜åŒ–ä¸€æ ·ï¼Œæ‚¨çš„é‡Œç¨‹å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚è¯·å‚é˜… [æ¥è‡ª Damian Gryski çš„ Twitter çº¿ç¨‹](https://twitter.com/dgryski/status/1140685755578118144) è·å–ä¸€ä¸ªæœ‰è¶£çš„ç¤ºä¾‹ï¼Œå…¶ä¸­ä»å¤§åœ°å›¾ä¸­åˆ é™¤å­—ç¬¦ä¸²ä»¥æ”¯æŒæ›´æ™ºèƒ½çš„æ•°æ®ç»“æ„å®é™…ä¸Š_å¢åŠ _äº†å†…å­˜ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä½ åº”è¯¥é˜…è¯»ä»–å‘è¡¨çš„æ‰€æœ‰å†…å®¹ã€‚

### Code generate marshalling code to avoid runtime reflection 

### ä»£ç ç”Ÿæˆç¼–ç»„ä»£ç ä»¥é¿å…è¿è¡Œæ—¶åå°„

Marshalling and unmarshalling your structure to and from various serialisation formats like JSON is a common operation; especially when building microservices. In fact, you'll often find that the only thing most microservices are actually doing is serialisation. Functions like `json.Marshal` and `json.Unmarshal` rely on [runtime reflection](https://blog.golang.org/laws-of-reflection) to serialise the struct fields to bytes, and vice versa. This can be slow: reflection is not anywhere near as performant as explicit code.

åœ¨å„ç§åºåˆ—åŒ–æ ¼å¼ï¼ˆå¦‚ JSONï¼‰ä¹‹é—´ç¼–ç»„å’Œè§£ç»„æ‚¨çš„ç»“æ„æ˜¯ä¸€ç§å¸¸è§æ“ä½œï¼›å°¤å…¶æ˜¯åœ¨æ„å»ºå¾®æœåŠ¡æ—¶ã€‚äº‹å®ä¸Šï¼Œæ‚¨ç»å¸¸ä¼šå‘ç°å¤§å¤šæ•°å¾®æœåŠ¡å®é™…ä¸Šå”¯ä¸€è¦åšçš„å°±æ˜¯åºåˆ—åŒ–ã€‚ `json.Marshal` å’Œ `json.Unmarshal` ç­‰å‡½æ•°ä¾èµ–äº [è¿è¡Œæ—¶åå°„](https://blog.golang.org/laws-of-reflection) å°†ç»“æ„å­—æ®µåºåˆ—åŒ–ä¸ºå­—èŠ‚ï¼Œåä¹‹äº¦ç„¶ã€‚è¿™å¯èƒ½å¾ˆæ…¢ï¼šåå°„è¿œä¸å¦‚æ˜¾å¼ä»£ç é‚£ä¹ˆé«˜æ•ˆã€‚

However, it doesn't have to be this way. The mechanics of marshalling JSON goes a little something like this:

ä½†æ˜¯ï¼Œä¸å¿…å¦‚æ­¤ã€‚ç¼–ç»„ JSON çš„æœºåˆ¶æœ‰ç‚¹åƒè¿™æ ·ï¼š

```go
package json

// Marshal take an object and returns its representation in JSON.
func Marshal(obj interface{}) ([]byte, error) {
    // Check if this object knows how to marshal itself to JSON
    // by satisfying the Marshaller interface.
    if m, is := obj.(json.Marshaller);is {
        return m.MarshalJSON()
    }

    // It doesn't know how to marshal itself.Do default reflection based marshallling.
    return marshal(obj)
}

```

If we know how to marshal ourselves to JSON, we have a hook for avoiding runtime reflection. But we don't want to hand write all of our marshalling code, so what do we do? Get computers to write code for us! Code generators like [easyjson](https://github.com/mailru/easyjson) look at a struct, and generate highly optimised code which is fully compatible with existing marshalling interfaces like `json.Marshaller`.

å¦‚æœæˆ‘ä»¬çŸ¥é“å¦‚ä½•å°†è‡ªå·±ç¼–ç»„åˆ° JSONï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªé¿å…è¿è¡Œæ—¶åå°„çš„é’©å­ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸æƒ³æ‰‹å†™æˆ‘ä»¬æ‰€æœ‰çš„ç¼–ç»„ä»£ç ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿè®©è®¡ç®—æœºä¸ºæˆ‘ä»¬ç¼–å†™ä»£ç ï¼åƒ [easyjson](https://github.com/mailru/easyjson) è¿™æ ·çš„ä»£ç ç”Ÿæˆå™¨æŸ¥çœ‹ä¸€ä¸ªç»“æ„ï¼Œå¹¶ç”Ÿæˆé«˜åº¦ä¼˜åŒ–çš„ä»£ç ï¼Œè¯¥ä»£ç ä¸ç°æœ‰çš„ç¼–ç»„æ¥å£ï¼ˆå¦‚ `json.Marshaller`)å®Œå…¨å…¼å®¹ã€‚

Download the package, and run the following on your `$file.go` containing the structs you would like to generate code for.

ä¸‹è½½åŒ…ï¼Œå¹¶åœ¨åŒ…å«è¦ä¸ºå…¶ç”Ÿæˆä»£ç çš„ç»“æ„çš„ `$file.go` ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤ã€‚

```
easyjson -all $file.go
```

You should find a `$file_easyjson.go` file has been generated. As `easyjson` has implemented the `json.Marshaller` interface for you, these functions will be called instead of the reflection based default. Congratulations: you just sped up your JSON marshalling code by 3x. There's lots of things that you can twiddle to increase the performance even more.

ä½ åº”è¯¥ä¼šå‘ç°å·²ç»ç”Ÿæˆäº†ä¸€ä¸ª `$file_easyjson.go` æ–‡ä»¶ã€‚ç”±äº`easyjson` å·²ç»ä¸ºä½ å®ç°äº†`json.Marshaller` æ¥å£ï¼Œè¿™äº›å‡½æ•°å°†è¢«è°ƒç”¨è€Œä¸æ˜¯åŸºäºåå°„çš„é»˜è®¤å€¼ã€‚æ­å–œï¼šæ‚¨çš„ JSON ç¼–ç»„ä»£ç é€Ÿåº¦æé«˜äº† 3 å€ã€‚æœ‰å¾ˆå¤šäº‹æƒ…ä½ å¯ä»¥æ‘†å¼„ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚

â„¹ï¸I recommend the package because I have used it before to good effect. Caveat emptor. Please do not take this as an invitation to start aggressive discussions about the fastest JSON marshalling packages on the market with me.

â„¹ï¸æˆ‘æ¨èè¿™ä¸ªåŒ…è£…ï¼Œå› ä¸ºæˆ‘ä¹‹å‰ç”¨è¿‡å®ƒï¼Œæ•ˆæœå¾ˆå¥½ã€‚ä¹°è€…è‡ªè´Ÿã€‚è¯·ä¸è¦ä»¥æ­¤ä¸ºé‚€è¯·ä¸æˆ‘å¼€å§‹ç§¯æè®¨è®ºå¸‚åœºä¸Šæœ€å¿«çš„ JSON ç¼–ç»„åŒ…ã€‚

âš ï¸ You'll need to make sure to re-generate the marshalling code when you change the struct. If you forget, new fields that you add won't be serialised and de-serialised, which can be confusing! You can use `go generate` to handle this code generation for you. In order to keep these in sync with structures, I like to have a `generate.go` in the root of the package that calls `go generate` for all files in the package: this can aid maintenance when you have many files that need generating. Top tip: call `go generate` in CI and check it has no diffs with the checked in code to ensure that structures are up to date.

âš ï¸ æ›´æ”¹ç»“æ„æ—¶ï¼Œæ‚¨éœ€è¦ç¡®ä¿é‡æ–°ç”Ÿæˆç¼–ç»„ä»£ç ã€‚å¦‚æœæ‚¨å¿˜è®°äº†ï¼Œæ‚¨æ·»åŠ çš„æ–°å­—æ®µä¸ä¼šè¢«åºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼Œè¿™å¯èƒ½ä¼šä»¤äººå›°æƒ‘ï¼æ‚¨å¯ä»¥ä½¿ç”¨ `go generate` ä¸ºæ‚¨å¤„ç†æ­¤ä»£ç ç”Ÿæˆã€‚ä¸ºäº†ä½¿è¿™äº›ä¸ç»“æ„ä¿æŒåŒæ­¥ï¼Œæˆ‘å–œæ¬¢åœ¨åŒ…çš„æ ¹ç›®å½•ä¸­æœ‰ä¸€ä¸ª `generate.go`ï¼Œå®ƒä¸ºåŒ…ä¸­çš„æ‰€æœ‰æ–‡ä»¶è°ƒç”¨ `go generate`ï¼šå½“ä½ æœ‰è®¸å¤šéœ€è¦çš„æ–‡ä»¶æ—¶ï¼Œè¿™å¯ä»¥å¸®åŠ©ç»´æŠ¤äº§ç”Ÿã€‚é‡è¦æç¤ºï¼šåœ¨ CI ä¸­è°ƒç”¨ `go generate` å¹¶æ£€æŸ¥å®ƒä¸ç­¾å…¥çš„ä»£ç æ²¡æœ‰å·®å¼‚ï¼Œä»¥ç¡®ä¿ç»“æ„æ˜¯æœ€æ–°çš„ã€‚

### Use `strings.Builder` to build up strings

### ä½¿ç”¨ `strings.Builder` æ¥æ„å»ºå­—ç¬¦ä¸²

In Go, strings are immutable: think of them as a read only slice of bytes. This means that every time you create a string, you're allocating new memory, and potentially creating more work for the garbage collector.

åœ¨ Go ä¸­ï¼Œå­—ç¬¦ä¸²æ˜¯ä¸å¯å˜çš„ï¼šå°†å®ƒä»¬è§†ä¸ºåªè¯»çš„å­—èŠ‚ç‰‡ã€‚è¿™æ„å‘³ç€æ¯æ¬¡åˆ›å»ºå­—ç¬¦ä¸²æ—¶ï¼Œéƒ½åœ¨åˆ†é…æ–°å†…å­˜ï¼Œå¹¶å¯èƒ½ä¸ºåƒåœ¾æ”¶é›†å™¨åˆ›å»ºæ›´å¤šå·¥ä½œã€‚

In Go 1.10, [`strings.Builder`](https://golang.org/pkg/strings/#Builder) was introduced as an efficient way to build up strings. Internally, it writes to a byte buffer. Only upon calling `String()` on the builder, is the string actually created. It relies on some `unsafe` trickery to return the underlying bytes as a string with zero allocations: see [this blog](https://syslog.ravelin.com/byte-vs-string-in-go-d645b67ca7ff) for a further look into how it works.

åœ¨ Go 1.10 ä¸­ï¼Œå¼•å…¥äº† [`strings.Builder`](https://golang.org/pkg/strings/#Builder) ä½œä¸ºæ„å»ºå­—ç¬¦ä¸²çš„æœ‰æ•ˆæ–¹æ³•ã€‚åœ¨å†…éƒ¨ï¼Œå®ƒå†™å…¥å­—èŠ‚ç¼“å†²åŒºã€‚åªæœ‰åœ¨æ„å»ºå™¨ä¸Šè°ƒç”¨ `String()` æ—¶ï¼Œå­—ç¬¦ä¸²æ‰ä¼šè¢«å®é™…åˆ›å»ºã€‚å®ƒä¾èµ–äºä¸€äº›â€œä¸å®‰å…¨â€çš„æŠ€å·§æ¥å°†åº•å±‚å­—èŠ‚ä½œä¸ºé›¶åˆ†é…çš„å­—ç¬¦ä¸²è¿”å›ï¼šè¯·å‚é˜…[æ­¤åšå®¢](https://syslog.ravelin.com/byte-vs-string-in-go-d645b67ca7ff)è¿›ä¸€æ­¥ç ”ç©¶å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

Let's do a performance comparison to verify the two approaches:

è®©æˆ‘ä»¬åšä¸€ä¸ªæ€§èƒ½æ¯”è¾ƒæ¥éªŒè¯è¿™ä¸¤ç§æ–¹æ³•ï¼š

```go
// main.go
package main

import "strings"

var strs = []string{
    "here's",
    "a",
    "some",
    "long",
    "list",
    "of",
    "strings",
    "for",
    "you",
}

func buildStrNaive() string {
    var s string

    for _, v := range strs {
        s += v
    }

    return s
}

func buildStrBuilder() string {
    b := strings.Builder{}

    // Grow the buffer to a decent length, so we don't have to continually
    // re-allocate.
    b.Grow(60)

    for _, v := range strs {
        b.WriteString(v)
    }

    return b.String()
}

```

```go
// main_test.go
package main

import (
    "testing"
)

var str string

func BenchmarkStringBuildNaive(b *testing.B) {
    for i := 0;i < b.N;i++ {
        str = buildStrNaive()
    }
}
func BenchmarkStringBuildBuilder(b *testing.B) {
    for i := 0;i < b.N;i++ {
        str = buildStrBuilder()
    }

```

I get the following results on my Macbook Pro:

æˆ‘åœ¨ Macbook Pro ä¸Šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š

```
ğŸ strbuild â†’ go test -bench=.-benchmem
goos: darwin
goarch: amd64
pkg: github.com/sjwhitworth/perfblog/strbuild
BenchmarkStringBuildNaive-8          5000000           255 ns/op         216 B/op          8 allocs/op
BenchmarkStringBuildBuilder-8       20000000            54.9 ns/op        64 B/op          1 allocs/op

```

As we can see, `strings.Builder` is 4.7x faster, results in 1/8th of the number of allocations, and 1/4 of the memory allocated.

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œ`strings.Builder` çš„é€Ÿåº¦æé«˜äº† 4.7 å€ï¼Œåˆ†é…æ¬¡æ•°æ˜¯ 1/8ï¼Œåˆ†é…çš„å†…å­˜æ˜¯ 1/4ã€‚

Where performance matters, use `strings.Builder`. In general, I recommend using it for all but the most trivial cases of building strings.

å¦‚æœæ€§èƒ½å¾ˆé‡è¦ï¼Œè¯·ä½¿ç”¨ `strings.Builder`ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘å»ºè®®å°†å®ƒç”¨äºé™¤äº†æ„å»ºå­—ç¬¦ä¸²çš„æœ€çç¢çš„æƒ…å†µä¹‹å¤–çš„æ‰€æœ‰æƒ…å†µã€‚

### Use strconv instead of fmt

### ä½¿ç”¨ strconv è€Œä¸æ˜¯ fmt

[`fmt`](https://golang.org/pkg/fmt/) is one of the most well known packages in Go. You'll have probably used it in your first Go program to print "hello, world" to the screen. However, when it comes to converting integers and floats in to strings, it's not as performant as its lower level cousin: [`strconv`](https://golang.org/pkg/strconv/). This package gives you a decent whack more performance, for some very small changes in API.

[`fmt`](https://golang.org/pkg/fmt/) æ˜¯ Go ä¸­æœ€è‘—åçš„åŒ…ä¹‹ä¸€ã€‚æ‚¨å¯èƒ½å·²ç»åœ¨ç¬¬ä¸€ä¸ª Go ç¨‹åºä¸­ä½¿ç”¨å®ƒåœ¨å±å¹•ä¸Šæ‰“å°â€œhello, worldâ€ã€‚ä½†æ˜¯ï¼Œåœ¨å°†æ•´æ•°å’Œæµ®ç‚¹æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ—¶ï¼Œå®ƒçš„æ€§èƒ½ä¸å¦‚å®ƒçš„ä½çº§è¡¨äº²ï¼š[`strconv`](https://golang.org/pkg/strconv/)ã€‚å¯¹äº API ä¸­çš„ä¸€äº›éå¸¸å°çš„æ›´æ”¹ï¼Œæ­¤åŒ…ä¸ºæ‚¨æä¾›äº†æ›´å¥½çš„æ€§èƒ½ã€‚

`fmt` mostly takes `interface{}` as arguments to functions. This has two downsides:

`fmt` ä¸»è¦å°† `interface{}` ä½œä¸ºå‡½æ•°çš„å‚æ•°ã€‚è¿™æœ‰ä¸¤ä¸ªç¼ºç‚¹ï¼š

- You lose type safety. This is a big one, for me.
- It can increase the number of allocations needed. Passing a non-pointer type as an`interface{}` usually causes heap allocations. Read into [this blog](https://www.darkcoding.net/software/go-the-price-of-interface/) to figure out why that is.

- ä½ å¤±å»äº†ç±»å‹å®‰å…¨ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„ï¼Œå¯¹æˆ‘æ¥è¯´ã€‚
- å®ƒå¯ä»¥å¢åŠ æ‰€éœ€çš„åˆ†é…æ•°é‡ã€‚å°†éæŒ‡é’ˆç±»å‹ä½œä¸º`interface{}` ä¼ é€’é€šå¸¸ä¼šå¯¼è‡´å †åˆ†é…ã€‚é˜…è¯» [æœ¬åšå®¢](https://www.darkcoding.net/software/go-the-price-of-interface/) æ‰¾å‡ºåŸå› ã€‚

The below program shows the difference in performance:

ä»¥ä¸‹ç¨‹åºæ˜¾ç¤ºäº†æ€§èƒ½å·®å¼‚ï¼š

```go
// main.go
package main

import (
    "fmt"
    "strconv"
)

func strconvFmt(a string, b int) string {
    return a + ":" + strconv.Itoa(b)
}

func fmtFmt(a string, b int) string {
    return fmt.Sprintf("%s:%d", a, b)
}

func main() {}

```

```go
// main_test.go
package main

import (
    "testing"
)

var (
    a    = "boo"
    blah = 42
    box  = ""
)

func BenchmarkStrconv(b *testing.B) {
    for i := 0;i < b.N;i++ {
        box = strconvFmt(a, blah)
    }
    a = box
}

func BenchmarkFmt(b *testing.B) {
    for i := 0;i < b.N;i++ {
        box = fmtFmt(a, blah)
    }
    a = box
}

```

The benchmark results on a Macbook Pro:

Macbook Pro çš„åŸºå‡†æµ‹è¯•ç»“æœï¼š

```go
ğŸ strfmt â†’ go test -bench=.-benchmem
goos: darwin
goarch: amd64
pkg: github.com/sjwhitworth/perfblog/strfmt
BenchmarkStrconv-8      30000000            39.5 ns/op        32 B/op          1 allocs/op
BenchmarkFmt-8          10000000           143 ns/op          72 B/op          3 allocs/op

```

We can see that the `strconv` version is 3.5x faster, results in 1/3rd the number of allocations, and half the memory allocated.

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `strconv` ç‰ˆæœ¬å¿«äº† 3.5 å€ï¼Œå¯¼è‡´åˆ†é…æ•°é‡å‡å°‘ 1/3ï¼Œåˆ†é…çš„å†…å­˜å‡å°‘ä¸€åŠã€‚

### Allocate capacity in make to avoid re-allocation

### åœ¨makeä¸­åˆ†é…å®¹é‡ä»¥é¿å…é‡æ–°åˆ†é…

Before we get to performance improvements, let's take a quick refresher on slices. The slice is a very useful construct in Go. It provides a re-sizable array, with the ability to take different views on the same underlying memory without re-allocation. If you peek under the hood, the slice is made up of three elements:

åœ¨æˆ‘ä»¬å¼€å§‹æ€§èƒ½æ”¹è¿›ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹åˆ‡ç‰‡ã€‚åˆ‡ç‰‡æ˜¯ Go ä¸­éå¸¸æœ‰ç”¨çš„æ„é€ ã€‚å®ƒæä¾›äº†ä¸€ä¸ªå¯é‡æ–°è°ƒæ•´å¤§å°çš„æ•°ç»„ï¼Œèƒ½å¤Ÿåœ¨ä¸é‡æ–°åˆ†é…çš„æƒ…å†µä¸‹å¯¹åŒä¸€åº•å±‚å†…å­˜é‡‡å–ä¸åŒçš„è§‚ç‚¹ã€‚å¦‚æœä½ åœ¨å¼•æ“ç›–ä¸‹å·çœ‹ï¼Œåˆ‡ç‰‡ç”±ä¸‰ä¸ªå…ƒç´ ç»„æˆï¼š

```go
type slice struct {
    // pointer to underlying data in the slice.
    data uintptr
    // the number of elements in the slice.
    len int
    // the number of elements that the slice can
    // grow to before a new underlying array
    // is allocated.
    cap int
}
```

What are these fields?

è¿™äº›é¢†åŸŸæ˜¯ä»€ä¹ˆï¼Ÿ

- `data`: pointer to underlying data in the slice
- `len`: the current number of elements in the slice.
- `cap`: the number of elements that the slice can grow to before re-allocation.

- `data`ï¼šæŒ‡å‘åˆ‡ç‰‡ä¸­åº•å±‚æ•°æ®çš„æŒ‡é’ˆ
- `len`ï¼šåˆ‡ç‰‡ä¸­çš„å½“å‰å…ƒç´ æ•°ã€‚
- `cap`ï¼šåœ¨é‡æ–°åˆ†é…ä¹‹å‰åˆ‡ç‰‡å¯ä»¥å¢é•¿åˆ°çš„å…ƒç´ æ•°é‡ã€‚

Under the hood, slices are fixed length arrays. When you reach the `cap` of a slice, a new array with double the cap of the previous slice is allocated, the memory is copied over from the old slice to the new one, and the old array is discarded

åœ¨å¼•æ“ç›–ä¸‹ï¼Œåˆ‡ç‰‡æ˜¯å›ºå®šé•¿åº¦çš„æ•°ç»„ã€‚å½“ä½ åˆ°è¾¾ä¸€ä¸ªåˆ‡ç‰‡çš„â€˜capâ€™æ—¶ï¼Œä¼šåˆ†é…ä¸€ä¸ªæ–°çš„æ•°ç»„ï¼Œå®ƒçš„å¤§å°æ˜¯å‰ä¸€ä¸ªåˆ‡ç‰‡çš„ä¸¤å€ï¼Œå†…å­˜ä»æ—§åˆ‡ç‰‡å¤åˆ¶åˆ°æ–°åˆ‡ç‰‡ï¼Œæ—§æ•°ç»„è¢«ä¸¢å¼ƒ

I often see code like the following that allocates a slice with zero capacity, when the capacity of the slice is known upfront.

æˆ‘ç»å¸¸çœ‹åˆ°ç±»ä¼¼ä¸‹é¢çš„ä»£ç ï¼Œå½“åˆ‡ç‰‡çš„å®¹é‡é¢„å…ˆçŸ¥é“æ—¶ï¼Œå®ƒä¼šåˆ†é…ä¸€ä¸ªå®¹é‡ä¸ºé›¶çš„åˆ‡ç‰‡ã€‚

```go
var userIDs []string
for _, bar := range rsp.Users {
    userIDs = append(userIDs, bar.ID)
}
```

In this case, the slice starts off with zero length, and zero capacity. After receiving the response, we append the users to the slice. As we do so, we hit the capacity of the slice: a new underlying array is allocated that is double the capacity of the previous slice, and the data from the slice is copied into it. If we had 8 users in the response, this would result in 5 allocations.

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåˆ‡ç‰‡ä»é›¶é•¿åº¦å’Œé›¶å®¹é‡å¼€å§‹ã€‚æ”¶åˆ°å“åº”åï¼Œæˆ‘ä»¬å°†ç”¨æˆ·é™„åŠ åˆ°åˆ‡ç‰‡ã€‚å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†åˆ‡ç‰‡çš„å®¹é‡ï¼šåˆ†é…äº†ä¸€ä¸ªæ–°çš„åº•å±‚æ•°ç»„ï¼Œå®ƒæ˜¯å‰ä¸€ä¸ªåˆ‡ç‰‡å®¹é‡çš„ä¸¤å€ï¼Œå¹¶å°†åˆ‡ç‰‡ä¸­çš„æ•°æ®å¤åˆ¶åˆ°å…¶ä¸­ã€‚å¦‚æœæˆ‘ä»¬åœ¨å“åº”ä¸­æœ‰ 8 ä¸ªç”¨æˆ·ï¼Œè¿™å°†å¯¼è‡´ 5 ä¸ªåˆ†é…ã€‚

A far more efficient way is to change it to the following:

æ›´æœ‰æ•ˆçš„æ–¹æ³•æ˜¯å°†å…¶æ›´æ”¹ä¸ºä»¥ä¸‹å†…å®¹ï¼š

```go
userIDs := make([]string, 0, len(rsp.Users)

for _, bar := range rsp.Users {
    userIDs = append(userIDs, bar.ID)
}

```

We have explicitly allocated the capacity to the slice by using make. Now, we can append to the slice, knowing that we won't trigger additional allocations and copys.

æˆ‘ä»¬å·²ç»ä½¿ç”¨ make æ˜¾å¼åœ°ä¸ºåˆ‡ç‰‡åˆ†é…äº†å®¹é‡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿½åŠ åˆ°åˆ‡ç‰‡ï¼ŒçŸ¥é“æˆ‘ä»¬ä¸ä¼šè§¦å‘é¢å¤–çš„åˆ†é…å’Œå¤åˆ¶ã€‚

If you don't know how much you should allocate because the capacity is dynamic or calculated later in the program, measure the distribution of the size of the slice that you end up with whilst the program is running. I usually take the 90th or 99th percentile, and hardcode the value in the progam. In cases where you have RAM to trade off for CPU, set this value to higher than you think you'll need.

å¦‚æœæ‚¨ä¸çŸ¥é“åº”è¯¥åˆ†é…å¤šå°‘å®¹é‡ï¼Œå› ä¸ºå®¹é‡æ˜¯åŠ¨æ€çš„æˆ–åœ¨ç¨‹åºç¨åè®¡ç®—ï¼Œè¯·æµ‹é‡ç¨‹åºè¿è¡Œæ—¶æœ€ç»ˆå¾—åˆ°çš„åˆ‡ç‰‡å¤§å°çš„åˆ†å¸ƒã€‚æˆ‘é€šå¸¸å–ç¬¬ 90 ä¸ªæˆ–ç¬¬ 99 ä¸ªç™¾åˆ†ä½æ•°ï¼Œå¹¶åœ¨ç¨‹åºä¸­ç¡¬ç¼–ç è¯¥å€¼ã€‚å¦‚æœæ‚¨æœ‰ RAM æ¥æ¢å– CPUï¼Œè¯·å°†æ­¤å€¼è®¾ç½®ä¸ºé«˜äºæ‚¨è®¤ä¸ºéœ€è¦çš„å€¼ã€‚

This advice is also applicable to maps: using `make(map[string]string, len(foo))` will allocate enough capacity under the hood to avoid re-allocation.

è¿™ä¸ªå»ºè®®ä¹Ÿé€‚ç”¨äºåœ°å›¾ï¼šä½¿ç”¨ `make(map[string]string, len(foo))` å°†åœ¨å¼•æ“ç›–ä¸‹åˆ†é…è¶³å¤Ÿçš„å®¹é‡æ¥é¿å…é‡æ–°åˆ†é…ã€‚

â„¹ï¸ See [Go Slices: usage and internals](https://blog.golang.org/go-slices-usage-and-internals) for more information about how slices work under the hood.

â„¹ï¸ è¯·å‚é˜… [Go Slicesï¼šç”¨æ³•å’Œå†…éƒ¨ç»“æ„](https://blog.golang.org/go-slices-usage-and-internals) ä»¥äº†è§£æœ‰å…³åˆ‡ç‰‡å¦‚ä½•åœ¨å¹•åå·¥ä½œçš„æ›´å¤šä¿¡æ¯ã€‚

### Use methods that allow you to pass byte slices

### ä½¿ç”¨å…è®¸æ‚¨ä¼ é€’å­—èŠ‚åˆ‡ç‰‡çš„æ–¹æ³•

When using packages, look to use methods that allow you to pass a byte slice: these methods usually give you more control over allocation.

ä½¿ç”¨åŒ…æ—¶ï¼Œè¯·æ³¨æ„ä½¿ç”¨å…è®¸æ‚¨ä¼ é€’å­—èŠ‚åˆ‡ç‰‡çš„æ–¹æ³•ï¼šè¿™äº›æ–¹æ³•é€šå¸¸å¯ä»¥è®©æ‚¨æ›´å¥½åœ°æ§åˆ¶åˆ†é…ã€‚

[`time.Format`](https://golang.org/pkg/time/#Time.Format) vs. [`time.AppendFormat`](https://golang.org/pkg/time/#Time.AppendFormat) is a good example. `time.Format` returns a string. Under the hood, this allocates a new byte slice and calls `time.AppendFormat` on it. `time.AppendFormat` takes a byte buffer, writes the formatted representation of the time, and returns the extended byte slice. This is common in other packages in the standard library: see [`strconv.AppendFloat`](https://golang.org/pkg/strconv/#AppendFloat), or [`bytes.NewBuffer`](https://golang.org/pkg/bytes/#NewBuffer).

[`time.Format`](https://golang.org/pkg/time/#Time.Format) ä¸ [`time.AppendFormat`](https://golang.org/pkg/time/#Time.AppendFormat) å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚ `time.Format` è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚åœ¨å¹•åï¼Œè¿™ä¼šåˆ†é…ä¸€ä¸ªæ–°çš„å­—èŠ‚åˆ‡ç‰‡å¹¶å¯¹å…¶è°ƒç”¨`time.AppendFormat`ã€‚ `time.AppendFormat` æ¥å—ä¸€ä¸ªå­—èŠ‚ç¼“å†²åŒºï¼Œå†™å…¥æ—¶é—´çš„æ ¼å¼åŒ–è¡¨ç¤ºï¼Œå¹¶è¿”å›æ‰©å±•å­—èŠ‚ç‰‡ã€‚è¿™åœ¨æ ‡å‡†åº“ä¸­çš„å…¶ä»–åŒ…ä¸­å¾ˆå¸¸è§ï¼šè¯·å‚é˜… [`strconv.AppendFloat`](https://golang.org/pkg/strconv/#AppendFloat) æˆ– [`bytes.NewBuffer`](https://golang.org/pkg/bytes/#NewBuffer)ã€‚

Why does this give you increased performance? Well, you can now pass byte slices that you've obtained from your `sync.Pool`, instead of allocating a new buffer every time. Or you can increase the initial buffer size to a value that you know is more suited to your program, to reduce slice re-copying.

ä¸ºä»€ä¹ˆè¿™å¯ä»¥æé«˜æ€§èƒ½ï¼Ÿå¥½å§ï¼Œæ‚¨ç°åœ¨å¯ä»¥ä¼ é€’ä»â€œsync.Poolâ€ä¸­è·å¾—çš„å­—èŠ‚åˆ‡ç‰‡ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½åˆ†é…ä¸€ä¸ªæ–°ç¼“å†²åŒºã€‚æˆ–è€…æ‚¨å¯ä»¥å°†åˆå§‹ç¼“å†²åŒºå¤§å°å¢åŠ åˆ°æ‚¨çŸ¥é“æ›´é€‚åˆæ‚¨çš„ç¨‹åºçš„å€¼ï¼Œä»¥å‡å°‘åˆ‡ç‰‡é‡æ–°å¤åˆ¶ã€‚

## Summary

##  æ¦‚æ‹¬

After reading this post, you should be able to take these techniques and apply them to your code base. Over time, you will build a mental model for reasoning about performance in Go programs. This can aid upfront design greatly.

é˜…è¯»å®Œè¿™ç¯‡æ–‡ç« åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿé‡‡ç”¨è¿™äº›æŠ€æœ¯å¹¶å°†å®ƒä»¬åº”ç”¨åˆ°æ‚¨çš„ä»£ç åº“ä¸­ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä½ å°†å»ºç«‹ä¸€ä¸ªæ€ç»´æ¨¡å‹æ¥æ¨ç† Go ç¨‹åºçš„æ€§èƒ½ã€‚è¿™å¯ä»¥æå¤§åœ°å¸®åŠ©å‰æœŸè®¾è®¡ã€‚

To close, a word of caution. Take my guidance as situationally-dependent advice, not gospel. Benchmark and measure.

è¦å…³é—­ï¼Œè¯·æ³¨æ„ã€‚å°†æˆ‘çš„æŒ‡å¯¼ä½œä¸ºè§†æƒ…å†µè€Œå®šçš„å»ºè®®ï¼Œè€Œä¸æ˜¯ç¦éŸ³ã€‚åŸºå‡†å’Œæµ‹é‡ã€‚

Know when to stop. Improving performance of a system is a feel-good exercise for an engineer: the problem is interesting, and the results are immediately visible. However, the usefulness of performance improvements is very dependent on the situation. If your service takes 10ms to respond, and 90ms round trip on the network, it doesn't seem worth it to try and halve that 10ms to 5ms: you'll still take 95ms. If you manage to optimise it to within an inch of it's life so it takes 1ms to respond, you're still only at 91ms. There are probably bigger fish to fry.

çŸ¥é“ä»€ä¹ˆæ—¶å€™åœæ­¢ã€‚æé«˜ç³»ç»Ÿæ€§èƒ½å¯¹å·¥ç¨‹å¸ˆæ¥è¯´æ˜¯ä¸€ç§æ„Ÿè§‰è‰¯å¥½çš„ç»ƒä¹ ï¼šé—®é¢˜å¾ˆæœ‰è¶£ï¼Œç»“æœç«‹å³å¯è§ã€‚ä½†æ˜¯ï¼Œæ€§èƒ½æ”¹è¿›çš„æœ‰ç”¨æ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæƒ…å†µã€‚å¦‚æœæ‚¨çš„æœåŠ¡éœ€è¦ 10 æ¯«ç§’æ¥å“åº”ï¼Œå¹¶ä¸”åœ¨ç½‘ç»œä¸Šå¾€è¿”éœ€è¦ 90 æ¯«ç§’ï¼Œé‚£ä¹ˆå°è¯•å°† 10 æ¯«ç§’å‡åŠåˆ° 5 æ¯«ç§’ä¼¼ä¹ä¸å€¼å¾—ï¼šæ‚¨ä»ç„¶éœ€è¦ 95 æ¯«ç§’ã€‚å¦‚æœä½ è®¾æ³•å°†å®ƒä¼˜åŒ–åˆ°å®ƒçš„ç”Ÿå‘½çš„ä¸€è‹±å¯¸ä¹‹å†…ï¼Œæ‰€ä»¥å®ƒéœ€è¦ 1 æ¯«ç§’çš„å“åº”æ—¶é—´ï¼Œé‚£ä¹ˆä½ ä»ç„¶åªæœ‰ 91 æ¯«ç§’ã€‚å¯èƒ½æœ‰æ›´å¤§çš„é±¼è¦ç…ã€‚

Optimise wisely!

æ˜æ™ºåœ°ä¼˜åŒ–ï¼

### References

###  å‚è€ƒ

I've linked heavily throughout the blog. If you're interested in further reading, the following are great sources of inspiration:

æˆ‘åœ¨æ•´ä¸ªåšå®¢ä¸­éƒ½æœ‰å¤§é‡é“¾æ¥ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£è¿›ä¸€æ­¥é˜…è¯»ï¼Œä»¥ä¸‹æ˜¯å¾ˆå¥½çš„çµæ„Ÿæ¥æºï¼š

- [Further dangers of large heaps in Go](https://syslog.ravelin.com/further-dangers-of-large-heaps-in-go-7a267b57d487)
- [Allocation efficiency in high performance Go services](https://segment.com/blog/allocation-efficiency-in-high-performance-go-services/) 

- [Go ä¸­å¤§å †çš„è¿›ä¸€æ­¥å±é™©](https://syslog.ravelin.com/further-dangers-of-large-heaps-in-go-7a267b57d487)
- [é«˜æ€§èƒ½ Go æœåŠ¡ä¸­çš„åˆ†é…æ•ˆç‡](https://segment.com/blog/allocation-efficiency-in-high-performance-go-services/)

