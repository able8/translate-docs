# Reverse Proxy, HTTP Keep-Alive Timeout, and sporadic HTTP 502s

# åå‘ä»£ç†ã€HTTP Keep-Alive è¶…æ—¶å’Œé›¶æ˜Ÿçš„ HTTP 502

https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s

https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s

## How HTTP Keep-Alive can cause TCP race condition

## HTTP Keep-Alive å¦‚ä½•å¯¼è‡´ TCP ç«äº‰æ¡ä»¶

September 30, 2021

2021 å¹´ 9 æœˆ 30 æ—¥

These mysterious HTTP 502s happened to me already twice over the past few years. Since the amount of service-to-service communications every year goes only up, I expect more and more people to experience the same issue. So, sharing it here.

è¿™äº›ç¥ç§˜çš„ HTTP 502 åœ¨è¿‡å»å‡ å¹´ä¸­å·²ç»å‘ç”Ÿè¿‡ä¸¤æ¬¡ã€‚ç”±äºæ¯å¹´æœåŠ¡é—´çš„é€šä¿¡é‡åªå¢ä¸å‡ï¼Œæˆ‘é¢„è®¡ä¼šæœ‰è¶Šæ¥è¶Šå¤šçš„äººé‡åˆ°åŒæ ·çš„é—®é¢˜ã€‚æ‰€ä»¥ï¼Œåˆ†äº«åˆ°è¿™é‡Œã€‚

**TL;DR:** HTTP Keep-Alive between a reverse proxy and an upstream server combined with some misfortunate downstream- and upstream-side timeout settings can make clients receiving HTTP 502s from the proxy.

**TL;DR:** åå‘ä»£ç†å’Œä¸Šæ¸¸æœåŠ¡å™¨ä¹‹é—´çš„ HTTP Keep-Alive ç»“åˆä¸€äº›ä¸å¹¸çš„ä¸‹æ¸¸å’Œä¸Šæ¸¸ç«¯è¶…æ—¶è®¾ç½®å¯ä»¥ä½¿å®¢æˆ·ç«¯ä»ä»£ç†æ¥æ”¶ HTTP 502ã€‚

[![How HTTP Keep-Alive between a reverse proxy and an upstream can cause HTTP 502s due to a TCP race condition.](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond-2000-opt.png)](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond.png)

tcp-conn-race-cond-2000-opt.png)](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond.png)

Below is a longer version...

ä¸‹é¢æ˜¯ä¸€ä¸ªæ›´é•¿çš„ç‰ˆæœ¬...

[HTTP 502](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502) status code, also know as `Bad Gateway` indicates that a server, while acting as a gateway or proxy, received an invalid response from the upstream server. Typical problems causing it ( [summarized by Datadog folks](https://www.datadoghq.com/blog/nginx-502-bad-gateway-errors-gunicorn/)):

[HTTP 502](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502) çŠ¶æ€ä»£ç ï¼Œä¹Ÿç§°ä¸ºâ€œBad Gatewayâ€ï¼Œè¡¨ç¤ºæœåŠ¡å™¨åœ¨å……å½“ç½‘å…³æˆ–ä»£ç†ï¼Œæ”¶åˆ°æ¥è‡ªä¸Šæ¸¸æœåŠ¡å™¨çš„æ— æ•ˆå“åº”ã€‚å¯¼è‡´å®ƒçš„å…¸å‹é—®é¢˜ï¼ˆ[ç”± Datadog äººå‘˜æ€»ç»“](https://www.datadoghq.com/blog/nginx-502-bad-gateway-errors-gunicorn/))ï¼š

- Upstream server isn't running
- Upstream server times out
- Reverse proxy is misconfigured (e.g., my favourite one - trying to call uWSGI over`HTTP` while it listens on `uwsgi` protocol ğŸ™ˆ)

- ä¸Šæ¸¸æœåŠ¡å™¨æœªè¿è¡Œ
- ä¸Šæ¸¸æœåŠ¡å™¨è¶…æ—¶
- åå‘ä»£ç†é…ç½®é”™è¯¯ï¼ˆä¾‹å¦‚ï¼Œæˆ‘æœ€å–œæ¬¢çš„ä¸€ä¸ª - åœ¨å®ƒç›‘å¬ `uwsgi` åè®®æ—¶è¯•å›¾é€šè¿‡ `HTTP` è°ƒç”¨ uWSGI ğŸ™ˆï¼‰

However, sometimes there seems to be no apparent reason for HTTP 502s responses while clients sporadically see them:

ç„¶è€Œï¼Œæœ‰æ—¶ä¼¼ä¹æ²¡æœ‰æ˜æ˜¾çš„ HTTP 502 å“åº”åŸå› ï¼Œè€Œå®¢æˆ·ç«¯å¶å°”ä¼šçœ‹åˆ°å®ƒä»¬ï¼š

- [NodeJs application behind Amazon ELB throws 502](https://stackoverflow.com/questions/45626787/nodejs-application-behind-amazon-elb-throws-502)
- [Proxy persistent upstream: occasional 502](https://github.com/h2o/h2o/issues/281)
- [Sporadic 502 response only when running through traefik](https://github.com/traefik/traefik/issues/3237)
- [HTTP 502 response generated by a proxy after it tries to send data upstream to a partially closed connection](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a)

- [Amazon ELB èƒŒåçš„ NodeJs åº”ç”¨ç¨‹åºæŠ›å‡º 502](https://stackoverflow.com/questions/45626787/nodejs-application-behind-amazon-elb-throws-502)
- [ä»£ç†æŒä¹…ä¸Šæ¸¸ï¼šå¶å°”å‡ºç° 502](https://github.com/h2o/h2o/issues/281)
- [ä»…åœ¨é€šè¿‡ traefik è¿è¡Œæ—¶å‡ºç°é›¶æ˜Ÿ 502 å“åº”](https://github.com/traefik/traefik/issues/3237)
- [ä»£ç†å°è¯•å°†æ•°æ®å‘ä¸Šæ¸¸å‘é€åˆ°éƒ¨åˆ†å…³é—­çš„è¿æ¥åç”Ÿæˆçš„ HTTP 502 å“åº”](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a)

For me personally, it happened twice already - the first time, with a Node.js application running behind AWS ALB; and the second time, with a Python (uWSGI) application running behind Traefik reverse proxy. And it took quite some time for my team to debug it back in the day (we even involved premium AWS support to pin down the root cause).

å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œå®ƒå·²ç»å‘ç”Ÿäº†ä¸¤æ¬¡ - ç¬¬ä¸€æ¬¡æ˜¯åœ¨ AWS ALB åé¢è¿è¡Œä¸€ä¸ª Node.js åº”ç”¨ç¨‹åºï¼›ç¬¬äºŒæ¬¡ï¼Œä½¿ç”¨åœ¨ Traefik åå‘ä»£ç†åé¢è¿è¡Œçš„ Python (uWSGI) åº”ç”¨ç¨‹åºã€‚å½“å¤©æˆ‘çš„å›¢é˜ŸèŠ±äº†ç›¸å½“é•¿çš„æ—¶é—´æ¥è°ƒè¯•å®ƒï¼ˆæˆ‘ä»¬ç”šè‡³è®©é«˜çº§ AWS æ”¯æŒæ¥ç¡®å®šæ ¹æœ¬åŸå› ï¼‰ã€‚

Long story short, it was a **TCP race condition**.

é•¿è¯çŸ­è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ª **TCP ç«äº‰æ¡ä»¶**ã€‚

A detailed explanation of the problem, including some low-level TCP packet analysis, can be found in [this lovely article](https://www.tessian.com/blog/how-to-fix-http-502-errors/). And here, I'll just try to give a quick summary.

å¯ä»¥åœ¨[è¿™ç¯‡å¯çˆ±çš„æ–‡ç« ](https://www.tessian.com/blog/how-to-fix-http-502-errors/).åœ¨è¿™é‡Œï¼Œæˆ‘å°†å°è¯•ç»™å‡ºä¸€ä¸ªå¿«é€Ÿæ€»ç»“ã€‚

First off, any TCP connection is a distributed system. Well, it should make sense - the actual state of a connection is distributed between its [remote] endpoints. And as the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) dictates, any distributed system can be either available 100% of the time or consistent 100% of the time, but not both. TCP obviously chooses the availability. Hence, the **state of a TCP connection is only eventually consistent**!

é¦–å…ˆï¼Œä»»ä½• TCP è¿æ¥éƒ½æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿã€‚å¥½å§ï¼Œå®ƒåº”è¯¥æ˜¯æœ‰é“ç†çš„â€”â€”è¿æ¥çš„å®é™…çŠ¶æ€åˆ†å¸ƒåœ¨å®ƒçš„ [è¿œç¨‹] ç«¯ç‚¹ä¹‹é—´ã€‚æ­£å¦‚ [CAP å®šç†](https://en.wikipedia.org/wiki/CAP_theorem) æ‰€æŒ‡å‡ºçš„ï¼Œä»»ä½•åˆ†å¸ƒå¼ç³»ç»Ÿéƒ½å¯ä»¥ 100% å¯ç”¨æˆ– 100% ä¸€è‡´ï¼Œä½†ä¸èƒ½ä¸¤è€…å…¼è€Œæœ‰ä¹‹ã€‚ TCPæ˜¾ç„¶é€‰æ‹©å¯ç”¨æ€§ã€‚å› æ­¤ï¼Œ**TCP è¿æ¥çš„çŠ¶æ€åªæ˜¯æœ€ç»ˆä¸€è‡´çš„**ï¼

Why does it matter for us?

ä¸ºä»€ä¹ˆè¿™å¯¹æˆ‘ä»¬å¾ˆé‡è¦ï¼Ÿ

While this [serverfault](https://serverfault.com/a/426741/292251) answer says that the HTTP Keep-Alive should be used only for the client-to-proxy communications, from my experience, proxies often keep the upstream connections alive too. This is a so-called _connection pool pattern_ when just a few connections are heavily reused to handle numerous requests. 

è™½ç„¶è¿™ä¸ª [serverfault](https://serverfault.com/a/426741/292251) å›ç­”è¯´ HTTP Keep-Alive åº”è¯¥åªç”¨äºå®¢æˆ·ç«¯åˆ°ä»£ç†çš„é€šä¿¡ï¼Œæ ¹æ®æˆ‘çš„ç»éªŒï¼Œä»£ç†é€šå¸¸ä¿æŒä¸Šæ¸¸è¿æ¥ä¹Ÿè¿˜æ´»ç€ã€‚è¿™å°±æ˜¯æ‰€è°“çš„_è¿æ¥æ± æ¨¡å¼_ï¼Œåªæœ‰å°‘æ•°è¿æ¥è¢«å¤§é‡é‡ç”¨æ¥å¤„ç†å¤§é‡è¯·æ±‚ã€‚

When a connection stays in the pool for too long without being reused, it's marked as _idle_. Normally, idle connections are closed after some period of inactivity. However, the server-side (i.e., the upstream) also can drop idle connections. So, what sometimes happens is that the proxy and the upstream have some misfortunate idle connection timeout settings. More specifically, if the proxy and the upstream have exactly the same idle timeout durations or the upstream drops connection sooner than the proxy.

å½“ä¸€ä¸ªè¿æ¥åœ¨æ± ä¸­åœç•™å¤ªä¹…è€Œæ²¡æœ‰è¢«é‡ç”¨æ—¶ï¼Œå®ƒè¢«æ ‡è®°ä¸º_idle_ã€‚é€šå¸¸ï¼Œç©ºé—²è¿æ¥ä¼šåœ¨ä¸€æ®µæ—¶é—´ä¸æ´»åŠ¨åå…³é—­ã€‚ä½†æ˜¯ï¼ŒæœåŠ¡å™¨ç«¯ï¼ˆå³ä¸Šæ¸¸ï¼‰ä¹Ÿå¯ä»¥ä¸¢å¼ƒç©ºé—²è¿æ¥ã€‚å› æ­¤ï¼Œæœ‰æ—¶ä¼šå‘ç”Ÿä»£ç†å’Œä¸Šæ¸¸æœ‰ä¸€äº›ä¸å¹¸çš„ç©ºé—²è¿æ¥è¶…æ—¶è®¾ç½®ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå¦‚æœä»£ç†å’Œä¸Šæ¸¸å…·æœ‰å®Œå…¨ç›¸åŒçš„ç©ºé—²è¶…æ—¶æŒç»­æ—¶é—´ï¼Œæˆ–è€…ä¸Šæ¸¸æ¯”ä»£ç†æ›´æ—©æ–­å¼€è¿æ¥ã€‚

While one endpoint (the proxy) can still be thinking that the connection is totally fine, the other endpoint may have already started closing the connection (the upstream).

è™½ç„¶ä¸€ä¸ªç«¯ç‚¹ï¼ˆä»£ç†ï¼‰ä»ç„¶è®¤ä¸ºè¿æ¥å®Œå…¨æ­£å¸¸ï¼Œä½†å¦ä¸€ä¸ªç«¯ç‚¹å¯èƒ½å·²ç»å¼€å§‹å…³é—­è¿æ¥ï¼ˆä¸Šæ¸¸ï¼‰ã€‚

So, the proxy starts writing a request into a connection that is being closed by the upstream. And instead of getting the TCP `ACK` for the sent bytes, the proxy gets TCP `FIN` (or even `RST`) from the upstream. Oftentimes, the proxy just gives up on such requests and responds with immediate HTTP 502 to the client. **And this is completely invisible on the application side!** So, no application logs will be helpful in such a case.

å› æ­¤ï¼Œä»£ç†å¼€å§‹å°†è¯·æ±‚å†™å…¥è¢«ä¸Šæ¸¸å…³é—­çš„è¿æ¥ã€‚ä»£ç†ä¸æ˜¯è·å–å·²å‘é€å­—èŠ‚çš„ TCP `ACK`ï¼Œè€Œæ˜¯ä»ä¸Šæ¸¸è·å– TCP `FIN`ï¼ˆç”šè‡³æ˜¯ `RST`ï¼‰ã€‚é€šå¸¸ï¼Œä»£ç†ä¼šæ”¾å¼ƒæ­¤ç±»è¯·æ±‚å¹¶ç«‹å³å‘å®¢æˆ·ç«¯å“åº” HTTP 502ã€‚ **è¿™åœ¨åº”ç”¨ç¨‹åºç«¯æ˜¯å®Œå…¨ä¸å¯è§çš„ï¼**å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»»ä½•åº”ç”¨ç¨‹åºæ—¥å¿—éƒ½æ— æµäºäº‹ã€‚

> HTTP Keep-Alive can cause TCP race conditions ğŸ¤¯
>
> Ok, it happened again... Hence, the diagram!
>
> In a (reverse\_proxy + app\_server) setup, the reverse proxy should be dropping connections more frequently than the app server. Otherwise, clients will see sporadic HTTP 502s.
>
> Why? ğŸ”½ [pic.twitter.com/YBb2tP3Zfg](https://t.co/YBb2tP3Zfg)
>
> â€” Ivan Velichko (@iximiuz) [September 29, 2021](https://twitter.com/iximiuz/status/1443305889616863238?ref_src=twsrc%5Etfw)

> HTTP Keep-Alive ä¼šå¯¼è‡´ TCP ç«äº‰æ¡ä»¶ğŸ¤¯
>
> å¥½å§ï¼Œå®ƒåˆå‘ç”Ÿäº†â€¦â€¦å› æ­¤ï¼Œå›¾è¡¨ï¼
>
> åœ¨ (reverse\_proxy + app\_server) è®¾ç½®ä¸­ï¼Œåå‘ä»£ç†åº”è¯¥æ¯”åº”ç”¨æœåŠ¡å™¨æ›´é¢‘ç¹åœ°æ–­å¼€è¿æ¥ã€‚å¦åˆ™ï¼Œå®¢æˆ·ç«¯å°†çœ‹åˆ°é›¶æ˜Ÿçš„ HTTP 502ã€‚
>
> ä¸ºä»€ä¹ˆï¼Ÿ ğŸ”½ [pic.twitter.com/YBb2tP3Zfg](https://t.co/YBb2tP3Zfg)
>
> â€” Ivan Velichko (@iximiuz) [2021 å¹´ 9 æœˆ 29 æ—¥](https://twitter.com/iximiuz/status/1443305889616863238?ref_src=twsrc%5Etfw)

**The problem is actually quite generic**. Theoretically, it can happen in any point-to-point TCP communication where the connections are reused if the upstream drops connections faster than the downstream. However, most of the time, the problem manifests itself with a reverse proxy talking to an upstream with HTTP Keep-Alive being on.

**è¿™ä¸ªé—®é¢˜å®é™…ä¸Šå¾ˆæ™®é**ã€‚ä»ç†è®ºä¸Šè®²ï¼Œå¦‚æœä¸Šæ¸¸æ–­å¼€è¿æ¥çš„é€Ÿåº¦æ¯”ä¸‹æ¸¸å¿«ï¼Œåˆ™å®ƒå¯èƒ½å‘ç”Ÿåœ¨ä»»ä½•ç‚¹å¯¹ç‚¹ TCP é€šä¿¡ä¸­ï¼Œå…¶ä¸­è¿æ¥è¢«é‡ç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ—¶å€™ï¼Œé—®é¢˜è¡¨ç°ä¸ºåå‘ä»£ç†ä¸ä¸Šæ¸¸é€šä¿¡å¹¶å¼€å¯ HTTP Keep-Aliveã€‚

How to mitigate:

å¦‚ä½•ç¼“è§£ï¼š

- Make sure the upstream (application) idle connection timeout is longer than the downstream (proxy) connection timeout
- Disable connection reuse between the upstream and the proxy (turn off HTTP Keep-Alive)
- Employ HTTP 5xx retries on the client-side (well, they are a must-have anyway).

- ç¡®ä¿ä¸Šæ¸¸ï¼ˆåº”ç”¨ç¨‹åºï¼‰ç©ºé—²è¿æ¥è¶…æ—¶æ—¶é—´é•¿äºä¸‹æ¸¸ï¼ˆä»£ç†ï¼‰è¿æ¥è¶…æ—¶æ—¶é—´
- ç¦ç”¨ä¸Šæ¸¸å’Œä»£ç†ä¹‹é—´çš„è¿æ¥é‡ç”¨ï¼ˆå…³é—­ HTTP Keep-Aliveï¼‰
- åœ¨å®¢æˆ·ç«¯ä½¿ç”¨ HTTP 5xx é‡è¯•ï¼ˆå¥½å§ï¼Œæ— è®ºå¦‚ä½•å®ƒä»¬éƒ½æ˜¯å¿…é¡»çš„ï¼‰ã€‚

---

---

Name   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)

åç§° [ä¸€å¹´å‰](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)

5XX are specifically called out as not retriable  i would not give this  advice because some impressionable soul out there might think it's a  good idea.

5XX ç‰¹åˆ«æŒ‡å‡ºæ˜¯ä¸å¯é‡è¯•çš„ï¼Œæˆ‘ä¸ä¼šç»™å‡ºè¿™ä¸ªå»ºè®®ï¼Œå› ä¸ºæœ‰äº›æ˜“å—å½±å“çš„äººå¯èƒ½è®¤ä¸ºè¿™æ˜¯ä¸ªå¥½ä¸»æ„ã€‚

-  [Ivan Velichko](https://disqus.com/by/iximiuz/) Mod [ Name](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5560305191)

   - [Ivan Velichko](https://disqus.com/by/iximiuz/) Mod [åç§°](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951) [ä¸€å¹´å‰](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5560305191)

  Hi! Could you please elaborate why do you think that 5xx aren't retriable? My reasoning is

   ä½ å¥½ï¼æ‚¨èƒ½å¦è¯¦ç»†è¯´æ˜ä¸ºä»€ä¹ˆæ‚¨è®¤ä¸º 5xx ä¸å¯é‡è¯•ï¼Ÿæˆ‘çš„æ¨ç†æ˜¯

  5xx - something failed on the server-side
   4xx - client did something wrong

   5xx - æœåŠ¡å™¨ç«¯å‘ç”Ÿæ•…éšœ
  4xx - å®¢æˆ·åšé”™äº†ä»€ä¹ˆ

  So, 4xx shouldn't be retried w/o modifying the request. But retrying 5xx  can (and often does) resolve the issue. Of course, it needs to be done  carefully if requests aren't idempotent. And 500 is a bit of a special  code here because it could indicate that a server simply failed to  prepare an otherwise-4xx response. However, 502, 503, and 504 are  generally retriable. Here is an [example of a (widely-used) retry strategy](https://disq.us/url?url=https%3A%2F%2Fdocs.aws.amazon.com%2Fapigateway%2Fapi-reference%2Fhandling-errors%2F%3A4PfjDpqD6KjInKtVqIWFEolbPmA&cuid=3735795) that follows the same reasoning.

  å› æ­¤ï¼Œä¸åº”åœ¨ä¸ä¿®æ”¹è¯·æ±‚çš„æƒ…å†µä¸‹é‡è¯• 4xxã€‚ä½†æ˜¯é‡è¯• 5xx å¯ä»¥ï¼ˆå¹¶ä¸”ç»å¸¸ï¼‰è§£å†³é—®é¢˜ã€‚å½“ç„¶ï¼Œå¦‚æœè¯·æ±‚ä¸æ˜¯å¹‚ç­‰çš„ï¼Œåˆ™éœ€è¦å°å¿ƒå®Œæˆã€‚ 500 åœ¨è¿™é‡Œæœ‰ç‚¹ç‰¹æ®Šä»£ç ï¼Œå› ä¸ºå®ƒå¯èƒ½è¡¨ç¤ºæœåŠ¡å™¨æ— æ³•å‡†å¤‡ otherwise-4xx å“åº”ã€‚ä½†æ˜¯ï¼Œ502ã€503 å’Œ 504 é€šå¸¸æ˜¯å¯é‡è¯•çš„ã€‚è¿™æ˜¯ä¸€ä¸ª [ï¼ˆå¹¿æ³›ä½¿ç”¨çš„ï¼‰é‡è¯•ç­–ç•¥çš„ç¤ºä¾‹](https://disq.us/url?url=https%3A%2F%2Fdocs.aws.amazon.com%2Fapigateway%2Fapi-reference%2Fhandling-errors%2F%3A4PfjDpqD6KjInKtVqIWFEolbPmA&cuid=3735795) éµå¾ªç›¸åŒçš„æ¨ç†ã€‚

 [Air Fresh](https://disqus.com/by/air_fresh/)    [6 months ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5940826210)  edited

[ç©ºæ°”æ¸…æ–°](https://disqus.com/by/air_fresh/) [6 ä¸ªæœˆå‰](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5940826210) å·²ç¼–è¾‘

Hello Ivan Velichko. It seems that you should switch the place of "Idle timeout of upstream  connection" and "Idle timeout of downstream connection" on image

ä½ å¥½ï¼Œä¼Šä¸‡ç»´åˆ©å¥‡ç§‘ã€‚çœ‹æ¥ä½ åº”è¯¥åœ¨å›¾åƒä¸Šåˆ‡æ¢â€œä¸Šæ¸¸è¿æ¥ç©ºé—²è¶…æ—¶â€å’Œâ€œä¸‹æ¸¸è¿æ¥ç©ºé—²è¶…æ—¶â€çš„ä½ç½®

- Hmm... I gave it fresh look, and the diagram still looks right to me. Care to elaborate?

   - å—¯...æˆ‘ç»™äº†å®ƒæ–°çš„å¤–è§‚ï¼Œå›¾è¡¨å¯¹æˆ‘æ¥è¯´ä»ç„¶æ­£ç¡®ã€‚æƒ³è¯¦ç»†è¯´æ˜å—ï¼Ÿ

  The upper diagram of "Idle timeout of upstream connection" is downstream,  so it's easy for the readers to think "Idle timeout of upstream  connection" is referred to Reversed Proxy.

   ä¸Šå›¾â€œupstream connection Idle timeoutâ€æ˜¯ä¸‹æ¸¸çš„ï¼Œæ‰€ä»¥è¯»è€…å¾ˆå®¹æ˜“è®¤ä¸ºâ€œupstream connection Idle timeoutâ€æŒ‡çš„æ˜¯Reversed Proxyã€‚

  So, for easier to interpretation, I suggest changing with below sentence on image:

   æ‰€ä»¥ï¼Œä¸ºäº†æ›´å®¹æ˜“è§£é‡Šï¼Œæˆ‘å»ºè®®åœ¨å›¾åƒä¸Šç”¨ä¸‹é¢çš„å¥å­æ¥æ”¹å˜ï¼š

  "Idle timeout of downstream connections" < "Idle timeout of upstream connections" 

"ä¸‹æ¸¸è¿æ¥ç©ºé—²è¶…æ—¶" < "ä¸Šæ¸¸è¿æ¥ç©ºé—²è¶…æ—¶"

Oh, I see the point now, but I'm not sure if it's a right thing to do... My reasoning is/was like that: on the "downstream" side (a reverse proxy  on the diagram), there is only one possible knob - the *downstream-to-upstream* (or just *upstream* for short) connection timeout. Hence, the name "upstream connection  timeout." And, symmetrically, on the upstream side there are only  (upstream-to-) downstream connections.

   å“¦ï¼Œæˆ‘ç°åœ¨æ˜ç™½äº†ï¼Œä½†æˆ‘ä¸ç¡®å®šè¿™æ ·åšæ˜¯å¦æ­£ç¡®......æˆ‘çš„æ¨ç†æ˜¯/æ˜¯è¿™æ ·çš„ï¼šåœ¨â€œä¸‹æ¸¸â€æ–¹é¢ï¼ˆå›¾ä¸­çš„åå‘ä»£ç†ï¼‰ï¼Œåªæœ‰ä¸€ä¸ªå¯èƒ½çš„æ—‹é’® - *ä¸‹æ¸¸åˆ°ä¸Šæ¸¸*ï¼ˆæˆ–ç®€ç§°*ä¸Šæ¸¸*ï¼‰è¿æ¥è¶…æ—¶ã€‚å› æ­¤ï¼Œåç§°ä¸ºâ€œä¸Šæ¸¸è¿æ¥è¶…æ—¶â€ã€‚å¹¶ä¸”ï¼Œå¯¹ç§°åœ°ï¼Œåœ¨ä¸Šæ¸¸ä¾§åªæœ‰ï¼ˆä¸Šæ¸¸åˆ°ï¼‰ä¸‹æ¸¸è¿æ¥ã€‚

  OK, now I can understand your intention, and I still have 2 questions:

   OKï¼Œç°åœ¨æˆ‘æ˜ç™½ä½ çš„æ„æ€äº†ï¼Œæˆ‘è¿˜æœ‰2ä¸ªé—®é¢˜ï¼š

  \1. How can I set idle connection timeout from downstream to upstream and vice versa?
   \2. What are the differences between Nginx's keepalive_timeout and idle connection timeout

  

   \1.å¦‚ä½•è®¾ç½®ä»ä¸‹æ¸¸åˆ°ä¸Šæ¸¸çš„ç©ºé—²è¿æ¥è¶…æ—¶ï¼Œåä¹‹äº¦ç„¶ï¼Ÿ
  \2ã€‚ Nginxçš„keepalive_timeoutå’Œç©ºé—²è¿æ¥è¶…æ—¶æœ‰ä»€ä¹ˆåŒºåˆ«

  

  - I think it's technology-specific. For instance, in nginx there seems to be two similarly named but distinct knobs:

     - æˆ‘è®¤ä¸ºè¿™æ˜¯ç‰¹å®šäºæŠ€æœ¯çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ nginx ä¸­ä¼¼ä¹æœ‰ä¸¤ä¸ªåç§°ç›¸ä¼¼ä½†ä¸åŒçš„æ—‹é’®ï¼š

    \- [`keepalive_timeout` set in the `http`, `server`, or `location` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_timeout%3AOqDTC-r2IaE1MYb8_g-BkgT0cfM&cuid=3735795) means the timeout for the downstream connections.

     \- [åœ¨ `http`ã€`server` æˆ– `location` ä¸Šä¸‹æ–‡ä¸­è®¾ç½®çš„ keepalive_timeout](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_timeout%3AOqDTC-r2IaE1MYb8_g-BkgT0cfM&cuid=3735795)è¡¨ç¤ºä¸‹æ¸¸è¿æ¥è¶…æ—¶ã€‚

    \- [` keepalive_timeout` set in the `upstream` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%3AEUWuizQE68MydDc5S6YVsO_vs9k&cuid=3735795) means the timeout during which an idle keepalive connection to an  upstream server will stay open (notice how there are two more knobs - ` keepalive_time` and ` keepalive_requests`).

     \- [åœ¨â€œä¸Šæ¸¸â€ä¸Šä¸‹æ–‡ä¸­è®¾ç½®çš„â€œkeepalive_timeoutâ€](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%3AEUWuizQE68MydDc5S6YVsO_vs9k&cuid=3735795) è¡¨ç¤ºè¶…æ—¶ï¼Œåœ¨æ­¤æœŸé—´ä¸ä¸Šæ¸¸æœåŠ¡å™¨çš„ç©ºé—²ä¿æŒè¿æ¥å°†ä¿æŒæ‰“å¼€çŠ¶æ€ï¼ˆæ³¨æ„è¿˜æœ‰ä¸¤ä¸ªæ—‹é’® - `keepalive_time` å’Œ `keepalive_requests`)ã€‚

    I remember how back in 2018 we couldn't find a similar upstream know for AWS ALB. But maybe things have changed since then.

     æˆ‘è®°å¾—åœ¨ 2018 å¹´æˆ‘ä»¬å¦‚ä½•æ‰¾ä¸åˆ°ç±»ä¼¼çš„ AWS ALB ä¸Šæ¸¸çŸ¥è¯†ã€‚ä½†ä¹Ÿè®¸ä»é‚£ä»¥åäº‹æƒ…å‘ç”Ÿäº†å˜åŒ–ã€‚

    Since nginx, as a reverse proxy, acts as a client and as a server at the same time, it has two different sets of configs. But on the actual client-  or server- ends, there is usually just one timeout value. Often it's  called *idle timeout*, like in this Go `net/http` case ([Server.IdleTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Server%3A0ZDn9syuWGxHZXkhSruki6UtEqc&cuid=3735795) for the server-side & [Transport.IdleConnTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Transport%3AAazGyQh9twbSYj_YPhznwvqL9qM&cuid=3735795) for the client-side).

    

ç”±äº nginx ä½œä¸ºåå‘ä»£ç†ï¼ŒåŒæ—¶ä½œä¸ºå®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ï¼Œå®ƒæœ‰ä¸¤å¥—ä¸åŒçš„é…ç½®ã€‚ä½†åœ¨å®é™…çš„å®¢æˆ·ç«¯æˆ–æœåŠ¡å™¨ç«¯ï¼Œé€šå¸¸åªæœ‰ä¸€ä¸ªè¶…æ—¶å€¼ã€‚é€šå¸¸å®ƒè¢«ç§°ä¸º*ç©ºé—²è¶…æ—¶*ï¼Œå°±åƒåœ¨è¿™ä¸ª Go `net/http` æ¡ˆä¾‹ä¸­ä¸€æ · ([Server.IdleTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Server%3A0ZDn9syuWGxHZXkhSruki6UtEqc&cuid=3735795) å¯¹äºæœåŠ¡å™¨ç«¯ & [Transport.IdleConnTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Transport%3AAazGyQh9twbSYj_YPhznwvqL9qM&cuid=3735795ï¼‰å¯¹äºå®¢æˆ·ç«¯)ã€‚

    

Thanks for sharing!
for the 2nd mitigation "Disable connection reuse between the upstream and  the proxy (turn off HTTP Keep-Alive)", I think it is not a good idea, as it is gonna cause load on the upstream server as for each new  connection, a new tcp connection is created which is expensive. so it is important for the proxy server to do connection reuse as much as it  can.

æ„Ÿè°¢åˆ†äº«ï¼
å¯¹äºç¬¬äºŒä¸ªç¼“è§£æªæ–½â€œç¦ç”¨ä¸Šæ¸¸å’Œä»£ç†ä¹‹é—´çš„è¿æ¥é‡ç”¨ï¼ˆå…³é—­ HTTP Keep-Aliveï¼‰â€ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºå¯¹äºæ¯ä¸ªæ–°è¿æ¥ï¼Œå®ƒéƒ½ä¼šå¯¹ä¸Šæ¸¸æœåŠ¡å™¨é€ æˆè´Ÿè½½ï¼Œaåˆ›å»ºæ–°çš„ tcp è¿æ¥æ˜¯æ˜‚è´µçš„ã€‚å› æ­¤ï¼Œä»£ç†æœåŠ¡å™¨å°½å¯èƒ½å¤šåœ°è¿›è¡Œè¿æ¥é‡ç”¨å¾ˆé‡è¦ã€‚

That's a fair concern and a very good point! However, I think option 2 remains valid. Not every service has a high load. And for applications serving  just a few tens requests a minute, recreating connections will unlikely  cause any noticeable overhead. The irony is that it's often such small  to medium services that miss other resilience mechanisms such as retries on the client. So, disabling the connection reuse might be the easiest  and the cheapest way to fix 502s for them. Of course, for apps serving  thousands of requests a second it's a totally different story. But  usually those apps would have a dedicated SRE team behind and all the  needed retries in place :)

è¿™æ˜¯ä¸€ä¸ªåˆç†çš„å…³æ³¨ç‚¹ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§‚ç‚¹ï¼ä½†æ˜¯ï¼Œæˆ‘è®¤ä¸ºé€‰é¡¹ 2 ä»ç„¶æœ‰æ•ˆã€‚å¹¶éæ¯ä¸ªæœåŠ¡éƒ½æœ‰é«˜è´Ÿè½½ã€‚å¯¹äºæ¯åˆ†é’Ÿåªå¤„ç†å‡ åä¸ªè¯·æ±‚çš„åº”ç”¨ç¨‹åºï¼Œé‡æ–°åˆ›å»ºè¿æ¥ä¸å¤ªå¯èƒ½å¯¼è‡´ä»»ä½•æ˜æ˜¾çš„å¼€é”€ã€‚å…·æœ‰è®½åˆºæ„å‘³çš„æ˜¯ï¼Œé€šå¸¸æ˜¯è¿™æ ·çš„ä¸­å°å‹æœåŠ¡é”™è¿‡äº†å…¶ä»–å¼¹æ€§æœºåˆ¶ï¼Œä¾‹å¦‚å®¢æˆ·ç«¯é‡è¯•ã€‚å› æ­¤ï¼Œç¦ç”¨è¿æ¥é‡ç”¨å¯èƒ½æ˜¯ä¸ºä»–ä»¬ä¿®å¤ 502 çš„æœ€ç®€å•å’Œæœ€ä¾¿å®œçš„æ–¹æ³•ã€‚å½“ç„¶ï¼Œå¯¹äºæ¯ç§’å¤„ç†æ•°åƒä¸ªè¯·æ±‚çš„åº”ç”¨ç¨‹åºæ¥è¯´ï¼Œæƒ…å†µå°±å®Œå…¨ä¸åŒäº†ã€‚ä½†é€šå¸¸è¿™äº›åº”ç”¨ç¨‹åºä¼šæœ‰ä¸“é—¨çš„ SRE å›¢é˜Ÿæ”¯æŒï¼Œå¹¶ä¸”æ‰€æœ‰éœ€è¦çš„é‡è¯•éƒ½å·²åˆ°ä½ :)

If we had sent ack of FIN before before the data we wouldn't face this  issue? (TCP Half close) Upstream closed the connection but proxy still  is using it.

å¦‚æœæˆ‘ä»¬åœ¨æ•°æ®ä¹‹å‰å‘é€äº† FIN ackï¼Œæˆ‘ä»¬å°±ä¸ä¼šé‡åˆ°è¿™ä¸ªé—®é¢˜äº†å—ï¼Ÿ ï¼ˆTCP åŠå…³é—­ï¼‰ä¸Šæ¸¸å…³é—­äº†è¿æ¥ï¼Œä½†ä»£ç†ä»åœ¨ä½¿ç”¨å®ƒã€‚

[ ![Thumbnail](https://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg?w=800&h=611)](http://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg)

Hmm... I'm not sure how reasonable is a situation when the upstream already  closed its end of the connection, but the proxy keeps the other end  open. What kind of information should the proxy be writing to its end of the connection then? 

å—¯...æˆ‘ä¸ç¡®å®šä¸Šæ¸¸å·²ç»å…³é—­å…¶è¿æ¥ç«¯ä½†ä»£ç†ä¿æŒå¦ä¸€ç«¯æ‰“å¼€çš„æƒ…å†µæœ‰å¤šåˆç†ã€‚é‚£ä¹ˆä»£ç†åº”è¯¥å‘å®ƒçš„è¿æ¥ç«¯å†™å…¥ä»€ä¹ˆæ ·çš„ä¿¡æ¯å‘¢ï¼Ÿ

The inverse situation looks legit, though. A proxy could push the request to the connection and then half-close it  before waiting for the upstream's reply. I'm not aware of any proxies  really doing it, but hypothetically such behaviour can be implemented by proxies allowing to configure max number of requests per connection ([example](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_requests%3AzFGxIfAi4lkU1QQWuwDNj5IBmRQ&cuid=3735795)).

ä¸è¿‡ï¼Œç›¸åçš„æƒ…å†µçœ‹èµ·æ¥æ˜¯åˆç†çš„ã€‚ä»£ç†å¯ä»¥å°†è¯·æ±‚æ¨é€åˆ°è¿æ¥ï¼Œç„¶ååœ¨ç­‰å¾…ä¸Šæ¸¸çš„å›å¤ä¹‹å‰åŠå…³é—­å®ƒã€‚æˆ‘ä¸çŸ¥é“æœ‰ä»»ä½•ä»£ç†ç¡®å®åœ¨è¿™æ ·åšï¼Œä½†å‡è®¾è¿™ç§è¡Œä¸ºå¯ä»¥é€šè¿‡å…è®¸é…ç½®æ¯ä¸ªè¿æ¥çš„æœ€å¤§è¯·æ±‚æ•°çš„ä»£ç†æ¥å®ç°ï¼ˆ[ç¤ºä¾‹](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_requests%3AzFGxIfAi4lkU1QQWuwDNj5IBmRQ&cuid=3735795))ã€‚

Hello Ivan, first of all thank you for writing this post!

ä½ å¥½ Ivanï¼Œé¦–å…ˆæ„Ÿè°¢ä½ å†™è¿™ç¯‡æ–‡ç« ï¼

I'm trying to solve this very problem at the moment but so far I've been  unable to find the right set of configuration both for Nginx and for  uWSGI. In my case I have AWS ALB -> NGINX -> uWSGI.

æˆ‘ç°åœ¨æ­£åœ¨å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä¸€ç›´æ— æ³•ä¸º Nginx å’Œ uWSGI æ‰¾åˆ°æ­£ç¡®çš„é…ç½®é›†ã€‚åœ¨æˆ‘çš„ä¾‹å­ä¸­ï¼Œæˆ‘æœ‰ AWS ALB -> NGINX -> uWSGIã€‚

In Nginx I'm basically using the reverse proxy configuration with uwsgi socket:
`server {    listen 80; location / {        include uwsgi_params; uwsgi_pass unix:///tmp/uwsgi.sock; }}`

åœ¨ Nginx ä¸­ï¼Œæˆ‘åŸºæœ¬ä¸Šä½¿ç”¨å¸¦æœ‰ uwsgi å¥—æ¥å­—çš„åå‘ä»£ç†é…ç½®ï¼š
`æœåŠ¡å™¨{å¬80;ä½ç½®/{åŒ…æ‹¬uwsgi_paramsï¼› uwsgi_pass unix:///tmp/uwsgi.sockï¼› }}`

I also noticed that I have **keepalive_timeout  65** set in the **/etc/nginx/nginx.conf**, but I assume it's ok since it's the AWS ALB who is communicating with Nginx.

æˆ‘è¿˜æ³¨æ„åˆ°æˆ‘åœ¨ **/etc/nginx/nginx.conf** ä¸­è®¾ç½®äº† **keepalive_timeout 65**ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ²¡é—®é¢˜ï¼Œå› ä¸ºä¸ Nginx é€šä¿¡çš„æ˜¯ AWS ALBã€‚

I think the problem I'm having (intermittent 502 responses) is because of my uWSGI configuration:
`socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664hook-master-start = unix_signal:15 gracefully_kill_them_allneed-app = truedie-on-term = trueshow-config = truemaster = truedie-on-term = truememory -report = falsegevent = 20gevent-monkey-patch = trueso-keepalive = falseharakiri = 80http-timeout = 70socket-timeout = 70# new relic mandatory options# https://docs.newrelic.com/docs/python/python-agent- and-uwsgienable-threads = truesingle-interpreter = true`

æˆ‘è®¤ä¸ºæˆ‘é‡åˆ°çš„é—®é¢˜ï¼ˆé—´æ­‡æ€§ 502 å“åº”ï¼‰æ˜¯å› ä¸ºæˆ‘çš„ uWSGI é…ç½®ï¼š
`socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664hook-master-start = unix_signal:15 gracefully_kill_them_allneed-app = truedie-on-term = trueshow-config = truemaster = truedie-on-term = truememory -report = falsegevent = 20gevent-monkey-patch = trueso-keepalive = falseharakiri = 80http-timeout = 70socket-timeout = 70# new relic mandatory options# https://docs.newrelic.com/docs/python/python-agent- and-uwsgienable-threads = truesingle-interpreter = true`

Would you mind to share some insights? I'm having a hard time trying to translate your conclusion with the right [uWSGI configuration](https://disq.us/url?url=https%3A%2F%2Fuwsgi-docs.readthedocs.io%2Fen%2Flatest%2FOptions.html%3ALPS9dziFKJTkzZr3PDDD-pszxis&cuid=3735795).

ä½ ä»‹æ„åˆ†äº«ä¸€äº›è§è§£å—ï¼Ÿæˆ‘å¾ˆéš¾å°è¯•ç”¨æ­£ç¡®çš„ [uWSGI é…ç½®](https://disq.us/url?url=https%3A%2F%2Fuwsgi-docs.readthedocs.io%2Fen%2Flatest%2FOptions.html%3ALPS9dziFKJTkzZr3PDDD-pszxis&cuid=3735795)ã€‚

Hi! It's likely that the problem is in between the ALB and nginx since the  uwsgi protocol doesn't have a notion of keepalive connections ([see this for more](https://disq.us/url?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%23keepalive%3AAVRyL044jRXhqj0kV67hZ9CCto8&cuid=3735795)). That 65 seconds setting of nginx is the time when it'll start closing  idle downstream connections (i.e., ALB's connections). You need to make  sure ALB has a shorter keepalive timeout for its upstream connections. Be careful and don't confuse the timeouts since often proxy and load  balancers (ALB, nginx, etc) have both the upstream and downstream  keepalive settings ([see this answer for more](http://disq.us/p/2qdh9vv)). ALB is a downstream for nginx, nginx is an upstream for ALB,  and end users are a downstream for ALB.

ä½ å¥½ï¼é—®é¢˜å¾ˆå¯èƒ½å‡ºåœ¨ ALB å’Œ nginx ä¹‹é—´ï¼Œå› ä¸º uwsgi åè®®æ²¡æœ‰ä¿æŒæ´»åŠ¨è¿æ¥çš„æ¦‚å¿µï¼ˆ[æ›´å¤šä¿¡æ¯è¯·å‚è§æ­¤å¤„](https://disq.us/url?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%23keepalive%3AAVRyL044jRXhqj0kV67hZ9CCto8&cuid=3735795))ã€‚ nginx çš„ 65 ç§’è®¾ç½®æ˜¯å®ƒå°†å¼€å§‹å…³é—­ç©ºé—²ä¸‹æ¸¸è¿æ¥ï¼ˆå³ ALB çš„è¿æ¥ï¼‰çš„æ—¶é—´ã€‚æ‚¨éœ€è¦ç¡®ä¿ ALB çš„ä¸Šæ¸¸è¿æ¥å…·æœ‰æ›´çŸ­çš„ä¿æŒæ´»åŠ¨è¶…æ—¶ã€‚å°å¿ƒä¸è¦æ··æ·†è¶…æ—¶ï¼Œå› ä¸ºä»£ç†å’Œè´Ÿè½½å¹³è¡¡å™¨ï¼ˆALBã€nginx ç­‰)é€šå¸¸åŒæ—¶å…·æœ‰ä¸Šæ¸¸å’Œä¸‹æ¸¸ keepalive è®¾ç½®ï¼ˆ[æ›´å¤šä¿¡æ¯è¯·å‚è§æ­¤ç­”æ¡ˆ](http://disq.us/p/2qdh9vv)). ALBæ˜¯nginxçš„ä¸‹æ¸¸ï¼Œnginxæ˜¯ALBçš„ä¸Šæ¸¸ï¼Œç»ˆç«¯ç”¨æˆ·æ˜¯ALBçš„ä¸‹æ¸¸ã€‚

- Thank you Ivan.

   - è°¢è°¢ä¼Šä¸‡ã€‚

  Just wanted to give a feedback in case someone faces this problem: as you  said, in our case the solution was to make sure the ALB's idle timeout  was shorter than the Nginx client timeout (client_header_timeout and,  client_body_timeout) and that the Nginx client timeout was shorter than  uwsgi timeout (upstream's configuration uwsgi_read_timeout and  uwsgi_send_timeout).

   åªæ˜¯æƒ³åœ¨æœ‰äººé‡åˆ°è¿™ä¸ªé—®é¢˜æ—¶æä¾›åé¦ˆï¼šæ­£å¦‚ä½ æ‰€è¯´ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯ç¡®ä¿ ALB çš„ç©ºé—²è¶…æ—¶æ—¶é—´çŸ­äº Nginx å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´ï¼ˆclient_header_timeout å’Œ client_body_timeoutï¼‰å¹¶ä¸” Nginx å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´æ˜¯æ¯” uwsgi è¶…æ—¶æ—¶é—´çŸ­ï¼ˆä¸Šæ¸¸çš„é…ç½® uwsgi_read_timeout å’Œ uwsgi_send_timeoutï¼‰ã€‚

  Example:
   **ALB's idle timeout:** 60s
   **Nginx.conf global configuration:** client_header_timeout 80s; client_body_timeout  80s;
   **uWSGI upstream configuration:** uwsgi_read_timeout 100s; uwsgi_send_timeout 100s; 

ä¾‹å­ï¼š
  **ALB çš„ç©ºé—²è¶…æ—¶ï¼š** 60s
  **Nginx.confå…¨å±€é…ç½®ï¼š**client_header_timeout 80sï¼› client_body_timeout 80 ç§’ï¼›
  **uWSGI ä¸Šæ¸¸é…ç½®ï¼š** uwsgi_read_timeout 100sï¼› uwsgi_send_timeout 100sï¼›

