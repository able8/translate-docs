# Reverse Proxy, HTTP Keep-Alive Timeout, and sporadic HTTP 502s

# 反向代理、HTTP Keep-Alive 超时和零星的 HTTP 502

https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s

https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s

## How HTTP Keep-Alive can cause TCP race condition

## HTTP Keep-Alive 如何导致 TCP 竞争条件

September 30, 2021

2021 年 9 月 30 日

These mysterious HTTP 502s happened to me already twice over the past few years. Since the amount of service-to-service communications every year goes only up, I expect more and more people to experience the same issue. So, sharing it here.

这些神秘的 HTTP 502 在过去几年中已经发生过两次。由于每年服务间的通信量只增不减，我预计会有越来越多的人遇到同样的问题。所以，分享到这里。

**TL;DR:** HTTP Keep-Alive between a reverse proxy and an upstream server combined with some misfortunate downstream- and upstream-side timeout settings can make clients receiving HTTP 502s from the proxy.

**TL;DR:** 反向代理和上游服务器之间的 HTTP Keep-Alive 结合一些不幸的下游和上游端超时设置可以使客户端从代理接收 HTTP 502。

[![How HTTP Keep-Alive between a reverse proxy and an upstream can cause HTTP 502s due to a TCP race condition.](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond-2000-opt.png)](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond.png)

tcp-conn-race-cond-2000-opt.png)](http://iximiuz.com/reverse-proxy-http-keep-alive-and-502s/tcp-conn-race-cond.png)

Below is a longer version...

下面是一个更长的版本...

[HTTP 502](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502) status code, also know as `Bad Gateway` indicates that a server, while acting as a gateway or proxy, received an invalid response from the upstream server. Typical problems causing it ( [summarized by Datadog folks](https://www.datadoghq.com/blog/nginx-502-bad-gateway-errors-gunicorn/)):

[HTTP 502](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502) 状态代码，也称为“Bad Gateway”，表示服务器在充当网关或代理，收到来自上游服务器的无效响应。导致它的典型问题（[由 Datadog 人员总结](https://www.datadoghq.com/blog/nginx-502-bad-gateway-errors-gunicorn/))：

- Upstream server isn't running
- Upstream server times out
- Reverse proxy is misconfigured (e.g., my favourite one - trying to call uWSGI over`HTTP` while it listens on `uwsgi` protocol 🙈)

- 上游服务器未运行
- 上游服务器超时
- 反向代理配置错误（例如，我最喜欢的一个 - 在它监听 `uwsgi` 协议时试图通过 `HTTP` 调用 uWSGI 🙈）

However, sometimes there seems to be no apparent reason for HTTP 502s responses while clients sporadically see them:

然而，有时似乎没有明显的 HTTP 502 响应原因，而客户端偶尔会看到它们：

- [NodeJs application behind Amazon ELB throws 502](https://stackoverflow.com/questions/45626787/nodejs-application-behind-amazon-elb-throws-502)
- [Proxy persistent upstream: occasional 502](https://github.com/h2o/h2o/issues/281)
- [Sporadic 502 response only when running through traefik](https://github.com/traefik/traefik/issues/3237)
- [HTTP 502 response generated by a proxy after it tries to send data upstream to a partially closed connection](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a)

- [Amazon ELB 背后的 NodeJs 应用程序抛出 502](https://stackoverflow.com/questions/45626787/nodejs-application-behind-amazon-elb-throws-502)
- [代理持久上游：偶尔出现 502](https://github.com/h2o/h2o/issues/281)
- [仅在通过 traefik 运行时出现零星 502 响应](https://github.com/traefik/traefik/issues/3237)
- [代理尝试将数据向上游发送到部分关闭的连接后生成的 HTTP 502 响应](https://serverfault.com/questions/845171/http-502-response-generated-by-a-proxy-after-it-tries-to-send-data-upstream-to-a)

For me personally, it happened twice already - the first time, with a Node.js application running behind AWS ALB; and the second time, with a Python (uWSGI) application running behind Traefik reverse proxy. And it took quite some time for my team to debug it back in the day (we even involved premium AWS support to pin down the root cause).

就我个人而言，它已经发生了两次 - 第一次是在 AWS ALB 后面运行一个 Node.js 应用程序；第二次，使用在 Traefik 反向代理后面运行的 Python (uWSGI) 应用程序。当天我的团队花了相当长的时间来调试它（我们甚至让高级 AWS 支持来确定根本原因）。

Long story short, it was a **TCP race condition**.

长话短说，这是一个 **TCP 竞争条件**。

A detailed explanation of the problem, including some low-level TCP packet analysis, can be found in [this lovely article](https://www.tessian.com/blog/how-to-fix-http-502-errors/). And here, I'll just try to give a quick summary.

可以在[这篇可爱的文章](https://www.tessian.com/blog/how-to-fix-http-502-errors/).在这里，我将尝试给出一个快速总结。

First off, any TCP connection is a distributed system. Well, it should make sense - the actual state of a connection is distributed between its [remote] endpoints. And as the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) dictates, any distributed system can be either available 100% of the time or consistent 100% of the time, but not both. TCP obviously chooses the availability. Hence, the **state of a TCP connection is only eventually consistent**!

首先，任何 TCP 连接都是分布式系统。好吧，它应该是有道理的——连接的实际状态分布在它的 [远程] 端点之间。正如 [CAP 定理](https://en.wikipedia.org/wiki/CAP_theorem) 所指出的，任何分布式系统都可以 100% 可用或 100% 一致，但不能两者兼而有之。 TCP显然选择可用性。因此，**TCP 连接的状态只是最终一致的**！

Why does it matter for us?

为什么这对我们很重要？

While this [serverfault](https://serverfault.com/a/426741/292251) answer says that the HTTP Keep-Alive should be used only for the client-to-proxy communications, from my experience, proxies often keep the upstream connections alive too. This is a so-called _connection pool pattern_ when just a few connections are heavily reused to handle numerous requests. 

虽然这个 [serverfault](https://serverfault.com/a/426741/292251) 回答说 HTTP Keep-Alive 应该只用于客户端到代理的通信，根据我的经验，代理通常保持上游连接也还活着。这就是所谓的_连接池模式_，只有少数连接被大量重用来处理大量请求。

When a connection stays in the pool for too long without being reused, it's marked as _idle_. Normally, idle connections are closed after some period of inactivity. However, the server-side (i.e., the upstream) also can drop idle connections. So, what sometimes happens is that the proxy and the upstream have some misfortunate idle connection timeout settings. More specifically, if the proxy and the upstream have exactly the same idle timeout durations or the upstream drops connection sooner than the proxy.

当一个连接在池中停留太久而没有被重用时，它被标记为_idle_。通常，空闲连接会在一段时间不活动后关闭。但是，服务器端（即上游）也可以丢弃空闲连接。因此，有时会发生代理和上游有一些不幸的空闲连接超时设置。更具体地说，如果代理和上游具有完全相同的空闲超时持续时间，或者上游比代理更早断开连接。

While one endpoint (the proxy) can still be thinking that the connection is totally fine, the other endpoint may have already started closing the connection (the upstream).

虽然一个端点（代理）仍然认为连接完全正常，但另一个端点可能已经开始关闭连接（上游）。

So, the proxy starts writing a request into a connection that is being closed by the upstream. And instead of getting the TCP `ACK` for the sent bytes, the proxy gets TCP `FIN` (or even `RST`) from the upstream. Oftentimes, the proxy just gives up on such requests and responds with immediate HTTP 502 to the client. **And this is completely invisible on the application side!** So, no application logs will be helpful in such a case.

因此，代理开始将请求写入被上游关闭的连接。代理不是获取已发送字节的 TCP `ACK`，而是从上游获取 TCP `FIN`（甚至是 `RST`）。通常，代理会放弃此类请求并立即向客户端响应 HTTP 502。 **这在应用程序端是完全不可见的！**因此，在这种情况下，任何应用程序日志都无济于事。

> HTTP Keep-Alive can cause TCP race conditions 🤯
>
> Ok, it happened again... Hence, the diagram!
>
> In a (reverse\_proxy + app\_server) setup, the reverse proxy should be dropping connections more frequently than the app server. Otherwise, clients will see sporadic HTTP 502s.
>
> Why? 🔽 [pic.twitter.com/YBb2tP3Zfg](https://t.co/YBb2tP3Zfg)
>
> — Ivan Velichko (@iximiuz) [September 29, 2021](https://twitter.com/iximiuz/status/1443305889616863238?ref_src=twsrc%5Etfw)

> HTTP Keep-Alive 会导致 TCP 竞争条件🤯
>
> 好吧，它又发生了……因此，图表！
>
> 在 (reverse\_proxy + app\_server) 设置中，反向代理应该比应用服务器更频繁地断开连接。否则，客户端将看到零星的 HTTP 502。
>
> 为什么？ 🔽 [pic.twitter.com/YBb2tP3Zfg](https://t.co/YBb2tP3Zfg)
>
> — Ivan Velichko (@iximiuz) [2021 年 9 月 29 日](https://twitter.com/iximiuz/status/1443305889616863238?ref_src=twsrc%5Etfw)

**The problem is actually quite generic**. Theoretically, it can happen in any point-to-point TCP communication where the connections are reused if the upstream drops connections faster than the downstream. However, most of the time, the problem manifests itself with a reverse proxy talking to an upstream with HTTP Keep-Alive being on.

**这个问题实际上很普遍**。从理论上讲，如果上游断开连接的速度比下游快，则它可能发生在任何点对点 TCP 通信中，其中连接被重用。然而，大多数时候，问题表现为反向代理与上游通信并开启 HTTP Keep-Alive。

How to mitigate:

如何缓解：

- Make sure the upstream (application) idle connection timeout is longer than the downstream (proxy) connection timeout
- Disable connection reuse between the upstream and the proxy (turn off HTTP Keep-Alive)
- Employ HTTP 5xx retries on the client-side (well, they are a must-have anyway).

- 确保上游（应用程序）空闲连接超时时间长于下游（代理）连接超时时间
- 禁用上游和代理之间的连接重用（关闭 HTTP Keep-Alive）
- 在客户端使用 HTTP 5xx 重试（好吧，无论如何它们都是必须的）。

---

---

Name   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)

名称 [一年前](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)

5XX are specifically called out as not retriable  i would not give this  advice because some impressionable soul out there might think it's a  good idea.

5XX 特别指出是不可重试的，我不会给出这个建议，因为有些易受影响的人可能认为这是个好主意。

-  [Ivan Velichko](https://disqus.com/by/iximiuz/) Mod [ Name](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951)   [a year ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5560305191)

   - [Ivan Velichko](https://disqus.com/by/iximiuz/) Mod [名称](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5559657951) [一年前](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5560305191)

  Hi! Could you please elaborate why do you think that 5xx aren't retriable? My reasoning is

   你好！您能否详细说明为什么您认为 5xx 不可重试？我的推理是

  5xx - something failed on the server-side
   4xx - client did something wrong

   5xx - 服务器端发生故障
  4xx - 客户做错了什么

  So, 4xx shouldn't be retried w/o modifying the request. But retrying 5xx  can (and often does) resolve the issue. Of course, it needs to be done  carefully if requests aren't idempotent. And 500 is a bit of a special  code here because it could indicate that a server simply failed to  prepare an otherwise-4xx response. However, 502, 503, and 504 are  generally retriable. Here is an [example of a (widely-used) retry strategy](https://disq.us/url?url=https%3A%2F%2Fdocs.aws.amazon.com%2Fapigateway%2Fapi-reference%2Fhandling-errors%2F%3A4PfjDpqD6KjInKtVqIWFEolbPmA&cuid=3735795) that follows the same reasoning.

  因此，不应在不修改请求的情况下重试 4xx。但是重试 5xx 可以（并且经常）解决问题。当然，如果请求不是幂等的，则需要小心完成。 500 在这里有点特殊代码，因为它可能表示服务器无法准备 otherwise-4xx 响应。但是，502、503 和 504 通常是可重试的。这是一个 [（广泛使用的）重试策略的示例](https://disq.us/url?url=https%3A%2F%2Fdocs.aws.amazon.com%2Fapigateway%2Fapi-reference%2Fhandling-errors%2F%3A4PfjDpqD6KjInKtVqIWFEolbPmA&cuid=3735795) 遵循相同的推理。

 [Air Fresh](https://disqus.com/by/air_fresh/)    [6 months ago](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5940826210)  edited

[空气清新](https://disqus.com/by/air_fresh/) [6 个月前](https://iximiuz.com/en/posts/reverse-proxy-http-keep-alive-and-502s/#comment-5940826210) 已编辑

Hello Ivan Velichko. It seems that you should switch the place of "Idle timeout of upstream  connection" and "Idle timeout of downstream connection" on image

你好，伊万维利奇科。看来你应该在图像上切换“上游连接空闲超时”和“下游连接空闲超时”的位置

- Hmm... I gave it fresh look, and the diagram still looks right to me. Care to elaborate?

   - 嗯...我给了它新的外观，图表对我来说仍然正确。想详细说明吗？

  The upper diagram of "Idle timeout of upstream connection" is downstream,  so it's easy for the readers to think "Idle timeout of upstream  connection" is referred to Reversed Proxy.

   上图“upstream connection Idle timeout”是下游的，所以读者很容易认为“upstream connection Idle timeout”指的是Reversed Proxy。

  So, for easier to interpretation, I suggest changing with below sentence on image:

   所以，为了更容易解释，我建议在图像上用下面的句子来改变：

  "Idle timeout of downstream connections" < "Idle timeout of upstream connections" 

"下游连接空闲超时" < "上游连接空闲超时"

Oh, I see the point now, but I'm not sure if it's a right thing to do... My reasoning is/was like that: on the "downstream" side (a reverse proxy  on the diagram), there is only one possible knob - the *downstream-to-upstream* (or just *upstream* for short) connection timeout. Hence, the name "upstream connection  timeout." And, symmetrically, on the upstream side there are only  (upstream-to-) downstream connections.

   哦，我现在明白了，但我不确定这样做是否正确......我的推理是/是这样的：在“下游”方面（图中的反向代理），只有一个可能的旋钮 - *下游到上游*（或简称*上游*）连接超时。因此，名称为“上游连接超时”。并且，对称地，在上游侧只有（上游到）下游连接。

  OK, now I can understand your intention, and I still have 2 questions:

   OK，现在我明白你的意思了，我还有2个问题：

  \1. How can I set idle connection timeout from downstream to upstream and vice versa?
   \2. What are the differences between Nginx's keepalive_timeout and idle connection timeout

  

   \1.如何设置从下游到上游的空闲连接超时，反之亦然？
  \2。 Nginx的keepalive_timeout和空闲连接超时有什么区别

  

  - I think it's technology-specific. For instance, in nginx there seems to be two similarly named but distinct knobs:

     - 我认为这是特定于技术的。例如，在 nginx 中似乎有两个名称相似但不同的旋钮：

    \- [`keepalive_timeout` set in the `http`, `server`, or `location` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_timeout%3AOqDTC-r2IaE1MYb8_g-BkgT0cfM&cuid=3735795) means the timeout for the downstream connections.

     \- [在 `http`、`server` 或 `location` 上下文中设置的 keepalive_timeout](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_timeout%3AOqDTC-r2IaE1MYb8_g-BkgT0cfM&cuid=3735795)表示下游连接超时。

    \- [` keepalive_timeout` set in the `upstream` context](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%3AEUWuizQE68MydDc5S6YVsO_vs9k&cuid=3735795) means the timeout during which an idle keepalive connection to an  upstream server will stay open (notice how there are two more knobs - ` keepalive_time` and ` keepalive_requests`).

     \- [在“上游”上下文中设置的“keepalive_timeout”](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%3AEUWuizQE68MydDc5S6YVsO_vs9k&cuid=3735795) 表示超时，在此期间与上游服务器的空闲保持连接将保持打开状态（注意还有两个旋钮 - `keepalive_time` 和 `keepalive_requests`)。

    I remember how back in 2018 we couldn't find a similar upstream know for AWS ALB. But maybe things have changed since then.

     我记得在 2018 年我们如何找不到类似的 AWS ALB 上游知识。但也许从那以后事情发生了变化。

    Since nginx, as a reverse proxy, acts as a client and as a server at the same time, it has two different sets of configs. But on the actual client-  or server- ends, there is usually just one timeout value. Often it's  called *idle timeout*, like in this Go `net/http` case ([Server.IdleTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Server%3A0ZDn9syuWGxHZXkhSruki6UtEqc&cuid=3735795) for the server-side & [Transport.IdleConnTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Transport%3AAazGyQh9twbSYj_YPhznwvqL9qM&cuid=3735795) for the client-side).

    

由于 nginx 作为反向代理，同时作为客户端和服务器，它有两套不同的配置。但在实际的客户端或服务器端，通常只有一个超时值。通常它被称为*空闲超时*，就像在这个 Go `net/http` 案例中一样 ([Server.IdleTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Server%3A0ZDn9syuWGxHZXkhSruki6UtEqc&cuid=3735795) 对于服务器端 & [Transport.IdleConnTimeout](https://disq.us/url?url=https%3A%2F%2Fpkg.go.dev%2Fnet%2Fhttp%23Transport%3AAazGyQh9twbSYj_YPhznwvqL9qM&cuid=3735795）对于客户端)。

    

Thanks for sharing!
for the 2nd mitigation "Disable connection reuse between the upstream and  the proxy (turn off HTTP Keep-Alive)", I think it is not a good idea, as it is gonna cause load on the upstream server as for each new  connection, a new tcp connection is created which is expensive. so it is important for the proxy server to do connection reuse as much as it  can.

感谢分享！
对于第二个缓解措施“禁用上游和代理之间的连接重用（关闭 HTTP Keep-Alive）”，我认为这不是一个好主意，因为对于每个新连接，它都会对上游服务器造成负载，a创建新的 tcp 连接是昂贵的。因此，代理服务器尽可能多地进行连接重用很重要。

That's a fair concern and a very good point! However, I think option 2 remains valid. Not every service has a high load. And for applications serving  just a few tens requests a minute, recreating connections will unlikely  cause any noticeable overhead. The irony is that it's often such small  to medium services that miss other resilience mechanisms such as retries on the client. So, disabling the connection reuse might be the easiest  and the cheapest way to fix 502s for them. Of course, for apps serving  thousands of requests a second it's a totally different story. But  usually those apps would have a dedicated SRE team behind and all the  needed retries in place :)

这是一个合理的关注点，也是一个很好的观点！但是，我认为选项 2 仍然有效。并非每个服务都有高负载。对于每分钟只处理几十个请求的应用程序，重新创建连接不太可能导致任何明显的开销。具有讽刺意味的是，通常是这样的中小型服务错过了其他弹性机制，例如客户端重试。因此，禁用连接重用可能是为他们修复 502 的最简单和最便宜的方法。当然，对于每秒处理数千个请求的应用程序来说，情况就完全不同了。但通常这些应用程序会有专门的 SRE 团队支持，并且所有需要的重试都已到位 :)

If we had sent ack of FIN before before the data we wouldn't face this  issue? (TCP Half close) Upstream closed the connection but proxy still  is using it.

如果我们在数据之前发送了 FIN ack，我们就不会遇到这个问题了吗？ （TCP 半关闭）上游关闭了连接，但代理仍在使用它。

[ ![Thumbnail](https://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg?w=800&h=611)](http://uploads.disquscdn.com/images/8f73543dca8511f5e8257f0f21a14de828aa5676545c981826b63a8cf4f8bdf6.jpg)

Hmm... I'm not sure how reasonable is a situation when the upstream already  closed its end of the connection, but the proxy keeps the other end  open. What kind of information should the proxy be writing to its end of the connection then? 

嗯...我不确定上游已经关闭其连接端但代理保持另一端打开的情况有多合理。那么代理应该向它的连接端写入什么样的信息呢？

The inverse situation looks legit, though. A proxy could push the request to the connection and then half-close it  before waiting for the upstream's reply. I'm not aware of any proxies  really doing it, but hypothetically such behaviour can be implemented by proxies allowing to configure max number of requests per connection ([example](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_requests%3AzFGxIfAi4lkU1QQWuwDNj5IBmRQ&cuid=3735795)).

不过，相反的情况看起来是合理的。代理可以将请求推送到连接，然后在等待上游的回复之前半关闭它。我不知道有任何代理确实在这样做，但假设这种行为可以通过允许配置每个连接的最大请求数的代理来实现（[示例](http://disq.us/url?url=http%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_core_module.html%23keepalive_requests%3AzFGxIfAi4lkU1QQWuwDNj5IBmRQ&cuid=3735795))。

Hello Ivan, first of all thank you for writing this post!

你好 Ivan，首先感谢你写这篇文章！

I'm trying to solve this very problem at the moment but so far I've been  unable to find the right set of configuration both for Nginx and for  uWSGI. In my case I have AWS ALB -> NGINX -> uWSGI.

我现在正在尝试解决这个问题，但到目前为止我一直无法为 Nginx 和 uWSGI 找到正确的配置集。在我的例子中，我有 AWS ALB -> NGINX -> uWSGI。

In Nginx I'm basically using the reverse proxy configuration with uwsgi socket:
`server {    listen 80; location / {        include uwsgi_params; uwsgi_pass unix:///tmp/uwsgi.sock; }}`

在 Nginx 中，我基本上使用带有 uwsgi 套接字的反向代理配置：
`服务器{听80;位置/{包括uwsgi_params； uwsgi_pass unix:///tmp/uwsgi.sock； }}`

I also noticed that I have **keepalive_timeout  65** set in the **/etc/nginx/nginx.conf**, but I assume it's ok since it's the AWS ALB who is communicating with Nginx.

我还注意到我在 **/etc/nginx/nginx.conf** 中设置了 **keepalive_timeout 65**，但我认为这没问题，因为与 Nginx 通信的是 AWS ALB。

I think the problem I'm having (intermittent 502 responses) is because of my uWSGI configuration:
`socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664hook-master-start = unix_signal:15 gracefully_kill_them_allneed-app = truedie-on-term = trueshow-config = truemaster = truedie-on-term = truememory -report = falsegevent = 20gevent-monkey-patch = trueso-keepalive = falseharakiri = 80http-timeout = 70socket-timeout = 70# new relic mandatory options# https://docs.newrelic.com/docs/python/python-agent- and-uwsgienable-threads = truesingle-interpreter = true`

我认为我遇到的问题（间歇性 502 响应）是因为我的 uWSGI 配置：
`socket = /tmp/uwsgi.sockchown-socket = nginx:nginxchmod-socket = 664hook-master-start = unix_signal:15 gracefully_kill_them_allneed-app = truedie-on-term = trueshow-config = truemaster = truedie-on-term = truememory -report = falsegevent = 20gevent-monkey-patch = trueso-keepalive = falseharakiri = 80http-timeout = 70socket-timeout = 70# new relic mandatory options# https://docs.newrelic.com/docs/python/python-agent- and-uwsgienable-threads = truesingle-interpreter = true`

Would you mind to share some insights? I'm having a hard time trying to translate your conclusion with the right [uWSGI configuration](https://disq.us/url?url=https%3A%2F%2Fuwsgi-docs.readthedocs.io%2Fen%2Flatest%2FOptions.html%3ALPS9dziFKJTkzZr3PDDD-pszxis&cuid=3735795).

你介意分享一些见解吗？我很难尝试用正确的 [uWSGI 配置](https://disq.us/url?url=https%3A%2F%2Fuwsgi-docs.readthedocs.io%2Fen%2Flatest%2FOptions.html%3ALPS9dziFKJTkzZr3PDDD-pszxis&cuid=3735795)。

Hi! It's likely that the problem is in between the ALB and nginx since the  uwsgi protocol doesn't have a notion of keepalive connections ([see this for more](https://disq.us/url?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%23keepalive%3AAVRyL044jRXhqj0kV67hZ9CCto8&cuid=3735795)). That 65 seconds setting of nginx is the time when it'll start closing  idle downstream connections (i.e., ALB's connections). You need to make  sure ALB has a shorter keepalive timeout for its upstream connections. Be careful and don't confuse the timeouts since often proxy and load  balancers (ALB, nginx, etc) have both the upstream and downstream  keepalive settings ([see this answer for more](http://disq.us/p/2qdh9vv)). ALB is a downstream for nginx, nginx is an upstream for ALB,  and end users are a downstream for ALB.

你好！问题很可能出在 ALB 和 nginx 之间，因为 uwsgi 协议没有保持活动连接的概念（[更多信息请参见此处](https://disq.us/url?url=https%3A%2F%2Fnginx.org%2Fen%2Fdocs%2Fhttp%2Fngx_http_upstream_module.html%23keepalive%3AAVRyL044jRXhqj0kV67hZ9CCto8&cuid=3735795))。 nginx 的 65 秒设置是它将开始关闭空闲下游连接（即 ALB 的连接）的时间。您需要确保 ALB 的上游连接具有更短的保持活动超时。小心不要混淆超时，因为代理和负载平衡器（ALB、nginx 等)通常同时具有上游和下游 keepalive 设置（[更多信息请参见此答案](http://disq.us/p/2qdh9vv)). ALB是nginx的下游，nginx是ALB的上游，终端用户是ALB的下游。

- Thank you Ivan.

   - 谢谢伊万。

  Just wanted to give a feedback in case someone faces this problem: as you  said, in our case the solution was to make sure the ALB's idle timeout  was shorter than the Nginx client timeout (client_header_timeout and,  client_body_timeout) and that the Nginx client timeout was shorter than  uwsgi timeout (upstream's configuration uwsgi_read_timeout and  uwsgi_send_timeout).

   只是想在有人遇到这个问题时提供反馈：正如你所说，在我们的例子中，解决方案是确保 ALB 的空闲超时时间短于 Nginx 客户端超时时间（client_header_timeout 和 client_body_timeout）并且 Nginx 客户端超时时间是比 uwsgi 超时时间短（上游的配置 uwsgi_read_timeout 和 uwsgi_send_timeout）。

  Example:
   **ALB's idle timeout:** 60s
   **Nginx.conf global configuration:** client_header_timeout 80s; client_body_timeout  80s;
   **uWSGI upstream configuration:** uwsgi_read_timeout 100s; uwsgi_send_timeout 100s; 

例子：
  **ALB 的空闲超时：** 60s
  **Nginx.conf全局配置：**client_header_timeout 80s； client_body_timeout 80 秒；
  **uWSGI 上游配置：** uwsgi_read_timeout 100s； uwsgi_send_timeout 100s；

