# Service Discovery in Kubernetes - Combining the Best of Two Worlds

# Kubernetes ä¸­çš„æœåŠ¡å‘ç° - ç»“åˆä¸¤ä¸ªä¸–ç•Œçš„ä¼˜ç‚¹

December 6, 2020 (Updated: July 31, 2021)

Before jumping to any Kubernetes specifics, let's talk about the service discovery problem in general.

åœ¨è·³åˆ°ä»»ä½• Kubernetes ç»†èŠ‚ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè°ˆè°ˆæœåŠ¡å‘ç°é—®é¢˜ã€‚

## What is Service Discovery

## ä»€ä¹ˆæ˜¯æœåŠ¡å‘ç°

In the world of web service development, it's a common practice to run multiple copies of a service at the same time. Every such copy is a separate instance of the service represented by a network endpoint (i.e. some _IP_ and _port_) exposing the service API. Traditionally, virtual or physical machines have been used to host such endpoints, with the shift towards containers in more recent times. Having multiple instances of the service running simultaneously increases its availability and helps to adjust the service capacity to meet the traffic demand. On the other hand, it also complicates the overall setup - before accessing the service, a client (the term _client_ is intentionally used loosely here; oftentimes a client of some service is another service) needs to figure out the actual IP address and the port it should use. The situation becomes even more tricky if we add the ephemeral nature of instances to the equation. New instances come and existing instances go because of the non-zero failure rate, up- and downscaling, or maintenance. That's how a so-called **_service discovery_** problem arises.

åœ¨ Web æœåŠ¡å¼€å‘é¢†åŸŸï¼ŒåŒæ—¶è¿è¡Œä¸€ä¸ªæœåŠ¡çš„å¤šä¸ªå‰¯æœ¬æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ã€‚æ¯ä¸ªè¿™æ ·çš„å‰¯æœ¬éƒ½æ˜¯ç”±æš´éœ²æœåŠ¡ API çš„ç½‘ç»œç«¯ç‚¹ï¼ˆå³æŸäº› _IP_ å’Œ _port_ï¼‰è¡¨ç¤ºçš„æœåŠ¡çš„å•ç‹¬å®ä¾‹ã€‚ä¼ ç»Ÿä¸Šï¼Œè™šæ‹Ÿæœºæˆ–ç‰©ç†æœºå·²è¢«ç”¨äºæ‰˜ç®¡æ­¤ç±»ç«¯ç‚¹ï¼Œæœ€è¿‘é€æ¸è½¬å‘å®¹å™¨ã€‚åŒæ—¶è¿è¡Œå¤šä¸ªæœåŠ¡å®ä¾‹å¯æé«˜å…¶å¯ç”¨æ€§ï¼Œå¹¶æœ‰åŠ©äºè°ƒæ•´æœåŠ¡å®¹é‡ä»¥æ»¡è¶³æµé‡éœ€æ±‚ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä¹Ÿä½¿æ•´ä½“è®¾ç½®å¤æ‚åŒ–â€”â€”åœ¨è®¿é—®æœåŠ¡ä¹‹å‰ï¼Œå®¢æˆ·ç«¯ï¼ˆæœ¯è¯­ _client_ åœ¨è¿™é‡Œæ•…æ„æ¾æ•£åœ°ä½¿ç”¨ï¼›é€šå¸¸æŸä¸ªæœåŠ¡çš„å®¢æˆ·ç«¯æ˜¯å¦ä¸€ä¸ªæœåŠ¡ï¼‰éœ€è¦å¼„æ¸…æ¥šå®é™…çš„ IP åœ°å€å’Œç«¯å£å®ƒåº”è¯¥ä½¿ç”¨ã€‚å¦‚æœæˆ‘ä»¬å°†å®ä¾‹çš„çŸ­æš‚æ€§è´¨æ·»åŠ åˆ°ç­‰å¼ä¸­ï¼Œæƒ…å†µä¼šå˜å¾—æ›´åŠ æ£˜æ‰‹ã€‚ç”±äºéé›¶æ•…éšœç‡ã€æ‰©å±•å’Œç¼©å‡æˆ–ç»´æŠ¤ï¼Œæ–°å®ä¾‹å‡ºç°å’Œç°æœ‰å®ä¾‹æ¶ˆå¤±ã€‚è¿™å°±æ˜¯æ‰€è°“çš„**_æœåŠ¡å‘ç°_**é—®é¢˜çš„äº§ç”Ÿæ–¹å¼ã€‚

![Service discovery problem](http://iximiuz.com/service-discovery-in-kubernetes/service-discovery-problem-4000-opt.png)

_Service discovery problem._

_æœåŠ¡å‘ç°é—®é¢˜ã€‚_

## Server-Side Service Discovery

## æœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°

A pretty common way of solving the service discovery problem is putting a load balancer _aka_ reverse proxy (e.g. Nginx or HAProxy) in front of the group of instances constituting a single service. An address (i.e. DNS name or less frequently IP) of such a load balancer is a much more stable piece of information. It can be communicated to the clients on development or configuration stages and assumed invariable during a single client lifespan. Then, from the client standpoint, accessing the multi-instance service is no different from accessing a single network endpoint. In other words, [the service discovery happens completely on the server-side](https://microservices.io/patterns/server-side-discovery.html).

è§£å†³æœåŠ¡å‘ç°é—®é¢˜çš„ä¸€ç§éå¸¸å¸¸è§çš„æ–¹æ³•æ˜¯åœ¨æ„æˆå•ä¸ªæœåŠ¡çš„å®ä¾‹ç»„ä¹‹å‰æ”¾ç½®ä¸€ä¸ªè´Ÿè½½å‡è¡¡å™¨_aka_åå‘ä»£ç†ï¼ˆä¾‹å¦‚ Nginx æˆ– HAProxyï¼‰ã€‚è¿™ç§è´Ÿè½½å‡è¡¡å™¨çš„åœ°å€ï¼ˆå³ DNS åç§°æˆ–é¢‘ç‡è¾ƒä½çš„ IPï¼‰æ˜¯ä¸€æ¡æ›´ç¨³å®šçš„ä¿¡æ¯ã€‚å®ƒå¯ä»¥åœ¨å¼€å‘æˆ–é…ç½®é˜¶æ®µä¼ è¾¾ç»™å®¢æˆ·ç«¯ï¼Œå¹¶å‡è®¾åœ¨å•ä¸ªå®¢æˆ·ç«¯ç”Ÿå‘½å‘¨æœŸå†…ä¿æŒä¸å˜ã€‚ç„¶åï¼Œä»å®¢æˆ·ç«¯çš„è§’åº¦æ¥çœ‹ï¼Œè®¿é—®å¤šå®ä¾‹æœåŠ¡ä¸è®¿é—®å•ä¸ªç½‘ç»œç«¯ç‚¹æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚æ¢å¥è¯è¯´ï¼Œ[æœåŠ¡å‘ç°å®Œå…¨å‘ç”Ÿåœ¨æœåŠ¡å™¨ç«¯](https://microservices.io/patterns/server-side-discovery.html)ã€‚

![Server-side service discovery example](http://iximiuz.com/service-discovery-in-kubernetes/server-side-service-discovery-4000-opt.png)

_Server-side service discovery._

_æœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°ã€‚_

The load balancer abstracts the volatile set of service instances away from the clients. However, the load balancer itself needs to be aware of the up to date state of the service fleet. This can be achieved by adding a [_service registry_](https://microservices.io/patterns/service-registry.html) component. On the startup, an instance needs to be added to the registry database. Upon the termination, the instance needs to be removed from it. One of the main tasks of the load balancer is to dynamically update its routing rules based on the service registry information.

è´Ÿè½½å¹³è¡¡å™¨ä»å®¢æˆ·ç«¯æŠ½è±¡å‡ºä¸€ç»„æ˜“å¤±çš„æœåŠ¡å®ä¾‹ã€‚ä½†æ˜¯ï¼Œè´Ÿè½½å‡è¡¡å™¨æœ¬èº«éœ€è¦äº†è§£æœåŠ¡é˜Ÿåˆ—çš„æœ€æ–°çŠ¶æ€ã€‚è¿™å¯ä»¥é€šè¿‡æ·»åŠ  [_service registry_](https://microservices.io/patterns/service-registry.html) ç»„ä»¶æ¥å®ç°ã€‚åœ¨å¯åŠ¨æ—¶ï¼Œéœ€è¦å°†ä¸€ä¸ªå®ä¾‹æ·»åŠ åˆ°æ³¨å†Œè¡¨æ•°æ®åº“ä¸­ã€‚ç»ˆæ­¢åï¼Œéœ€è¦å°†å®ä¾‹ä»ä¸­åˆ é™¤ã€‚è´Ÿè½½å‡è¡¡å™¨çš„ä¸»è¦ä»»åŠ¡ä¹‹ä¸€æ˜¯æ ¹æ®æœåŠ¡æ³¨å†Œä¿¡æ¯åŠ¨æ€æ›´æ–°å…¶è·¯ç”±è§„åˆ™ã€‚

While it looks very appealing from the client standpoint, the server-side service discovery may quickly reveal its downsides, especially in highly-loaded environments. The load balancer component is a **single point of failure and a potential throughput bottleneck**. To overcome this, the load balancing layer needs to be designed with a reasonable level of redundancy. Additionally, the load balancer may **need to be aware of all the communication protocols** used between services and clients and there will be always an **extra network hop** on the request path.

è™½ç„¶ä»å®¢æˆ·ç«¯çš„è§’åº¦æ¥çœ‹å®ƒçœ‹èµ·æ¥å¾ˆæœ‰å¸å¼•åŠ›ï¼Œä½†æœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°å¯èƒ½å¾ˆå¿«å°±ä¼šæš´éœ²å‡ºå®ƒçš„ç¼ºç‚¹ï¼Œå°¤å…¶æ˜¯åœ¨é«˜è´Ÿè½½ç¯å¢ƒä¸­ã€‚è´Ÿè½½å‡è¡¡å™¨ç»„ä»¶æ˜¯**å•ç‚¹æ•…éšœå’Œæ½œåœ¨çš„ååé‡ç“¶é¢ˆ**ã€‚ä¸ºäº†å…‹æœè¿™ä¸ªé—®é¢˜ï¼Œè´Ÿè½½å‡è¡¡å±‚éœ€è¦è®¾è®¡æœ‰åˆç†çš„å†—ä½™åº¦ã€‚æ­¤å¤–ï¼Œè´Ÿè½½å‡è¡¡å™¨å¯èƒ½**éœ€è¦äº†è§£æœåŠ¡å’Œå®¢æˆ·ç«¯ä¹‹é—´ä½¿ç”¨çš„æ‰€æœ‰é€šä¿¡åè®®**ï¼Œå¹¶ä¸”è¯·æ±‚è·¯å¾„ä¸Šæ€»ä¼šæœ‰**é¢å¤–çš„ç½‘ç»œè·ƒç‚¹**ã€‚

## Client-Side Service Discovery 

## å®¢æˆ·ç«¯æœåŠ¡å‘ç°

Can we solve the service discovery problem without introducing the centralized load balancing component? Sure! If we keep the _service registry_ component around, we can teach the clients to look up the service instance addresses in the _service registry_ directly. After fetching the full list of IP addresses constituting the service fleet, a client could pick up an instance based on the load balancing strategy at its disposal. In such a case, [the service discovery would be happening solely on the client-side](https://microservices.io/patterns/client-side-discovery.html). Probably the most prominent real-world implementation of the client-side service discovery is Netflix [Eureka](https://github.com/Netflix/eureka) and [Ribbon](https://github.com/Netflix/ribbon) projects.

åœ¨ä¸å¼•å…¥é›†ä¸­å¼è´Ÿè½½å‡è¡¡ç»„ä»¶çš„æƒ…å†µä¸‹ï¼Œèƒ½å¦è§£å†³æœåŠ¡å‘ç°é—®é¢˜ï¼Ÿå½“ç„¶ï¼å¦‚æœæˆ‘ä»¬ä¿ç•™ _service registry_ ç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥æ•™å®¢æˆ·ç«¯ç›´æ¥åœ¨ _service registry_ ä¸­æŸ¥æ‰¾æœåŠ¡å®ä¾‹åœ°å€ã€‚åœ¨è·å–æ„æˆæœåŠ¡é˜Ÿåˆ—çš„å®Œæ•´ IP åœ°å€åˆ—è¡¨åï¼Œå®¢æˆ·ç«¯å¯ä»¥æ ¹æ®å…¶å¤„ç†çš„è´Ÿè½½å¹³è¡¡ç­–ç•¥é€‰æ‹©ä¸€ä¸ªå®ä¾‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ[æœåŠ¡å‘ç°å°†ä»…å‘ç”Ÿåœ¨å®¢æˆ·ç«¯](https://microservices.io/patterns/client-side-discovery.html)ã€‚å®¢æˆ·ç«¯æœåŠ¡å‘ç°çš„æœ€çªå‡ºçš„ç°å®ä¸–ç•Œå®ç°å¯èƒ½æ˜¯ Netflix [Eureka](https://github.com/Netflix/eureka) å’Œ [Ribbon](https://github.com/Netflix/ribbon)é¡¹ç›®ã€‚

![Client-side service discovery example](http://iximiuz.com/service-discovery-in-kubernetes/client-side-service-discovery-4000-opt.png)

_Client-side service discovery._

_å®¢æˆ·ç«¯æœåŠ¡å‘ç°ã€‚_

The benefits of the client-side approach mostly come from the absence of the load balancer. There is neither a **single point of failure** nor a potential **throughput bottleneck** in the system design. There is also one less moving part which is usually a good thing and **no extra network hops on the packet path**.

å®¢æˆ·ç«¯æ–¹æ³•çš„å¥½å¤„ä¸»è¦æ¥è‡ªæ²¡æœ‰è´Ÿè½½å¹³è¡¡å™¨ã€‚ç³»ç»Ÿè®¾è®¡ä¸­æ—¢ä¸å­˜åœ¨**å•ç‚¹æ•…éšœ**ï¼Œä¹Ÿä¸å­˜åœ¨æ½œåœ¨çš„**ååé‡ç“¶é¢ˆ**ã€‚è¿˜æœ‰ä¸€ä¸ªè¾ƒå°‘ç§»åŠ¨çš„éƒ¨åˆ†ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä»¶å¥½äº‹ï¼Œ**æ•°æ®åŒ…è·¯å¾„ä¸Šæ²¡æœ‰é¢å¤–çš„ç½‘ç»œè·ƒç‚¹**ã€‚

However, as with the server-side service discovery, there are some significant drawbacks as well. Client-side service discovery **couples clients with the _service registry_**. It **requires some integration code** to be written for every programming language or framework in your ecosystem. And obviously, this **extra logic complicates the clients**. There seem to be an effort to [offload the client-side service discovery logic to the service proxy sidecars](http://iximiuz.com/en/posts/service-proxy-pod-sidecar-oh-my/) but that's already a different story...

ä½†æ˜¯ï¼Œä¸æœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°ä¸€æ ·ï¼Œä¹Ÿå­˜åœ¨ä¸€äº›æ˜æ˜¾çš„ç¼ºç‚¹ã€‚å®¢æˆ·ç«¯æœåŠ¡å‘ç°**å°†å®¢æˆ·ç«¯ä¸ _service registry_** ç»“åˆèµ·æ¥ã€‚å®ƒ**éœ€è¦ä¸ºç”Ÿæ€ç³»ç»Ÿä¸­çš„æ¯ç§ç¼–ç¨‹è¯­è¨€æˆ–æ¡†æ¶ç¼–å†™ä¸€äº›é›†æˆä»£ç **ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªé¢å¤–çš„é€»è¾‘ä½¿å®¢æˆ·ç«¯å¤æ‚åŒ–**ã€‚ä¼¼ä¹æœ‰åŠªåŠ›[å°†å®¢æˆ·ç«¯æœåŠ¡å‘ç°é€»è¾‘å¸è½½åˆ°æœåŠ¡ä»£ç†è¾¹è½¦](http://iximiuz.com/en/posts/service-proxy-pod-sidecar-oh-my/) ä½†å°±æ˜¯è¿™æ ·å·²ç»æ˜¯å¦ä¸€ä¸ªæ•…äº‹äº†...

## DNS and Service Discovery

## DNS å’ŒæœåŠ¡å‘ç°

Service discovery can be implemented in more than just two ways. There is a well-known [DNS-based service discovery (DNS-SD)](https://en.wikipedia.org/wiki/Zero-configuration_networking#DNS-based_service_discovery) approach that actually even predates the massive spread of microservices. Quoting the Wikipedia, _"a client discovers the list of available instances for a given service type by querying the DNS PTR record of that service type's name; the server returns zero or more names of the form `<Service>.<Domain>` , each corresponding to a SRV/TXT record pair. The SRV record resolves to the domain name providing the instance..."_ and then another DNS query can be used to resolve a chosen instance's domain name to the actual IP address. Wow, so many layers of indirection, sounds fun ğŸ™ˆ

æœåŠ¡å‘ç°å¯ä»¥é€šè¿‡ä¸æ­¢ä¸¤ç§æ–¹å¼å®ç°ã€‚æœ‰ä¸€ç§ä¼—æ‰€å‘¨çŸ¥çš„ [åŸºäº DNS çš„æœåŠ¡å‘ç° (DNS-SD)](https://en.wikipedia.org/wiki/Zero-configuration_networking#DNS-based_service_discovery) æ–¹æ³•å®é™…ä¸Šç”šè‡³æ—©äºå¾®æœåŠ¡çš„å¤§è§„æ¨¡ä¼ æ’­ã€‚å¼•ç”¨ç»´åŸºç™¾ç§‘ï¼Œ_â€œå®¢æˆ·ç«¯é€šè¿‡æŸ¥è¯¢è¯¥æœåŠ¡ç±»å‹åç§°çš„ DNS PTR è®°å½•æ¥å‘ç°ç»™å®šæœåŠ¡ç±»å‹çš„å¯ç”¨å®ä¾‹åˆ—è¡¨ï¼›æœåŠ¡å™¨è¿”å›é›¶ä¸ªæˆ–å¤šä¸ªå½¢å¼ä¸º `<Service>.<Domain>` çš„åç§°ï¼Œæ¯ä¸ªå¯¹åº”ä¸€ä¸ª SRV/TXT è®°å½•å¯¹ã€‚SRV è®°å½•è§£æä¸ºæä¾›å®ä¾‹çš„åŸŸå..."_ ç„¶åå¯ä»¥ä½¿ç”¨å¦ä¸€ä¸ª DNS æŸ¥è¯¢å°†æ‰€é€‰å®ä¾‹çš„åŸŸåè§£æä¸ºå®é™… IP åœ°å€ã€‚å“‡ï¼Œè¿™ä¹ˆå¤šå±‚çš„é—´æ¥ï¼Œå¬èµ·æ¥å¾ˆæœ‰è¶£ğŸ™ˆ

I never worked with DNS-SD, but to me, it doesn't sound like full-fledged service discovery. Rather, DNS is used as a _service registry_, and then depending on the dislocation of the code that knows how to query and interpret DNS-SD records, we can get either a [canonical client-side](https://github.com/benschw/srv-lb) or a [canonical server-side](https://www.haproxy.com/blog/dns-service-discovery-haproxy/) implementation.

æˆ‘ä»æœªä½¿ç”¨è¿‡ DNS-SDï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™å¬èµ·æ¥ä¸åƒæ˜¯æˆç†Ÿçš„æœåŠ¡å‘ç°ã€‚ç›¸åï¼ŒDNS è¢«ç”¨ä½œ_æœåŠ¡æ³¨å†Œä¸­å¿ƒ_ï¼Œç„¶åæ ¹æ®çŸ¥é“å¦‚ä½•æŸ¥è¯¢å’Œè§£é‡Š DNS-SD è®°å½•çš„ä»£ç çš„é”™ä½ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ª[è§„èŒƒå®¢æˆ·ç«¯](https://github.com/benschw/srv-lb) æˆ– [è§„èŒƒæœåŠ¡å™¨ç«¯](https://www.haproxy.com/blog/dns-service-discovery-haproxy/) å®ç°ã€‚

Alternatively, [Round-robin DNS](https://en.wikipedia.org/wiki/Round-robin_DNS) can be (ab)used for service discovery. Even though it was originally designed for load balancing, or rather load distribution, having multiple A records for the same hostname (i.e. service name) returned in a rotating manner implicitly abstracts multiple replicas behind a single service name.

æˆ–è€…ï¼Œ[Round-robin DNS](https://en.wikipedia.org/wiki/Round-robin_DNS)å¯ä»¥ï¼ˆabï¼‰ç”¨äºæœåŠ¡å‘ç°ã€‚å°½ç®¡å®ƒæœ€åˆæ˜¯ä¸ºè´Ÿè½½å¹³è¡¡æˆ–è´Ÿè½½åˆ†é…è€Œè®¾è®¡çš„ï¼Œä½†ä»¥è½®æ¢æ–¹å¼è¿”å›çš„ç›¸åŒä¸»æœºåï¼ˆå³æœåŠ¡åç§°)çš„å¤šä¸ª A è®°å½•éšå¼åœ°æŠ½è±¡äº†å•ä¸ªæœåŠ¡åç§°åé¢çš„å¤šä¸ªå‰¯æœ¬ã€‚

In any case, **DNS has a significant drawback when used for service discovery**. Updating DNS records is a slow procedure. There are **multiple layers of caches**, including the client-side libraries, and historically record's TTL isn't strictly respected. As a result, propagating the change in the set of service instances to all the clients can take a while.

åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œ**DNS åœ¨ç”¨äºæœåŠ¡å‘ç°æ—¶éƒ½æœ‰ä¸€ä¸ªæ˜æ˜¾çš„ç¼ºç‚¹**ã€‚æ›´æ–° DNS è®°å½•æ˜¯ä¸€ä¸ªç¼“æ…¢çš„è¿‡ç¨‹ã€‚æœ‰**å¤šå±‚ç¼“å­˜**ï¼ŒåŒ…æ‹¬å®¢æˆ·ç«¯åº“ï¼Œå¹¶ä¸”å†å²è®°å½•çš„ TTL æ²¡æœ‰ä¸¥æ ¼éµå®ˆã€‚å› æ­¤ï¼Œå°†ä¸€ç»„æœåŠ¡å®ä¾‹ä¸­çš„æ›´æ”¹ä¼ æ’­åˆ°æ‰€æœ‰å®¢æˆ·ç«¯å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ã€‚

## Service Discovery in Kubernetes

## Kubernetes ä¸­çš„æœåŠ¡å‘ç°

First, let's play an analogy game! 

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç©ä¸€ä¸ªç±»æ¯”æ¸¸æˆï¼

If I were to draw some analogies between Kubernetes and more traditional architectures, I'd compare Kubernetes [_Pods_](https://kubernetes.io/docs/concepts/workloads/pods/) with service instances. _Pods_ are [many things](https://www.reddit.com/r/kubernetes/comments/k0zc0m/kubernetes_pods_are_logical_hosts_or_simply/?utm_source=share&utm_medium=web2x&context=3) to [many people](https://twitter.com/iximiuz/status/1331630336707596288?s=20), however, when it comes to networking [the documentation clearly states that](https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model) _"...Pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration."_

å¦‚æœæˆ‘è¦åœ¨ Kubernetes å’Œæ›´ä¼ ç»Ÿçš„æ¶æ„ä¹‹é—´è¿›è¡Œä¸€äº›ç±»æ¯”ï¼Œæˆ‘ä¼šå°† Kubernetes [_Pods_](https://kubernetes.io/docs/concepts/workloads/pods/) ä¸æœåŠ¡å®ä¾‹è¿›è¡Œæ¯”è¾ƒã€‚ _Pods_ æ˜¯ [è®¸å¤šä¸œè¥¿](https://www.reddit.com/r/kubernetes/comments/k0zc0m/kubernetes_pods_are_logical_hosts_or_simply/?utm_source=share&utm_medium=web2x&context=3) å¯¹ [è®¸å¤šäºº](https://twitter.com/iximiuz/status/1331630336707596288?s=20)ï¼Œç„¶è€Œï¼Œå½“æ¶‰åŠåˆ°ç½‘ç»œæ—¶[æ–‡æ¡£æ˜ç¡®æŒ‡å‡º](https://kubernetes.io/docs/concepts/cluster-administration/networking/#the-kubernetes-network-model) _"...ä»ç«¯å£åˆ†é…ã€å‘½åã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡ã€åº”ç”¨ç¨‹åºé…ç½®å’Œè¿ç§»çš„è§’åº¦æ¥çœ‹ï¼ŒPods å¯ä»¥åƒè™šæ‹Ÿæœºæˆ–ç‰©ç†ä¸»æœºä¸€æ ·å¯¹å¾…ã€‚"_

If _Pods_ correspond to individual instances of a service, I'd expect a similar analogy for the _service_, as a logical grouping of instances, itself. And indeed there is a suitable concept in Kubernetes called... surprise, surprise, a [_Service_](https://kubernetes.io/docs/concepts/services-networking/service/). _" In Kubernetes, a Service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service)."_

å¦‚æœ _Pods_ å¯¹åº”äºæœåŠ¡çš„å•ä¸ªå®ä¾‹ï¼Œæˆ‘å¸Œæœ› _service_ æœ‰ç±»ä¼¼çš„ç±»æ¯”ï¼Œä½œä¸ºå®ä¾‹çš„é€»è¾‘åˆ†ç»„æœ¬èº«ã€‚äº‹å®ä¸Šï¼ŒKubernetes ä¸­æœ‰ä¸€ä¸ªåˆé€‚çš„æ¦‚å¿µå«åš...æƒŠå–œï¼ŒæƒŠå–œï¼Œä¸€ä¸ª [_Service_](https://kubernetes.io/docs/concepts/services-networking/service/)ã€‚ _â€œåœ¨ Kubernetes ä¸­ï¼ŒæœåŠ¡æ˜¯ä¸€ç§æŠ½è±¡ï¼Œå®ƒå®šä¹‰äº†ä¸€ç»„é€»è¾‘ Pod å’Œè®¿é—®å®ƒä»¬çš„ç­–ç•¥ï¼ˆæœ‰æ—¶è¿™ç§æ¨¡å¼ç§°ä¸ºå¾®æœåŠ¡)ã€‚â€_

Strengthening the analogy, the set of Pods making up a Service should be also considered ephemeral because neither the Pods' headcount nor the final set of IP addresses is stable. Thus, in Kubernetes, the problem of providing a reliable **service discovery** remains actual.

åŠ å¼ºç±»æ¯”ï¼Œç»„æˆ Service çš„ä¸€ç»„ Pod ä¹Ÿåº”è¯¥è¢«è§†ä¸ºçŸ­æš‚çš„ï¼Œå› ä¸º Pod çš„äººæ•°å’Œæœ€ç»ˆçš„ IP åœ°å€é›†éƒ½ä¸æ˜¯ç¨³å®šçš„ã€‚å› æ­¤ï¼Œåœ¨ Kubernetes ä¸­ï¼Œæä¾›å¯é çš„**æœåŠ¡å‘ç°**çš„é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚

While creating a new Service, one should choose a name that will be used to refer to the set of Pods constituting the service. Among other things, the Service maintains an up to date list of IP addresses of its Pods organized as an [_Endpoints_](https://kubernetes.io/docs/reference/glossary/?all=true#term-endpoints) (or [_EndpontSlice_](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/) since Kubernetes 1.17) object. [Citing the documentation](https://kubernetes.io/docs/concepts/services-networking/service/#cloud-native-service-discovery) one more time, _"...if you're able to use Kubernetes APIs for service discovery in your application, you can query the API server for Endpoints,_ [I guess using the Service name] _that get updated whenever the set of Pods in a Service changes."_ Well, sounds like an invitation to implement a cloud-native Kubernetes-native client-side service discovery with the Kubernetes control plane playing (in particular) the role of the _service registry_.

åœ¨åˆ›å»ºæ–°æœåŠ¡æ—¶ï¼Œåº”è¯¥é€‰æ‹©ä¸€ä¸ªåç§°ï¼Œç”¨äºæŒ‡ä»£æ„æˆæœåŠ¡çš„ä¸€ç»„ Podã€‚é™¤å…¶ä»–å¤–ï¼Œè¯¥æœåŠ¡ç»´æŠ¤å…¶ Pod çš„æœ€æ–° IP åœ°å€åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨ç»„ç»‡ä¸º [_Endpoints_](https://kubernetes.io/docs/reference/glossary/?all=true#term-endpoints)ï¼ˆæˆ–[_EndpontSlice_](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/) è‡ª Kubernetes 1.17) å¯¹è±¡ã€‚ [å¼•ç”¨æ–‡æ¡£](https://kubernetes.io/docs/concepts/services-networking/service/#cloud-native-service-discovery) å†æ¥ä¸€æ¬¡ï¼Œ_"...å¦‚æœä½ èƒ½å¤Ÿä½¿ç”¨ Kubernetesåœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ç”¨äºæœåŠ¡å‘ç°çš„ APIï¼Œæ‚¨å¯ä»¥æŸ¥è¯¢ API æœåŠ¡å™¨ä»¥è·å–ç«¯ç‚¹ï¼Œ_ [æˆ‘çŒœä½¿ç”¨æœåŠ¡åç§°] _æ¯å½“æœåŠ¡ä¸­çš„ä¸€ç»„ Pod å‘ç”Ÿå˜åŒ–æ—¶éƒ½ä¼šæ›´æ–°ã€‚â€_ å¥½å§ï¼Œå¬èµ·æ¥åƒæ˜¯å®ç°ä¸€ä¸ªäº‘åŸç”Ÿ Kubernetes åŸç”Ÿå®¢æˆ·ç«¯æœåŠ¡å‘ç°ï¼ŒKubernetes æ§åˆ¶å¹³é¢æ‰®æ¼”ï¼ˆç‰¹åˆ«æ˜¯)_service registry_ çš„è§’è‰²ã€‚

![Kubernetes-native client-side service discovery](http://iximiuz.com/service-discovery-in-kubernetes/kube-native-service-discovery-4000-opt.png)

_Kubernetes-native client-side service discovery._

_Kubernetes åŸç”Ÿå®¢æˆ·ç«¯æœåŠ¡å‘ç°ã€‚_

However, the only real-world usage of this mechanism I've stumbled upon so far was in the [_service mesh_](https://linkerd.io/2020/07/23/under-the-hood-of-linkerds-state-of-the-art-rust-proxy-linkerd2-proxy/#the-life-of-a-request) kind of software. It's a bit unfair to mention it here though because _service mesh_ itself is needed to provide, in particular, the service discovery mechanism for its users. So, if you're aware of the client-side service discovery implementations leveraging Kubernetes Endpoints API please drop a comment below.

ä½†æ˜¯ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å¶ç„¶å‘ç°çš„è¿™ç§æœºåˆ¶çš„å”¯ä¸€å®é™…ç”¨æ³•æ˜¯åœ¨ [_service mesh_](https://linkerd.io/2020/07/23/under-the-hood-of-linkerds-state-of-the-art-rust-proxy-linkerd2-proxy/#the-life-of-a-request) ç±»å‹çš„è½¯ä»¶ã€‚åœ¨è¿™é‡ŒæåŠå®ƒæœ‰ç‚¹ä¸å…¬å¹³ï¼Œå› ä¸ºéœ€è¦ _service mesh_ æœ¬èº«æ¥ä¸ºå…¶ç”¨æˆ·æä¾›æœåŠ¡å‘ç°æœºåˆ¶ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨äº†è§£åˆ©ç”¨ Kubernetes Endpoints API çš„å®¢æˆ·ç«¯æœåŠ¡å‘ç°å®ç°ï¼Œè¯·åœ¨ä¸‹é¢å‘è¡¨è¯„è®ºã€‚

Luckily, as with many other things in Kubernetes, there's more than one way to skin a cat to get the service discovery done. And the applications that weren't born cloud-native (i.e. 99% of them) will likely find the next service-discovery mechanism much more appealing.

å¹¸è¿çš„æ˜¯ï¼Œä¸ Kubernetes ä¸­çš„è®¸å¤šå…¶ä»–ä¸œè¥¿ä¸€æ ·ï¼Œæœ‰ä¸æ­¢ä¸€ç§æ–¹æ³•å¯ä»¥ç»™çŒ«å‰¥çš®æ¥å®ŒæˆæœåŠ¡å‘ç°ã€‚è€Œé‚£äº›ä¸æ˜¯åŸç”Ÿäº‘çš„åº”ç”¨ç¨‹åºï¼ˆå³ 99%ï¼‰å¯èƒ½ä¼šå‘ç°ä¸‹ä¸€ä¸ªæœåŠ¡å‘ç°æœºåˆ¶æ›´å…·å¸å¼•åŠ›ã€‚

## Network-Side Service Discovery

## ç½‘ç»œç«¯æœåŠ¡å‘ç°

_Disclaimer: I've no idea if there is such thing as network-side service discovery in other domains and I've never seen the usage of this term in the microservices world. But I find it funny and suitable for the purpose of this paragraph._ 

_å…è´£å£°æ˜ï¼šæˆ‘ä¸çŸ¥é“å…¶ä»–é¢†åŸŸæ˜¯å¦æœ‰ç½‘ç»œç«¯æœåŠ¡å‘ç°è¿™æ ·çš„ä¸œè¥¿ï¼Œè€Œä¸”æˆ‘ä»æœªåœ¨å¾®æœåŠ¡ä¸–ç•Œä¸­çœ‹åˆ°è¿‡è¿™ä¸ªæœ¯è¯­çš„ç”¨æ³•ã€‚ä½†æˆ‘è§‰å¾—è¿™å¾ˆæœ‰è¶£ï¼Œé€‚åˆæœ¬æ®µçš„ç›®çš„ã€‚_

In Kubernetes, the name of a Service object must be a valid [DNS label name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names). It's not a coincidence. When the DNS add-on is enabled ( [and I guess it's almost always the case](https://kubernetes.io/docs/concepts/services-networking/service/#dns)), every Service gets a DNS record like `<service-name>.<namespace-name>`. Obviously, this name can be used by applications to access the service and it simplifies the life of the clients to the highest possible extent. A single well-known address behind every Service **eradicates the need for any service discovery logic on the client-side**.

åœ¨ Kubernetes ä¸­ï¼ŒService å¯¹è±¡çš„åç§°å¿…é¡»æ˜¯æœ‰æ•ˆçš„ [DNS æ ‡ç­¾åç§°](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names)ã€‚è¿™ä¸æ˜¯å·§åˆã€‚å½“å¯ç”¨ DNS é™„åŠ ç»„ä»¶æ—¶ï¼ˆ[æˆ‘çŒœå®ƒå‡ ä¹æ€»æ˜¯è¿™æ ·](https://kubernetes.io/docs/concepts/services-networking/service/#dns))ï¼Œæ¯ä¸ªæœåŠ¡éƒ½ä¼šè·å¾—ä¸€ä¸ª DNS è®°å½•ï¼Œä¾‹å¦‚`<æœåŠ¡åç§°>.<å‘½åç©ºé—´åç§°>`ã€‚æ˜¾ç„¶ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥ä½¿ç”¨è¯¥åç§°æ¥è®¿é—®æœåŠ¡ï¼Œå¹¶ä¸”æœ€å¤§é™åº¦åœ°ç®€åŒ–äº†å®¢æˆ·ç«¯çš„ç”Ÿå‘½å‘¨æœŸã€‚æ¯ä¸ªæœåŠ¡èƒŒåçš„ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„åœ°å€**æ¶ˆé™¤äº†å®¢æˆ·ç«¯å¯¹ä»»ä½•æœåŠ¡å‘ç°é€»è¾‘çš„éœ€æ±‚**ã€‚

However, as we already know, DNS is often ill-suited for service discovery and [the Kubernetes ecosystem is not an exception](https://kubernetes.io/docs/concepts/services-networking/service/#why-not-use-round-robin-dns). Therefore, instead of using round-robin DNS to list Pods' IP addresses, Kubernetes introduces one more IP address for every service. This IP address is called `clusterIP` (not to be confused with the [`ClusterIP` service type](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)). Similar to the DNS name, this address can be used to transparently access Pods constituting the service.

ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬å·²ç»çŸ¥é“çš„ï¼ŒDNS é€šå¸¸ä¸é€‚åˆæœåŠ¡å‘ç°ï¼Œ[Kubernetes ç”Ÿæ€ç³»ç»Ÿä¹Ÿä¸ä¾‹å¤–](https://kubernetes.io/docs/concepts/services-networking/service/#why-not-ä½¿ç”¨round-robin-dnsï¼‰ã€‚å› æ­¤ï¼ŒKubernetes ä¸æ˜¯ä½¿ç”¨å¾ªç¯ DNS æ¥åˆ—å‡º Pod çš„ IP åœ°å€ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªæœåŠ¡å¼•å…¥äº†ä¸€ä¸ª IP åœ°å€ã€‚è¿™ä¸ªIPåœ°å€ç§°ä¸º`clusterIP`ï¼ˆä¸è¦ä¸[`ClusterIP`æœåŠ¡ç±»å‹](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-typesï¼‰æ··æ·†))ã€‚ä¸ DNS åç§°ç±»ä¼¼ï¼Œè¯¥åœ°å€å¯ç”¨äºé€æ˜è®¿é—®æ„æˆæœåŠ¡çš„ Podã€‚

_NB: there is actually no hard dependency on DNS for Kubernetes applications. Clients can always learn a `clusterIP` of a service by inspecting their [environment variables](https://kubernetes.io/docs/concepts/services-networking/service/#environment-variables). Upon a pod startup **for every running service** Kubernetes injects a couple of env variables looking like `<service-name>_SERVICE_HOST` and `<service-name>_SERVICE_PORT`._

_NBï¼šKubernetes åº”ç”¨ç¨‹åºå®é™…ä¸Šæ²¡æœ‰å¯¹ DNS çš„ç¡¬ä¾èµ–ã€‚å®¢æˆ·ç«¯æ€»æ˜¯å¯ä»¥é€šè¿‡æ£€æŸ¥ä»–ä»¬çš„ [ç¯å¢ƒå˜é‡]æ¥äº†è§£æœåŠ¡çš„ `clusterIP`ã€‚åœ¨ pod å¯åŠ¨æ—¶**å¯¹äºæ¯ä¸ªæ­£åœ¨è¿è¡Œçš„æœåŠ¡** Kubernetes æ³¨å…¥å‡ ä¸ªç¯å¢ƒå˜é‡ï¼Œçœ‹èµ·æ¥åƒ `<service-name>_SERVICE_HOST` å’Œ `<service-name>_SERVICE_PORT`ã€‚_

Ok, here is one more analogy. The resulting (logical) setup looks much like a load balancer or reverse proxy sitting in front of the set of virtual machines.

å¥½çš„ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªç±»æ¯”ã€‚ç»“æœï¼ˆé€»è¾‘ï¼‰è®¾ç½®çœ‹èµ·æ¥å¾ˆåƒä½äºä¸€ç»„è™šæ‹Ÿæœºå‰é¢çš„è´Ÿè½½å¹³è¡¡å™¨æˆ–åå‘ä»£ç†ã€‚

![Server-side (logically) service discovery in Kubernetes](http://iximiuz.com/service-discovery-in-kubernetes/kube-logical-service-discovery-4000-opt.png)

_Server-side (logically) service discovery in Kubernetes._

_Kubernetes ä¸­çš„æœåŠ¡å™¨ç«¯ï¼ˆé€»è¾‘ä¸Šï¼‰æœåŠ¡å‘ç°ã€‚_

But there is more to it than just that. This `clusterIP` is a so-called virtual address. When I stumbled upon the concept of the virtual IP for the first time it was a real mind-bender.

ä½†è¿˜æœ‰æ›´å¤šçš„ä¸œè¥¿ã€‚è¿™ä¸ª`clusterIP`å°±æ˜¯æ‰€è°“çš„è™šæ‹Ÿåœ°å€ã€‚å½“æˆ‘ç¬¬ä¸€æ¬¡å¶ç„¶å‘ç°è™šæ‹Ÿ IP çš„æ¦‚å¿µæ—¶ï¼Œæˆ‘çœŸçš„å¾ˆå›°æƒ‘ã€‚

A _virtual IP_ basically means that there is no single network interface in the whole system carrying it around! Instead, there is a super-powerful and likely underestimated background component called [_kube-proxy_](https://kubernetes.io/docs/concepts/overview/components/#kube-proxy) that magically makes all the Pods (and even Nodes) [thinking the Service IPs do exist](https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies): _"Every node in a Kubernetes cluster runs a `kube-proxy`. `kube-proxy` is responsible for implementing a form of virtual IP for Services of type other than ExternalName."_ 

_è™šæ‹Ÿ IP_ åŸºæœ¬ä¸Šæ„å‘³ç€æ•´ä¸ªç³»ç»Ÿä¸­æ²¡æœ‰å•ä¸ªç½‘ç»œæ¥å£æ‰¿è½½å®ƒï¼ç›¸åï¼Œæœ‰ä¸€ä¸ªåä¸º [_kube-proxy_](https://kubernetes.io/docs/concepts/overview/components/#kube-proxy) çš„è¶…çº§å¼ºå¤§ä¸”å¯èƒ½è¢«ä½ä¼°çš„åå°ç»„ä»¶ï¼Œå®ƒç¥å¥‡åœ°ä½¿æ‰€æœ‰ Podï¼ˆç”šè‡³Nodes) [è®¤ä¸º Service IP ç¡®å®å­˜åœ¨](https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies)ï¼š_"Kubernetes é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½è¿è¡Œä¸€ä¸ª `kube-proxy`ã€‚`kube-proxy` è´Ÿè´£ä¸ºé ExternalName ç±»å‹çš„æœåŠ¡å®ç°ä¸€ç§å½¢å¼çš„è™šæ‹Ÿ IPã€‚â€_

Funnily enough, the _kube-proxy_ component is actually a [misnomer](https://en.wikipedia.org/wiki/Misnomer). I.e. it's not really a proxy anymore, although [it was born as a true user space proxy](https://github.com/kubernetes/kubernetes/issues/1107). I'm not going to dive into implementation details here, there is plenty of information on the Internet including the official Kubernetes documentation and [this great article](https://arthurchiao.art/blog/cracking-k8s-node-proxy/) of Arthur Chiao. Long story short - _kube-proxy_ **operates on the network layer** using such Linux capabilities as iptables or IPVS and transparently substitutes the destination `clusterIP` with an IP address of some service's Pod. Thus, **_kube-proxy_ is one of the main implementers of the service discovery and load balancing in the cluster**.

æœ‰è¶£çš„æ˜¯ï¼Œ_kube-proxy_ ç»„ä»¶å®é™…ä¸Šæ˜¯ä¸€ä¸ª [misnomer](https://en.wikipedia.org/wiki/Misnomer)ã€‚ IEã€‚å®ƒä¸å†æ˜¯çœŸæ­£çš„ä»£ç†ï¼Œå°½ç®¡ [å®ƒæ˜¯ä½œä¸ºçœŸæ­£çš„ç”¨æˆ·ç©ºé—´ä»£ç†è¯ç”Ÿçš„](https://github.com/kubernetes/kubernetes/issues/1107)ã€‚æˆ‘ä¸æ‰“ç®—åœ¨è¿™é‡Œæ·±å…¥ç ”ç©¶å®ç°ç»†èŠ‚ï¼Œäº’è”ç½‘ä¸Šæœ‰å¾ˆå¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å®˜æ–¹ Kubernetes æ–‡æ¡£å’Œ [è¿™ç¯‡å¾ˆæ£’çš„æ–‡ç« ](https://arthurchio.art/blog/cracking-k8s-node-proxy/) äºšç‘ŸÂ·ä¹”ã€‚é•¿è¯çŸ­è¯´ - _kube-proxy_**åœ¨ç½‘ç»œå±‚**è¿è¡Œï¼Œä½¿ç”¨è¯¸å¦‚ iptables æˆ– IPVS ä¹‹ç±»çš„ Linux åŠŸèƒ½ï¼Œå¹¶é€æ˜åœ°å°†ç›®æ ‡ `clusterIP` æ›¿æ¢ä¸ºæŸäº›æœåŠ¡çš„ Pod çš„ IP åœ°å€ã€‚å› æ­¤ï¼Œ**_kube-proxy_ æ˜¯é›†ç¾¤ä¸­æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡çš„ä¸»è¦å®ç°è€…ä¹‹ä¸€**ã€‚

![Kube-proxy implements service discovery in Kubernetes](http://iximiuz.com/service-discovery-in-kubernetes/kube-proxy-service-discovery-4000-opt.png)

_Kube-proxy implements service discovery in Kubernetes._

_Kube-proxy åœ¨ Kubernetes ä¸­å®ç°æœåŠ¡å‘ç°ã€‚_

The _kube-proxy_ component turns every Kubernetes node into a service proxy (just another fancy name for a client-side proxy) and all pod-to-pod traffic always goes through its local service proxy.

_kube-proxy_ ç»„ä»¶å°†æ¯ä¸ª Kubernetes èŠ‚ç‚¹å˜æˆä¸€ä¸ªæœåŠ¡ä»£ç†ï¼ˆå®¢æˆ·ç«¯ä»£ç†çš„å¦ä¸€ä¸ªèŠ±å“¨åç§°ï¼‰ï¼Œå¹¶ä¸”æ‰€æœ‰ pod åˆ° pod çš„æµé‡æ€»æ˜¯é€šè¿‡å…¶æœ¬åœ°æœåŠ¡ä»£ç†ã€‚

Now, let's see what does it mean from the service discovery standpoint. Since there are as many self-sufficient proxies as the number of nodes in the cluster, there is **no single point of failure** when it comes to load balancing. Unlike the canonical server-side service discovery technique with the centralized load balancer component, _kube-proxy_-based service discovery follows the decentralized approach with all the nodes sharing a comparable amount of traffic. Hence, **the probability of getting a throughput bottleneck is also much lower.** On top of that, there is **no extra network hop** on the packet's path because every Pod contacts its node-local copy of proxy.

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ä»æœåŠ¡å‘ç°çš„è§’åº¦æ¥çœ‹è¿™æ„å‘³ç€ä»€ä¹ˆã€‚ç”±äºè‡ªç»™è‡ªè¶³çš„ä»£ç†æ•°é‡ä¸é›†ç¾¤ä¸­çš„èŠ‚ç‚¹æ•°é‡ä¸€æ ·å¤šï¼Œå› æ­¤åœ¨è´Ÿè½½å‡è¡¡æ–¹é¢**æ²¡æœ‰å•ç‚¹æ•…éšœ**ã€‚ä¸å…·æœ‰é›†ä¸­å¼è´Ÿè½½å‡è¡¡å™¨ç»„ä»¶çš„è§„èŒƒæœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°æŠ€æœ¯ä¸åŒï¼ŒåŸºäº _kube-proxy_ çš„æœåŠ¡å‘ç°éµå¾ªåˆ†æ•£å¼æ–¹æ³•ï¼Œæ‰€æœ‰èŠ‚ç‚¹å…±äº«ç›¸å½“æ•°é‡çš„æµé‡ã€‚å› æ­¤ï¼Œ**å‡ºç°ååé‡ç“¶é¢ˆçš„å¯èƒ½æ€§ä¹Ÿä½å¾—å¤šã€‚** æœ€é‡è¦çš„æ˜¯ï¼Œåœ¨æ•°æ®åŒ…çš„è·¯å¾„ä¸Š**æ²¡æœ‰é¢å¤–çš„ç½‘ç»œè·ƒç‚¹**ï¼Œå› ä¸ºæ¯ä¸ª Pod éƒ½è”ç³»å…¶èŠ‚ç‚¹æœ¬åœ°çš„ä»£ç†å‰¯æœ¬ã€‚

**Thereby, Kubernetes takes the best of both worlds. As with the server-side service discovery, clients can simply access a single endpoint, a stable Service IP address, i.e. there is no need for advanced logic on the application side. At the same time, physically the service discovery and load balancing happen on every cluster node, i.e. close to the client-side. Thus, there are no traditional downsides of the server-side service discovery.**

**å› æ­¤ï¼ŒKubernetes ä¸¤å…¨å…¶ç¾ã€‚ä¸æœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°ä¸€æ ·ï¼Œå®¢æˆ·ç«¯å¯ä»¥ç®€å•åœ°è®¿é—®å•ä¸ªç«¯ç‚¹ã€ä¸€ä¸ªç¨³å®šçš„æœåŠ¡ IP åœ°å€ï¼Œå³ä¸éœ€è¦åº”ç”¨ç¨‹åºç«¯çš„é«˜çº§é€»è¾‘ã€‚åŒæ—¶ï¼ŒæœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡åœ¨ç‰©ç†ä¸Šå‘ç”Ÿåœ¨æ¯ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Šï¼Œå³é è¿‘å®¢æˆ·ç«¯ã€‚å› æ­¤ï¼ŒæœåŠ¡å™¨ç«¯æœåŠ¡å‘ç°æ²¡æœ‰ä¼ ç»Ÿçš„ç¼ºç‚¹ã€‚**

Since the implementation of the service discovery in Kubernetes heavily relies on the Linux network stack, I'm inclined to call it a _network-side_ service discovery. Although, the term _service-side_ service discovery might work as well.

ç”±äº Kubernetes ä¸­æœåŠ¡å‘ç°çš„å®ç°ä¸¥é‡ä¾èµ– Linux ç½‘ç»œå †æ ˆï¼Œå› æ­¤æˆ‘å€¾å‘äºå°†å…¶ç§°ä¸º _network-side_ æœåŠ¡å‘ç°ã€‚è™½ç„¶ï¼Œæœ¯è¯­ _service-side_ æœåŠ¡å‘ç°ä¹Ÿå¯èƒ½é€‚ç”¨ã€‚

## Conclusion

##  ç»“è®º

Kubernetes tries hard to make the transition from more traditional virtual or bare-metal ecosystems to containers simple. Kubernetes NAT-less networking model, Pods, and Services allow familiar designs to be reapplied without significant adjustments. But Kubernetes goes even further and provides [a very reliable and elegant solution for the in-cluster service discovery and load balancing problems](https://kubernetespodcast.com/episode/129-linkerd/) out of the box. On top of that, the provided solution turned out to be easy to extend and that gave birth to such an amazing piece of software as a [Kubernetes-native service mesh](https://github.com/linkerd/linkerd2).

Kubernetes åŠªåŠ›ä½¿ä»æ›´ä¼ ç»Ÿçš„è™šæ‹Ÿæˆ–è£¸æœºç”Ÿæ€ç³»ç»Ÿå‘å®¹å™¨çš„è¿‡æ¸¡å˜å¾—ç®€å•ã€‚ Kubernetes æ—  NAT ç½‘ç»œæ¨¡å‹ã€Pod å’ŒæœåŠ¡å…è®¸é‡æ–°åº”ç”¨ç†Ÿæ‚‰çš„è®¾è®¡ï¼Œè€Œæ— éœ€è¿›è¡Œé‡å¤§è°ƒæ•´ã€‚ä½†æ˜¯ Kubernetes èµ°å¾—æ›´è¿œï¼Œæä¾›äº† [ä¸€ä¸ªéå¸¸å¯é å’Œä¼˜é›…çš„é›†ç¾¤å†…æœåŠ¡å‘ç°å’Œè´Ÿè½½å¹³è¡¡é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ](https://kubernetespodcast.com/episode/129-linkerd/) å¼€ç®±å³ç”¨ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæ‰€æä¾›çš„è§£å†³æ–¹æ¡ˆè¢«è¯æ˜å¾ˆå®¹æ˜“æ‰©å±•ï¼Œå¹¶å‚¬ç”Ÿäº†è¿™æ ·ä¸€ä¸ªæƒŠäººçš„è½¯ä»¶ï¼Œå¦‚ [Kubernetes åŸç”ŸæœåŠ¡ç½‘æ ¼](https://github.com/linkerd/linkerd2)ã€‚

_Disclaimer: This article intentionally omits the questions of external service (Service type `ExternalName`) discovering and discovering of the Kubernetes services from the outside world (Ingress Controller). These two deserve a dedicated article each._

_å…è´£å£°æ˜ï¼šæœ¬æ–‡æœ‰æ„çœç•¥äº†å¤–éƒ¨æœåŠ¡ï¼ˆService type `ExternalName`ï¼‰å‘ç°å’Œä»å¤–ç•Œï¼ˆIngress Controllerï¼‰å‘ç°KubernetesæœåŠ¡çš„é—®é¢˜ã€‚è¿™ä¸¤ä¸ªäººéƒ½åº”è¯¥æœ‰ä¸€ç¯‡ä¸“é—¨çš„æ–‡ç« ã€‚_

### Further reading

### è¿›ä¸€æ­¥é˜…è¯»

- [Pattern: Server-side service discovery](https://microservices.io/patterns/server-side-discovery.html)
- [Pattern: Client-side service discovery](https://microservices.io/patterns/client-side-discovery.html)
- [Pattern: Service registry](https://microservices.io/patterns/service-registry.html)
- [Service Discovery in a Microservices Architecture](https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/)
- [Microservices: Client Side Load Balancing](https://www.linkedin.com/pulse/microservices-client-side-load-balancing-amit-kumar-sharma/)
- [Kubernetes Podcast from Google: Linkerd, with Thomas Rampelberg](https://kubernetespodcast.com/episode/129-linkerd/) 

- [æ¨¡å¼ï¼šæœåŠ¡ç«¯æœåŠ¡å‘ç°](https://microservices.io/patterns/server-side-discovery.html)
- [æ¨¡å¼ï¼šå®¢æˆ·ç«¯æœåŠ¡å‘ç°](https://microservices.io/patterns/client-side-discovery.html)
- [æ¨¡å¼ï¼šæœåŠ¡æ³¨å†Œ](https://microservices.io/patterns/service-registry.html)
- [å¾®æœåŠ¡æ¶æ„ä¸­çš„æœåŠ¡å‘ç°](https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/)
- [å¾®æœåŠ¡ï¼šå®¢æˆ·ç«¯è´Ÿè½½å‡è¡¡](https://www.linkedin.com/pulse/microservices-client-side-load-balancing-amit-kumar-sharma/)
- [æ¥è‡ª Google çš„ Kubernetes æ’­å®¢ï¼šLinkerdï¼Œä¸ Thomas Rampelberg](https://kubernetespodcast.com/episode/129-linkerd/)

- [Baker Street: Avoiding Bottlenecks with a Client-Side Load Balancer for Microservices](https://thenewstack.io/baker-street-avoiding-bottlenecks-with-a-client-side-load-balancer-for-microservices/)

- [è´å…‹è¡—ï¼šä½¿ç”¨å¾®æœåŠ¡çš„å®¢æˆ·ç«¯è´Ÿè½½å‡è¡¡å™¨é¿å…ç“¶é¢ˆ](https://thenewstack.io/baker-street-avoiding-bottlenecks-with-a-client-side-load-balancer-for-microservices/)

### Other posts you may like

### æ‚¨å¯èƒ½å–œæ¬¢çš„å…¶ä»–å¸–å­

- [Service proxy, pod, sidecar, oh my!](http://iximiuz.com/en/posts/service-proxy-pod-sidecar-oh-my/)
- [Exploring Kubernetes Operator Pattern](http://iximiuz.com/en/posts/kubernetes-operator-pattern/)

- [æœåŠ¡ä»£ç†, pod, sidecar, oh my!](http://iximiuz.com/en/posts/service-proxy-pod-sidecar-oh-my/)
- [æ¢ç´¢Kubernetes Operatoræ¨¡å¼](http://iximiuz.com/en/posts/kubernetes-operator-pattern/)

[kubernetes,](javascript: void 0) [service-discovery](javascript: void 0)

[kubernetes,](javascript: void 0) [service-discovery](javascript: void 0)

#### Written by Ivan Velichko

#### ç”±ä¼Šä¸‡Â·ç»´åˆ©å¥‡ç§‘ (Ivan Velichko) æ’°å†™

_Follow me on twitter [@iximiuz](https://twitter.com/iximiuz)_

_åœ¨æ¨ç‰¹ä¸Šå…³æ³¨æˆ‘ [@iximiuz](https://twitter.com/iximiuz)_

Liked this article? Let it be the beginning of a great friendship. Leave your email so I could notify you about new articles or any other interesting happenings around the topics of this blog. No spam whatsoever, I promise!

å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿè®©å®ƒæˆä¸ºä¸€æ®µä¼Ÿå¤§å‹è°Šçš„å¼€å§‹ã€‚ç•™ä¸‹æ‚¨çš„ç”µå­é‚®ä»¶ï¼Œä»¥ä¾¿æˆ‘å¯ä»¥é€šçŸ¥æ‚¨æœ‰å…³æ­¤åšå®¢ä¸»é¢˜çš„æ–°æ–‡ç« æˆ–ä»»ä½•å…¶ä»–æœ‰è¶£çš„äº‹ä»¶ã€‚æ²¡æœ‰ä»»ä½•åƒåœ¾é‚®ä»¶ï¼Œæˆ‘ä¿è¯ï¼

Copyright Ivan Velichko Â© 2021Â FeedÂ [Atom](http://iximiuz.com/feed.atom)[RSS](http://iximiuz.com/feed.rss) 

ç‰ˆæƒæ‰€æœ‰ Ivan Velichko Â© 2021 Feed [Atom](http://iximiuz.com/feed.atom)[RSS](http://iximiuz.com/feed.rss)

