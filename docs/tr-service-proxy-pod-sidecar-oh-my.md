# Sidecar Proxy Pattern - The Basis Of Service Mesh

# Sidecar ä»£ç†æ¨¡å¼â€”â€”æœåŠ¡ç½‘æ ¼çš„åŸºç¡€

## Service proxy, pod, sidecar, oh my!

## æœåŠ¡ä»£ç†ã€podã€sidecarï¼Œå“¦ï¼Œå¤©å“ªï¼

September 6, 2020 (Updated: August 7, 2021)

## How services talk to each other?

## æœåŠ¡å¦‚ä½•ç›¸äº’é€šä¿¡ï¼Ÿ

Imagine you're developing a service... For certainty, let's call it _A_. It's going to provide some public HTTP API to its clients. However, to serve requests it needs to call another service. Let's call this upstream service - _B_.

æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æ­£åœ¨å¼€å‘ä¸€é¡¹æœåŠ¡â€¦â€¦å¯ä»¥è‚¯å®šçš„æ˜¯ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸º _A_ã€‚å®ƒå°†ä¸ºå…¶å®¢æˆ·ç«¯æä¾›ä¸€äº›å…¬å…± HTTP APIã€‚ç„¶è€Œï¼Œä¸ºäº†æœåŠ¡è¯·æ±‚ï¼Œå®ƒéœ€è¦è°ƒç”¨å¦ä¸€ä¸ªæœåŠ¡ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªä¸Šæ¸¸æœåŠ¡ä¸º - _B_ã€‚

![Service A talks to Service B directly.](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/10-service-a-service-b.png)

Obviously, neither network nor service _B_ is ideal. If service _A_ wants to decrease the impact of the failing upstream requests on its public API success rate, it has to do something about errors. For instance, it could start retrying failed requests.

æ˜¾ç„¶ï¼Œç½‘ç»œå’ŒæœåŠ¡ _B_ éƒ½ä¸æ˜¯ç†æƒ³çš„ã€‚å¦‚æœæœåŠ¡ _A_ æƒ³è¦å‡å°‘å¤±è´¥çš„ä¸Šæ¸¸è¯·æ±‚å¯¹å…¶å…¬å…± API æˆåŠŸç‡çš„å½±å“ï¼Œå®ƒå¿…é¡»å¯¹é”™è¯¯åšä¸€äº›äº‹æƒ…ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥å¼€å§‹é‡è¯•å¤±è´¥çš„è¯·æ±‚ã€‚

![Service A retries failed requests Service B.](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/20-service-a-service-b-with-retries.png)

Implementation of the retry mechanism requires some code changes in the service _A_, but the codebase is fresh, there are tons of advanced HTTP libraries, so you just need to grab one... Easy-peasy, right?

é‡è¯•æœºåˆ¶çš„å®ç°éœ€è¦åœ¨æœåŠ¡ _A_ ä¸­è¿›è¡Œä¸€äº›ä»£ç æ›´æ”¹ï¼Œä½†æ˜¯ä»£ç åº“æ˜¯æ–°é²œçš„ï¼Œæœ‰å¤§é‡é«˜çº§ HTTP åº“ï¼Œæ‰€ä»¥ä½ åªéœ€è¦æŠ“ä½ä¸€ä¸ª......è½»æ¾ï¼Œå¯¹å§ï¼Ÿ

Unfortunately, this simplicity is not always the case. Replace service _A_ with service _Z_ that was written 10 years ago in some esoteric language by a developer that already retired. Or add to the equitation services _Q_, _U_, and _X_ written by different teams in three different languages. As a result, the cumulative cost of the company-wide retry mechanism implementation in the code gets really high...

ä¸å¹¸çš„æ˜¯ï¼Œè¿™ç§ç®€å•æ€§å¹¶éæ€»æ˜¯å¦‚æ­¤ã€‚å°†æœåŠ¡ _A_ æ›¿æ¢ä¸ºæœåŠ¡ _Z_ï¼Œè¯¥æœåŠ¡æ˜¯ 10 å¹´å‰ç”±å·²é€€ä¼‘çš„å¼€å‘äººå‘˜ç”¨æŸç§æ·±å¥¥çš„è¯­è¨€ç¼–å†™çš„ã€‚æˆ–è€…æ·»åŠ ç”±ä¸åŒå›¢é˜Ÿä»¥ä¸‰ç§ä¸åŒè¯­è¨€ç¼–å†™çš„é©¬æœ¯æœåŠ¡ _Q_ã€_U_ å’Œ _X_ã€‚ç»“æœï¼Œä»£ç ä¸­å…¨å…¬å¸é‡è¯•æœºåˆ¶å®ç°çš„ç´¯ç§¯æˆæœ¬å˜å¾—éå¸¸é«˜......

![Service Mesh example](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/30-service-qux-service-b.png)

But what if retries are not the only thing you need? Proper _request timeouts_ have to be ensured as well. And how about _distributed tracing_? It'd be nice to correlate the whole request tree with the original customer transaction by propagating some additional HTTP headers. However, every such capability would make the HTTP libraries even more bloated...

ä½†æ˜¯ï¼Œå¦‚æœé‡è¯•ä¸æ˜¯æ‚¨å”¯ä¸€éœ€è¦çš„å‘¢ï¼Ÿè¿˜å¿…é¡»ç¡®ä¿é€‚å½“çš„_è¯·æ±‚è¶…æ—¶_ã€‚é‚£ä¹ˆ_åˆ†å¸ƒå¼è·Ÿè¸ª_å‘¢ï¼Ÿé€šè¿‡ä¼ æ’­ä¸€äº›é¢å¤–çš„ HTTP æ ‡å¤´ï¼Œå°†æ•´ä¸ªè¯·æ±‚æ ‘ä¸åŸå§‹å®¢æˆ·äº‹åŠ¡ç›¸å…³è”ä¼šå¾ˆå¥½ã€‚ç„¶è€Œï¼Œæ¯ä¸€ä¸ªè¿™æ ·çš„åŠŸèƒ½éƒ½ä¼šä½¿ HTTP åº“æ›´åŠ è‡ƒè‚¿......

## What is a sidecar proxy?

## ä»€ä¹ˆæ˜¯è¾¹è½¦ä»£ç†ï¼Ÿ

Let's try to go one level higher... or lower? ğŸ¤”

è®©æˆ‘ä»¬å°è¯•æé«˜ä¸€çº§â€¦â€¦æˆ–æ›´ä½ï¼Ÿ ğŸ¤”

In our original setup, service _A_ has been communicating with service _B_ directly. But what if we put an intermediary infrastructure component in between those services? Thanks to containerization, orchestration, devops, add a buzz word of your choice here, nowadays, it became so simple to configure infrastructure, that the cost of adding another infra component is often lower than the cost of writing application code...

åœ¨æˆ‘ä»¬æœ€åˆçš„è®¾ç½®ä¸­ï¼ŒæœåŠ¡ _A_ ä¸€ç›´ä¸æœåŠ¡ _B_ ç›´æ¥é€šä¿¡ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬åœ¨è¿™äº›æœåŠ¡ä¹‹é—´æ”¾ç½®ä¸€ä¸ªä¸­é—´åŸºç¡€è®¾æ–½ç»„ä»¶å‘¢ï¼Ÿæ„Ÿè°¢å®¹å™¨åŒ–ã€ç¼–æ’ã€devopsï¼Œåœ¨è¿™é‡Œæ·»åŠ æ‚¨é€‰æ‹©çš„æµè¡Œè¯­ï¼Œå¦‚ä»Šï¼Œé…ç½®åŸºç¡€è®¾æ–½å˜å¾—å¦‚æ­¤ç®€å•ï¼Œæ·»åŠ å¦ä¸€ä¸ªåŸºç¡€è®¾æ–½ç»„ä»¶çš„æˆæœ¬é€šå¸¸ä½äºç¼–å†™åº”ç”¨ç¨‹åºä»£ç çš„æˆæœ¬â€¦â€¦

![Sidecar Proxy Pattern visualized](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/40-service-a-sidecar-service-b.png)

For the sake of simplicity, let's call the box enclosing the service _A_ and the secret intermediary component _a server_ (bare metal or virtual, doesn't really matter). And now it's about time to introduce one of the fancy words from the article's title. Any piece of software running on the server _alongside_ the primary service and helping it do its job is called _a sidecar_. I hope, the idea behind the name is more or less straightforward here.

ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†åŒ…å«æœåŠ¡ _A_ å’Œç§˜å¯†ä¸­ä»‹ç»„ä»¶ _a server_ çš„ç›’å­ç§°ä¸ºï¼ˆè£¸æœºæˆ–è™šæ‹Ÿï¼Œå¹¶ä¸é‡è¦ï¼‰ã€‚ç°åœ¨æ˜¯æ—¶å€™ä»‹ç»æ–‡ç« æ ‡é¢˜ä¸­çš„ä¸€ä¸ªèŠ±å“¨è¯äº†ã€‚ä»»ä½•åœ¨æœåŠ¡å™¨ä¸Š_ä¸ä¸»è¦æœåŠ¡ä¸€èµ·è¿è¡Œå¹¶å¸®åŠ©å…¶å®Œæˆå·¥ä½œçš„è½¯ä»¶éƒ½ç§°ä¸º_a sidecar_ã€‚æˆ‘å¸Œæœ›ï¼Œè¿™ä¸ªåå­—èƒŒåçš„æƒ³æ³•åœ¨è¿™é‡Œæˆ–å¤šæˆ–å°‘æ˜¯ç›´æˆªäº†å½“çš„ã€‚

But getting back to the service-to-service communication problem, what sidecar should we use to keep the service code free of the low-level details such as retries or request tracing? Well, the needed piece of software is called a _service proxy_. Probably, the most widely used implementation of the service proxy in the real world is [envoy](https://www.envoyproxy.io/). 

ä½†æ˜¯å›åˆ°æœåŠ¡åˆ°æœåŠ¡çš„é€šä¿¡é—®é¢˜ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨ä»€ä¹ˆ sidecar æ¥ä¿æŒæœåŠ¡ä»£ç ä¸å—é‡è¯•æˆ–è¯·æ±‚è·Ÿè¸ªç­‰ä½çº§ç»†èŠ‚çš„å½±å“ï¼Ÿå¥½å§ï¼Œæ‰€éœ€çš„è½¯ä»¶ç§°ä¸º_æœåŠ¡ä»£ç†_ã€‚å¯èƒ½ï¼Œç°å®ä¸–ç•Œä¸­ä½¿ç”¨æœ€å¹¿æ³›çš„æœåŠ¡ä»£ç†å®ç°æ˜¯[envoy](https://www.envoyproxy.io/)ã€‚

The idea of the service proxy is the following: instead of accessing the service _B_ directly, code in the service _A_ now will be sending requests to the service proxy sidecar. Since both of the processes run on the same server, the loopback network interface (i.e. `127.0.0.1` _aka_ `localhost`) is perfectly suitable for this part of the communication. On every received HTTP request, the service proxy sidecar will make a request to the upstream service using the external network interface of the server. The response from the upstream will be eventually forwarded back by the sidecar to the service _A_.

æœåŠ¡ä»£ç†çš„æƒ³æ³•å¦‚ä¸‹ï¼šæœåŠ¡_A_ä¸­çš„ä»£ç ç°åœ¨å°†å‘æœåŠ¡ä»£ç†sidecarå‘é€è¯·æ±‚ï¼Œè€Œä¸æ˜¯ç›´æ¥è®¿é—®æœåŠ¡_B_ã€‚ç”±äºä¸¤ä¸ªè¿›ç¨‹è¿è¡Œåœ¨åŒä¸€å°æœåŠ¡å™¨ä¸Šï¼Œç¯å›ç½‘ç»œæ¥å£ï¼ˆå³`127.0.0.1`_aka_`localhost`ï¼‰éå¸¸é€‚åˆè¿™éƒ¨åˆ†é€šä¿¡ã€‚åœ¨æ¯ä¸ªæ”¶åˆ°çš„ HTTP è¯·æ±‚ä¸Šï¼ŒæœåŠ¡ä»£ç† sidecar å°†ä½¿ç”¨æœåŠ¡å™¨çš„å¤–éƒ¨ç½‘ç»œæ¥å£å‘ä¸Šæ¸¸æœåŠ¡å‘å‡ºè¯·æ±‚ã€‚æ¥è‡ªä¸Šæ¸¸çš„å“åº”æœ€ç»ˆä¼šè¢« sidecar è½¬å‘å›æœåŠ¡ _A_ã€‚

I think, at this time, it's already obvious where the retry, timeouts, tracing, etc. logic should reside. Having this kind of functionality provided by a separate sidecar process makes enhancing any service written in any language with such capabilities rather trivial.

æˆ‘è®¤ä¸ºï¼Œæ­¤æ—¶ï¼Œé‡è¯•ã€è¶…æ—¶ã€è·Ÿè¸ªç­‰é€»è¾‘åº”è¯¥é©»ç•™åœ¨ä½•å¤„å·²ç»å¾ˆæ˜æ˜¾äº†ã€‚æ‹¥æœ‰ç”±å•ç‹¬çš„ sidecar è¿›ç¨‹æä¾›çš„è¿™ç§åŠŸèƒ½ä½¿å¾—å¢å¼ºä»¥ä»»ä½•è¯­è¨€ç¼–å†™çš„å…·æœ‰æ­¤ç±»åŠŸèƒ½çš„ä»»ä½•æœåŠ¡å˜å¾—ç›¸å½“ç®€å•ã€‚

Interestingly enough, that service proxy could be used not only for outgoing traffic (egress) but also for the incoming traffic (ingress) of the service _A_. Usually, there is plenty of cross-cutting things that can be tackled on the ingress stage. For instance, proxy sidecars can do _SSL_ termination, request authentication, and more. A detailed diagram of a single server setup could look something like that:

æœ‰è¶£çš„æ˜¯ï¼Œè¯¥æœåŠ¡ä»£ç†ä¸ä»…å¯ä»¥ç”¨äºä¼ å‡ºæµé‡ï¼ˆå‡ºå£ï¼‰ï¼Œè¿˜å¯ä»¥ç”¨äºæœåŠ¡ _A_ çš„ä¼ å…¥æµé‡ï¼ˆå…¥å£ï¼‰ã€‚é€šå¸¸ï¼Œæœ‰å¾ˆå¤šè·¨é¢†åŸŸçš„äº‹æƒ…å¯ä»¥åœ¨å…¥å£é˜¶æ®µè§£å†³ã€‚ä¾‹å¦‚ï¼Œä»£ç† sidecar å¯ä»¥æ‰§è¡Œ _SSL_ ç»ˆæ­¢ã€è¯·æ±‚èº«ä»½éªŒè¯ç­‰ã€‚å•ä¸ªæœåŠ¡å™¨è®¾ç½®çš„è¯¦ç»†å›¾è¡¨å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š

![Local service proxy intercepting ingress and egress traffic](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/50-single-host-sidecar.png)

Probably, the last fancy term we are going to cover here is _a pod_. People have been deploying code using virtual machines or bare metal servers for a long time... A server itself is already a good abstraction and a unit of encapsulation. For instance, every server has at least one external network interface, a network loopback interface for the internal [IPC](https://en.wikipedia.org/wiki/Inter-process_communication) needs, and it can run a bunch of processes sharing access to these communication means. Servers are usually addressable within the private network of the company by their IPs. Last but not least, it's pretty common to use a whole server for a single purpose (otherwise, maintenance quickly becomes a nightmare). I.e. you may have a group of identical servers running instances of service _A_, another group of servers each running an instance of service _B_, etc. So, why on earth would anybody want something better than a server? 

å¯èƒ½ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œä»‹ç»çš„æœ€åä¸€ä¸ªèŠ±å“¨æœ¯è¯­æ˜¯ _a pod_ã€‚é•¿æœŸä»¥æ¥ï¼Œäººä»¬ä¸€ç›´åœ¨ä½¿ç”¨è™šæ‹Ÿæœºæˆ–è£¸æœºæœåŠ¡å™¨æ¥éƒ¨ç½²ä»£ç â€¦â€¦æœåŠ¡å™¨æœ¬èº«å·²ç»æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ½è±¡å’Œå°è£…å•å…ƒã€‚ä¾‹å¦‚ï¼Œæ¯å°æœåŠ¡å™¨è‡³å°‘æœ‰ä¸€ä¸ªå¤–éƒ¨ç½‘ç»œæ¥å£ï¼Œä¸€ä¸ªç”¨äºå†…éƒ¨ [IPC](https://en.wikipedia.org/wiki/Inter-process_communication) éœ€æ±‚çš„ç½‘ç»œç¯å›æ¥å£ï¼Œå®ƒå¯ä»¥è¿è¡Œä¸€å †è¿›ç¨‹å…±äº«å¯¹è¿™äº›é€šä¿¡æ‰‹æ®µçš„è®¿é—®ã€‚æœåŠ¡å™¨é€šå¸¸å¯é€šè¿‡å…¶ IP åœ¨å…¬å¸çš„ä¸“ç”¨ç½‘ç»œå†…è¿›è¡Œå¯»å€ã€‚æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œå°†æ•´ä¸ªæœåŠ¡å™¨ç”¨äºå•ä¸€ç›®çš„æ˜¯å¾ˆå¸¸è§çš„ï¼ˆå¦åˆ™ï¼Œç»´æŠ¤å¾ˆå¿«å°±ä¼šå˜æˆä¸€åœºå™©æ¢¦)ã€‚ IEã€‚æ‚¨å¯èƒ½æœ‰ä¸€ç»„ç›¸åŒçš„æœåŠ¡å™¨è¿è¡ŒæœåŠ¡ _A_ çš„å®ä¾‹ï¼Œå¦ä¸€ç»„æœåŠ¡å™¨æ¯ä¸ªè¿è¡Œä¸€ä¸ªæœåŠ¡ _B_ çš„å®ä¾‹ï¼Œç­‰ç­‰ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆæœ‰äººæƒ³è¦æ¯”æœåŠ¡å™¨æ›´å¥½çš„ä¸œè¥¿å‘¢ï¼Ÿ

Despite being a good abstraction, the orchestration overhead servers introduce is often too high. So people started thinking about how to package applications more efficiently and that's how we got containers. Well, probably you know that _Docker_ and _container_ had been kind of a synonym for a long time and folks from Docker have been actively advocating for _"a one process per container"_ model. Obviously, this model is pretty different from the widely used _server_ abstraction where multiple processes are allowed to work side by side. And that's how we got the concept of _pods_. A pod is just a group of containers sharing a bunch of namespaces. If we now run a single process per container all of the processes in the pod will still share the common execution environment. In particular, the network namespace. Thus, all the containers in the pod will have a shared loopback interface and a shared external interface with an IP address assigned to it. Then it's up to the orchestration layer (say hi to Kubernetes) how to make all the pods reachable within the network by their IPs. And that's how people reinvented servers...

å°½ç®¡æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æŠ½è±¡ï¼Œä½†æœåŠ¡å™¨å¼•å…¥çš„ç¼–æ’å¼€é”€é€šå¸¸å¤ªé«˜ã€‚æ‰€ä»¥äººä»¬å¼€å§‹è€ƒè™‘å¦‚ä½•æ›´æœ‰æ•ˆåœ°æ‰“åŒ…åº”ç”¨ç¨‹åºï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è·å¾—å®¹å™¨çš„æ–¹å¼ã€‚å¥½å§ï¼Œæ‚¨å¯èƒ½çŸ¥é“ _Docker_ å’Œ _container_ é•¿æœŸä»¥æ¥ä¸€ç›´æ˜¯åŒä¹‰è¯ï¼ŒDocker çš„äººä»¬ä¸€ç›´åœ¨ç§¯æå€¡å¯¼_â€œæ¯ä¸ªå®¹å™¨ä¸€ä¸ªè¿›ç¨‹â€_ æ¨¡å‹ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªæ¨¡å‹ä¸å¹¿æ³›ä½¿ç”¨çš„ _server_ æŠ½è±¡éå¸¸ä¸åŒï¼Œåè€…å…è®¸å¤šä¸ªè¿›ç¨‹å¹¶æ’å·¥ä½œã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•å¾—åˆ° _pods_ çš„æ¦‚å¿µã€‚ Pod åªæ˜¯ä¸€ç»„å…±äº«ä¸€å †å‘½åç©ºé—´çš„å®¹å™¨ã€‚å¦‚æœæˆ‘ä»¬ç°åœ¨ä¸ºæ¯ä¸ªå®¹å™¨è¿è¡Œä¸€ä¸ªè¿›ç¨‹ï¼Œé‚£ä¹ˆ pod ä¸­çš„æ‰€æœ‰è¿›ç¨‹ä»å°†å…±äº«å…¬å…±æ‰§è¡Œç¯å¢ƒã€‚ç‰¹åˆ«æ˜¯ç½‘ç»œå‘½åç©ºé—´ã€‚å› æ­¤ï¼ŒPod ä¸­çš„æ‰€æœ‰å®¹å™¨éƒ½å°†æ‹¥æœ‰ä¸€ä¸ªå…±äº«çš„ç¯å›æ¥å£å’Œä¸€ä¸ªå…±äº«çš„å¤–éƒ¨æ¥å£ï¼Œå¹¶ä¸ºå…¶åˆ†é…äº† IP åœ°å€ã€‚ç„¶åç”±ç¼–æ’å±‚ï¼ˆè·Ÿ Kubernetes æ‰“ä¸ªæ‹›å‘¼ï¼‰å¦‚ä½•è®©æ‰€æœ‰ pod åœ¨ç½‘ç»œå†…é€šè¿‡å®ƒä»¬çš„ IP è®¿é—®ã€‚è¿™å°±æ˜¯äººä»¬å¦‚ä½•æ”¹é€ æœåŠ¡å™¨......

So, getting back to all those blue boxes enclosing the service process and the sidecar on the diagrams above - we can think of them as being either a virtual machine, a bare metal server, or a pod. All three of them are more or less interchangeable abstractions.

å› æ­¤ï¼Œå›åˆ°ä¸Šå›¾ä¸­åŒ…å«æœåŠ¡æµç¨‹å’Œè¾¹è½¦çš„æ‰€æœ‰è“è‰²æ¡† - æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è§†ä¸ºè™šæ‹Ÿæœºã€è£¸æœºæœåŠ¡å™¨æˆ– Podã€‚è¿™ä¸‰ä¸ªæˆ–å¤šæˆ–å°‘éƒ½æ˜¯å¯äº’æ¢çš„æŠ½è±¡ã€‚

To summarize, let's try to visualize how the service to service communication could look like with the proxy sidecars:

æ€»è€Œè¨€ä¹‹ï¼Œè®©æˆ‘ä»¬å°è¯•å¯è§†åŒ–ä½¿ç”¨ä»£ç†è¾¹è½¦çš„æœåŠ¡åˆ°æœåŠ¡é€šä¿¡çš„æ ·å­ï¼š

![Mesh of services talking to each other through sidecar proxies](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/60-service-to-service-topology.png)

_Example of service to service communication topology, a.k.a. service mesh._

_æœåŠ¡åˆ°æœåŠ¡é€šä¿¡æ‹“æ‰‘çš„ç¤ºä¾‹ï¼Œä¹Ÿå°±æ˜¯æœåŠ¡ç½‘æ ¼ã€‚_

## Sidecar proxy example (practical part)

## Sidecar ä»£ç†ç¤ºä¾‹ï¼ˆå®æˆ˜éƒ¨åˆ†ï¼‰

Since the only way to really understand something is to write a blog post about it implement it yourself, let's quickly hack a [demo environment](https://github.com/iximiuz/envoy-playground).

ç”±äºçœŸæ­£ç†è§£æŸäº‹çš„å”¯ä¸€æ–¹æ³•æ˜¯å†™ä¸€ç¯‡å…³äºå®ƒè‡ªå·±å®ç°å®ƒçš„åšå®¢æ–‡ç« ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿç ´è§£ä¸€ä¸ª[æ¼”ç¤ºç¯å¢ƒ](https://github.com/iximiuz/envoy-playground)ã€‚

#### Service A talks to service B directly

#### æœåŠ¡ A ç›´æ¥ä¸æœåŠ¡ B å¯¹è¯

We will start from the simple setup where service _A_ will be accessing service _B_ directly:

æˆ‘ä»¬å°†ä»ç®€å•çš„è®¾ç½®å¼€å§‹ï¼Œå…¶ä¸­æœåŠ¡ _A_ å°†ç›´æ¥è®¿é—®æœåŠ¡ _B_ï¼š

![Multi-service demo setup](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/70-demo-direct.png)

The code of the [service _A_](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a) is relatively straightforward. It's just a simple HTTP server that makes a call to its upstream service _B_ on every client request. Depending on the response from the upstream, _A_ returns either an HTTP 200 or HTTP 500 to the client.

[service_A_](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a)çš„ä»£ç æ¯”è¾ƒç®€å•ã€‚å®ƒåªæ˜¯ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œå®ƒåœ¨æ¯ä¸ªå®¢æˆ·ç«¯è¯·æ±‚ä¸Šè°ƒç”¨å…¶ä¸Šæ¸¸æœåŠ¡ _B_ã€‚æ ¹æ®ä¸Šæ¸¸çš„å“åº”ï¼Œ_A_ å‘å®¢æˆ·ç«¯è¿”å› HTTP 200 æˆ– HTTP 500ã€‚

```go
package main

// ...

var requestCounter = prometheus.NewCounterVec(
    prometheus.CounterOpts{
        Name: "service_a_requests_total",
        Help: "The total number of requests received by Service A.",
    },
    []string{"status"},
)

func handler(w http.ResponseWriter, r *http.Request) {
    resp, err := httpGet(os.Getenv("UPSTREAM_SERVICE"))
    if err == nil {
        fmt.Fprintln(w, "Service A: upstream responded with:", resp)
        requestCounter.WithLabelValues("2xx").Inc()
    } else {
        http.Error(w, fmt.Sprintf("Service A: upstream failed with: %v", err.Error()),
            http.StatusInternalServerError)
        requestCounter.WithLabelValues("5xx").Inc()
    }
}

func main() {
    // init prometheus /metrics endpoint

    http.HandleFunc("/", handler)
    log.Fatal(http.ListenAndServe(
        os.Getenv("SERVICE_HOST")+":"+os.Getenv("SERVICE_PORT"), nil))
}

```


_(see full version on [GitHub](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/main.go))_

_ï¼ˆè¯·å‚é˜… [GitHub] ä¸Šçš„å®Œæ•´ç‰ˆæœ¬ï¼ˆhttps://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/main.goï¼‰ï¼‰_

Notice that instead of hard-coding, we use `SERVICE_HOST` and `SERVICE_PORT` env variables to specify the host and port of the HTTP API endpoint. It'll come in handy soon. Additionally, the code of the service relies on the `UPSTREAM_SERVICE` env variable when accessing the upstream service _B_.

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨ `SERVICE_HOST` å’Œ `SERVICE_PORT` ç¯å¢ƒå˜é‡æ¥æŒ‡å®š HTTP API ç«¯ç‚¹çš„ä¸»æœºå’Œç«¯å£ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç ã€‚å¾ˆå¿«å°±ä¼šæ´¾ä¸Šç”¨åœºã€‚æ­¤å¤–ï¼ŒæœåŠ¡çš„ä»£ç åœ¨è®¿é—®ä¸Šæ¸¸æœåŠ¡ _B_ æ—¶ä¾èµ–äº `UPSTREAM_SERVICE` ç¯å¢ƒå˜é‡ã€‚

To get some visibility, the code is instrumented with the primitive counter metric `service_a_requests_total` that gets incremented on every incoming request. We will use an instance of [prometheus](https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus) service to scrape the metrics exposed by the service _A_. 

ä¸ºäº†è·å¾—ä¸€äº›å¯è§æ€§ï¼Œä»£ç ä½¿ç”¨åŸå§‹è®¡æ•°å™¨æŒ‡æ ‡â€œservice_a_requests_totalâ€è¿›è¡Œæ£€æµ‹ï¼Œè¯¥æŒ‡æ ‡åœ¨æ¯ä¸ªä¼ å…¥è¯·æ±‚æ—¶é€’å¢ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [prometheus](https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus) æœåŠ¡çš„å®ä¾‹æ¥æŠ“å–æœåŠ¡ _A_ å…¬å¼€çš„æŒ‡æ ‡ã€‚

The implementation of the upstream [service _B_](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b) is trivial as well. It's yet another HTTP server. However its behavior is rather close to a static endpoint.

ä¸Šæ¸¸ [service _B_](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b) çš„å®ç°ä¹Ÿå¾ˆç®€å•ã€‚å®ƒæ˜¯å¦ä¸€ä¸ª HTTP æœåŠ¡å™¨ã€‚ç„¶è€Œï¼Œå®ƒçš„è¡Œä¸ºä¸é™æ€ç«¯ç‚¹ç›¸å½“æ¥è¿‘ã€‚

```go
package main

// ...

var ERROR_RATE int

var (
    requestCounter = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "service_b_requests_total",
            Help: "The total number of requests received by Service B.",
        },
        []string{"status"},
    )
)

func handler(w http.ResponseWriter, r *http.Request) {
    if rand.Intn(100) >= ERROR_RATE {
        fmt.Fprintln(w, "Service B: Yay! nounce", rand.Uint32())
        requestCounter.WithLabelValues("2xx").Inc()
    } else {
        http.Error(w, fmt.Sprintf("Service B: Ooops... nounce %v", rand.Uint32()),
            http.StatusInternalServerError)
        requestCounter.WithLabelValues("5xx").Inc()
    }
}

func main() {
    // set ERROR_RATE
    // init prometheus /metrics endpoint

    http.HandleFunc("/", handler)

    // Listen on all interfaces
    log.Fatal(http.ListenAndServe(":"+os.Getenv("SERVICE_PORT"), nil))
}

```


_(see full version on [GitHub](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/main.go))_

_ï¼ˆè¯·å‚é˜… [GitHub] ä¸Šçš„å®Œæ•´ç‰ˆæœ¬ï¼ˆhttps://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/main.goï¼‰ï¼‰_

Probably the only interesting part here is `ERROR_RATE`. The service is designed to fail requests with some constant rate, i.e. if `ERROR_RATE` is _20_, approximately 20% of requests will fail with HTTP 500 status code. As with the service _A_, we will use prometheus to scrape basic usage statistics, see the counter `service_b_requests_total`.

å¯èƒ½è¿™é‡Œå”¯ä¸€æœ‰è¶£çš„éƒ¨åˆ†æ˜¯â€œERROR_RATEâ€ã€‚è¯¥æœåŠ¡æ—¨åœ¨ä»¥æŸç§æ’å®šé€Ÿç‡ä½¿è¯·æ±‚å¤±è´¥ï¼Œå³å¦‚æœâ€œERROR_RATEâ€ä¸º _20_ï¼Œåˆ™å¤§çº¦ 20% çš„è¯·æ±‚å°†å¤±è´¥å¹¶æ˜¾ç¤º HTTP 500 çŠ¶æ€ä»£ç ã€‚ä¸æœåŠ¡ _A_ ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ prometheus æ¥æŠ“å–åŸºæœ¬ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å‚é˜…è®¡æ•°å™¨â€œservice_b_requests_totalâ€ã€‚

Now it's time to launch the services and wire them up together. We are going to use [podman](https://github.com/containers/podman) to build and run services. Mostly because unlike Docker, podman [supports the concept of pods out of the box](https://developers.redhat.com/blog/2019/01/15/podman-managing-containers-pods/). Heck, look at its name, it's **POD** man ğŸµ

ç°åœ¨æ˜¯å¯åŠ¨æœåŠ¡å¹¶å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·çš„æ—¶å€™äº†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [podman](https://github.com/containers/podman) æ¥æ„å»ºå’Œè¿è¡ŒæœåŠ¡ã€‚ä¸»è¦æ˜¯å› ä¸ºä¸ Docker ä¸åŒï¼Œpodman [æ”¯æŒå¼€ç®±å³ç”¨çš„ pod æ¦‚å¿µ](https://developers.redhat.com/blog/2019/01/15/podman-managing-containers-pods/)ã€‚å“å‘€ï¼Œçœ‹å®ƒçš„åå­—ï¼Œå®ƒæ˜¯**POD**äººğŸµ

We will start from creating the service _B_ since it's a dependency of the service _A_. Clone the [demo repository](https://github.com/iximiuz/envoy-playground) and run the following commands from its root (a Linux host with installed podman is assumed):

æˆ‘ä»¬å°†ä»åˆ›å»ºæœåŠ¡ _B_ å¼€å§‹ï¼Œå› ä¸ºå®ƒæ˜¯æœåŠ¡ _A_ çš„ä¾èµ–é¡¹ã€‚å…‹éš† [demo å­˜å‚¨åº“](https://github.com/iximiuz/envoy-playground) å¹¶ä»å…¶æ ¹ç›®å½•è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆå‡è®¾æ˜¯å®‰è£…äº† podman çš„ Linux ä¸»æœº)ï¼š

_Click here to see service B Dockerfile._

_ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹æœåŠ¡B Dockerfileã€‚_

```dockerfile
FROM golang:1.15

# Build
ENV GO111MODULE=on

WORKDIR /app

COPY go.mod .
COPY main.go .

RUN go mod download
RUN go build -o service-b

# Run
ENV ERROR_RATE=30
ENV SERVICE_PORT=80
ENV METRICS_PORT=8081
CMD ["/app/service-b"]
```


[source on GitHub](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/Dockerfile)

[GitHub ä¸Šçš„æ¥æº](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/Dockerfile)

```bash
# Build service B image
$ sudo podman build -t service-b -f service-b/Dockerfile

# Create a pod (read "server") for service B
$ sudo podman pod create --name service-b-pod

# Start service B container in the pod
$ sudo podman run --name service-b -d --rm --pod service-b-pod \
    -e ERROR_RATE=20 service-b

# Keep pod's IP address for future use
$ POD_B_IP=$(sudo podman inspect -f "{{.NetworkSettings.IPAddress}}" \
    $(sudo podman pod inspect -f "{{.InfraContainerID}}" service-b-pod))

$ echo $POD_B_IP
> 10.88.0.164  # output on my machine

```


Notice that the server is listening on pod's external network interface, port _80_:

è¯·æ³¨æ„ï¼ŒæœåŠ¡å™¨æ­£åœ¨ä¾¦å¬ pod çš„å¤–éƒ¨ç½‘ç»œæ¥å£ç«¯å£ _80_ï¼š

```bash
$ curl $POD_B_IP
> Service B: Yay!nounce 3494557023
$ curl $POD_B_IP
> Service B: Yay!nounce 1634910179
$ curl $POD_B_IP
> Service B: Yay!nounce 2013866549
$ curl $POD_B_IP
> Service B: Ooops... nounce 1258862891

```


Now we are ready to proceed with the service _A_. First, let's create a pod:

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½ç»§ç»­æœåŠ¡_A_ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ª podï¼š

```bash
# Create a pod (read "server") for service A
$ sudo podman pod create --name service-a-pod \
    --add-host b.service:$POD_B_IP --publish 8080:80

```


Notice how we injected a DNS record like `b.service 10.88.0.164`. Since both pods reside in the same podman network, they can reach each other using assigned IP addresses. However, as of the time of writing this, podman doesn't provide DNS support for pods (yet). So, we have to maintain the mappings manually. Of course, we could use the plain IP address of the _B_'s pod while accessing the upstream from the service _A_ code. However, it's always nice to have human-readable hostnames instead of raw IP addresses. We will also see how this technique comes in handy with the envoy proxy sidecar below.

æ³¨æ„æˆ‘ä»¬æ˜¯å¦‚ä½•æ³¨å…¥ä¸€ä¸ªåƒâ€œb.service 10.88.0.164â€è¿™æ ·çš„ DNS è®°å½•çš„ã€‚ç”±äºä¸¤ä¸ª Pod ä½äºåŒä¸€ä¸ª Podman ç½‘ç»œä¸­ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥ä½¿ç”¨åˆ†é…çš„ IP åœ°å€ç›¸äº’è®¿é—®ã€‚ä½†æ˜¯ï¼Œæˆªè‡³æ’°å†™æœ¬æ–‡æ—¶ï¼Œpodman å°šä¸ä¸º pod æä¾› DNS æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨ç»´æŠ¤æ˜ å°„ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»æœåŠ¡ _A_ ä»£ç è®¿é—®ä¸Šæ¸¸æ—¶ä½¿ç”¨ _B_ çš„ pod çš„æ™®é€š IP åœ°å€ã€‚ç„¶è€Œï¼Œæ‹¥æœ‰äººç±»å¯è¯»çš„ä¸»æœºåè€Œä¸æ˜¯åŸå§‹ IP åœ°å€æ€»æ˜¯å¥½çš„ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°è¿™ç§æŠ€æœ¯å¦‚ä½•åœ¨ä¸‹é¢çš„ç‰¹ä½¿ä»£ç†è¾¹è½¦ä¸­æ´¾ä¸Šç”¨åœºã€‚

Let's continue with the service itself. We need to build it and run inside the pod we've just created.

è®©æˆ‘ä»¬ç»§ç»­æœåŠ¡æœ¬èº«ã€‚æˆ‘ä»¬éœ€è¦æ„å»ºå®ƒå¹¶åœ¨æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„ pod ä¸­è¿è¡Œã€‚

_Click here to see service A Dockerfile._

_ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹æœåŠ¡A Dockerfileã€‚_

```dockerfile
FROM golang:1.15

# Build
ENV GO111MODULE=on

WORKDIR /app

COPY go.mod .
COPY main.go .

RUN go mod download
RUN go build -o service-a

# Run
ENV SERVICE_HOST="0.0.0.0"
ENV SERVICE_PORT=80

ENV METRICS_HOST="0.0.0.0"
ENV METRICS_PORT=8081

ENV UPSTREAM_SERVICE="http://b.service/"

CMD ["/app/service-a"]

```


[source on GitHub](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/Dockerfile-service)

[GitHub ä¸Šçš„æ¥æº](https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/Dockerfile-service)

```bash
# Build service A image
$ sudo podman build -t service-a -f service-a/Dockerfile-service

# Start service A container in the pod
$ sudo podman run --name service-a -d --rm --pod service-a-pod \
   -e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80 \
   -e UPSTREAM_SERVICE=http://b.service:80 \
service-a

# Keep pod's IP address for future use
$ POD_A_IP=$(sudo podman inspect -f "{{.NetworkSettings.IPAddress}}" \
    $(sudo podman pod inspect -f "{{.InfraContainerID}}" service-a-pod))

$ echo $POD_A_IP
> 10.88.0.165  # output on my machine

```


Remember the diagram from the beginning of this section. At this part of the exercise service _A_ has to be directly exposed to the outside world (i.e. the host machine) and it has to communicate with the service _B_ directly as well. That's why we made service _A_ listening on the pod's external network interface using `-e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80` and provided it with the knowledge how to reach the service _B_ `-e UPSTREAM_SERVICE=http://b.service:80`.

è®°ä½æœ¬èŠ‚å¼€å¤´çš„å›¾è¡¨ã€‚åœ¨ç»ƒä¹ çš„è¿™ä¸€éƒ¨åˆ†ï¼ŒæœåŠ¡ _A_ å¿…é¡»ç›´æ¥æš´éœ²ç»™å¤–éƒ¨ä¸–ç•Œï¼ˆå³ä¸»æœºï¼‰ï¼Œå¹¶ä¸”å®ƒä¹Ÿå¿…é¡»ç›´æ¥ä¸æœåŠ¡ _B_ é€šä¿¡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨ `-e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80` è®©æœåŠ¡ _A_ ç›‘å¬ pod çš„å¤–éƒ¨ç½‘ç»œæ¥å£ï¼Œå¹¶ä¸ºå®ƒæä¾›å¦‚ä½•è®¿é—®æœåŠ¡ _B_ `-e UPSTREAM_SERVICE=http://b çš„çŸ¥è¯†.æœåŠ¡ï¼š80`ã€‚

The last preparation before pouring some traffic - starting a [prometheus](https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus) node:

æ³¨å…¥æµé‡å‰çš„æœ€åå‡†å¤‡â€”â€”å¯åŠ¨ä¸€ä¸ª [prometheus](https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus) èŠ‚ç‚¹ï¼š

```bash
# Scrape configs
$ cat prometheus/prometheus.yml
> scrape_configs:
>   - job_name: service-a
>     scrape_interval: 1s
>     static_configs:
>       - targets: ['a.service:8081']
>
>   - job_name: service-b
>     scrape_interval: 1s
>     static_configs:
>       - targets: ['b.service:8081']

# Dockerfile
$ cat prometheus/Dockerfile
> FROM prom/prometheus:v2.20.1
> COPY prometheus.yml /etc/prometheus/prometheus.yml

# Build & run
$ sudo podman build -t envoy-prom -f prometheus/Dockerfile

$ sudo podman run --name envoy-prom -d --rm \
   --publish 9090:9090 \
   --add-host a.service:$POD_A_IP \
   --add-host b.service:$POD_B_IP \
envoy-prom

```


At this time, we have two services within a shared network. They can talk to each other using their IP addresses. Additionally, port _80_ of the service _A_ is mapped to the port _8080_ of the host machine and prometheus is exposed on the port _9090_. I intentionally made these two ports mapped on `0.0.0.0` 'cause I run the demo inside a VirtualBox machine. This way, I can access prometheus graphical interface from the laptop's host operating system via using `<vm_ip_address>:9090/graph`.

æ­¤æ—¶ï¼Œæˆ‘ä»¬åœ¨å…±äº«ç½‘ç»œä¸­æœ‰ä¸¤ä¸ªæœåŠ¡ã€‚ä»–ä»¬å¯ä»¥ä½¿ç”¨ä»–ä»¬çš„ IP åœ°å€ç›¸äº’äº¤è°ˆã€‚å¦å¤–ï¼ŒæœåŠ¡_A_çš„_80_ç«¯å£æ˜ å°„åˆ°ä¸»æœºçš„_8080_ç«¯å£ï¼Œprometheusæš´éœ²åœ¨_9090_ç«¯å£ã€‚æˆ‘æ•…æ„å°†è¿™ä¸¤ä¸ªç«¯å£æ˜ å°„åˆ°â€œ0.0.0.0â€ï¼Œå› ä¸ºæˆ‘åœ¨ VirtualBox æœºå™¨å†…è¿è¡Œæ¼”ç¤ºã€‚è¿™æ ·ï¼Œæˆ‘å°±å¯ä»¥é€šè¿‡ä½¿ç”¨ `<vm_ip_address>:9090/graph` ä»ç¬”è®°æœ¬ç”µè„‘çš„ä¸»æœºæ“ä½œç³»ç»Ÿè®¿é—® prometheus å›¾å½¢ç•Œé¢ã€‚

Finally, we can send some traffic to the service _A_ and see what happens:

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å‘æœåŠ¡ _A_ å‘é€ä¸€äº›æµé‡ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼š

```bash
$ for _ in {1..1000};do curl --silent localhost:8080;done |sort |uniq -w 24 -c
>    1000
>     208 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1007409508
>     792 Service A: upstream responded with: Service B: Yay!nounce 1008262846

```


Yay! ğŸ‰ As expected, ca. 20% of the upstream requests failed with the HTTP 500 status code. Let's take a look at the prometheus metrics to see the per-service statistics:

å¥½æäº†ï¼ ğŸ‰æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¤§çº¦ã€‚ 20% çš„ä¸Šæ¸¸è¯·æ±‚å¤±è´¥å¹¶æ˜¾ç¤º HTTP 500 çŠ¶æ€ä»£ç ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ prometheus æŒ‡æ ‡ä»¥æŸ¥çœ‹æ¯ä¸ªæœåŠ¡çš„ç»Ÿè®¡ä¿¡æ¯ï¼š

![Service A - 20% of outgoing requests failed](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-service-a-direct.png)

_`service_a_requests_total`_

![Service B - 20% of incoming requests failed](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-service-b-direct.png)

_`service_b_requests_total`_ 

Well, I believe it's not a surprise that both services handled each 1000 of requests and the service _A_ failed as many requests as the service _B_.

å¥½å§ï¼Œæˆ‘ç›¸ä¿¡è¿™ä¸¤ä¸ªæœåŠ¡æ¯å¤„ç† 1000 ä¸ªè¯·æ±‚å¹¶ä¸”æœåŠ¡ _A_ å¤±è´¥çš„è¯·æ±‚ä¸æœåŠ¡ _B_ ä¸€æ ·å¤šï¼Œè¿™å¹¶ä¸å¥‡æ€ªã€‚

#### Service A talks to service B through envoy proxy sidecar

#### æœåŠ¡ A é€šè¿‡ç‰¹ä½¿ä»£ç† sidecar ä¸æœåŠ¡ B å¯¹è¯

Let's enhance our setup by adding a service proxy sidecar to the service _A_. For the sake of simplicity of this demo, the only thing the sidecar will be doing is making up to 2 retries of the failed HTTP requests. Hopefully, it'll improve the overall service _A_ success rate. The desired setup will look as follows:

è®©æˆ‘ä»¬é€šè¿‡å‘æœåŠ¡ _A_ æ·»åŠ æœåŠ¡ä»£ç†è¾¹è½¦æ¥å¢å¼ºæˆ‘ä»¬çš„è®¾ç½®ã€‚ä¸ºäº†è¿™ä¸ªæ¼”ç¤ºçš„ç®€å•èµ·è§ï¼Œsidecar å”¯ä¸€è¦åšçš„å°±æ˜¯å¯¹å¤±è´¥çš„ HTTP è¯·æ±‚è¿›è¡Œæœ€å¤š 2 æ¬¡é‡è¯•ã€‚å¸Œæœ›å®ƒä¼šæé«˜æ•´ä½“æœåŠ¡_A_æˆåŠŸç‡ã€‚æ‰€éœ€çš„è®¾ç½®å¦‚ä¸‹æ‰€ç¤ºï¼š

![Multi-service demo setup with sidecar proxy intercepting traffic](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/80-demo-sidecar.png)

The main difference is that with the sidecar all incoming and outgoing requests will be passing through envoy. In contrast with the previous section setup, service _A_ will neither be exposed publicly nor will be allowed to contact service _B_ directly.

ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä½¿ç”¨ sidecarï¼Œæ‰€æœ‰ä¼ å…¥å’Œä¼ å‡ºçš„è¯·æ±‚éƒ½å°†é€šè¿‡ Envoyã€‚ä¸ä¸Šä¸€èŠ‚è®¾ç½®ç›¸æ¯”ï¼ŒæœåŠ¡ _A_ æ—¢ä¸ä¼šå…¬å¼€ï¼Œä¹Ÿä¸å…è®¸ç›´æ¥è”ç³»æœåŠ¡ _B_ã€‚

Let's review two scenarios:

è®©æˆ‘ä»¬å›é¡¾ä¸¤ä¸ªåœºæ™¯ï¼š

- **Ingress**: a client sends an HTTP request to `$POD_A_IP`. It hits the envoy sidecar listening on `$POD_A_IP:80`. Envoy, in turn, makes a request to the service _A_ container listening on the pod's `localhost:8000`. Once the envoy process gets the response from the service _A_ container, it forwards it back to the client.

- **Ingress**ï¼šå®¢æˆ·ç«¯å‘ `$POD_A_IP` å‘é€ HTTP è¯·æ±‚ã€‚å®ƒå‡»ä¸­äº†ç›‘å¬ `$POD_A_IP:80` çš„ç‰¹ä½¿è¾¹è½¦ã€‚åè¿‡æ¥ï¼ŒEnvoy å‘æœåŠ¡ _A_ å®¹å™¨å‘å‡ºè¯·æ±‚ï¼Œè¯¥å®¹å™¨ä¾¦å¬ pod çš„â€œlocalhost:8000â€ã€‚ä¸€æ—¦ç‰¹ä½¿è¿›ç¨‹ä»æœåŠ¡ _A_ å®¹å™¨è·å¾—å“åº”ï¼Œå®ƒå°†å…¶è½¬å‘å›å®¢æˆ·ç«¯ã€‚

- **Egress**: service _A_ gets a request from envoy. To handle it, the service process needs to access the upstream service _B_. The service _A_ container sends a request to another envoy listener sitting on pod's `localhost:9001`. Additionally, service _A_ specifies the HTTP Host header `b.local.service` allowing envoy to route this request appropriately. When envoy receives a request on `localhost:9001` it knows that it's egress traffic. It checks the Host header and if it looks like the service _B_, it makes a request to `$POD_B_IP`.


- **Egress**ï¼šæœåŠ¡ _A_ ä» Envoy è·å–è¯·æ±‚ã€‚ä¸ºäº†å¤„ç†å®ƒï¼ŒæœåŠ¡è¿›ç¨‹éœ€è¦è®¿é—®ä¸Šæ¸¸æœåŠ¡_B_ã€‚æœåŠ¡ _A_ å®¹å™¨å‘ä½äº pod çš„â€œlocalhost:9001â€ä¸Šçš„å¦ä¸€ä¸ªç‰¹ä½¿ä¾¦å¬å™¨å‘é€è¯·æ±‚ã€‚æ­¤å¤–ï¼ŒæœåŠ¡ _A_ æŒ‡å®šäº† HTTP ä¸»æœºæ ‡å¤´ `b.local.service`ï¼Œå…è®¸ Envoy é€‚å½“åœ°è·¯ç”±æ­¤è¯·æ±‚ã€‚å½“ Envoy åœ¨ localhost:9001 ä¸Šæ”¶åˆ°è¯·æ±‚æ—¶ï¼Œå®ƒçŸ¥é“è¿™æ˜¯å‡ºå£æµé‡ã€‚å®ƒæ£€æŸ¥ä¸»æœºå¤´ï¼Œå¦‚æœå®ƒçœ‹èµ·æ¥åƒæœåŠ¡ _B_ï¼Œå®ƒä¼šå‘`$POD_B_IP` å‘å‡ºè¯·æ±‚ã€‚


Configuring envoy could quickly become tricky due to its huge set of capabilities. However, the [official documentation](https://www.envoyproxy.io/docs/envoy/latest/) is a great place to start. It not only describes the configuration format but also highlights some best practices and explains some concepts. In particular, I suggest these two articles ["Life of a Request"](https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/life_of_A_request) and ["Service to service only"](https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/deployment_types/service_to_service) for a better understanding of the material.

ç”±äºå…¶åºå¤§çš„åŠŸèƒ½é›†ï¼Œé…ç½® envoy å¾ˆå¿«å°±ä¼šå˜å¾—æ£˜æ‰‹ã€‚ç„¶è€Œï¼Œ[å®˜æ–¹æ–‡æ¡£](https://www.envoyproxy.io/docs/envoy/latest/) æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚å®ƒä¸ä»…æè¿°äº†é…ç½®æ ¼å¼ï¼Œè¿˜çªå‡ºäº†ä¸€äº›æœ€ä½³å®è·µå¹¶è§£é‡Šäº†ä¸€äº›æ¦‚å¿µã€‚ç‰¹åˆ«æ¨èè¿™ä¸¤ç¯‡æ–‡ç«  ["Life of a Request"](https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/life_of_A_request)å’Œ["Service to service only"](https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/life_of_A_request)ä»¥æ›´å¥½åœ°ç†è§£ææ–™ã€‚

From a very high-level overview, Envoy could be seen as a bunch of pipelines. A pipeline starts from the listener and then connected through a set of filters to some number of clusters, where a cluster is just a logical group of network endpoints. Trying to be less abstract:

ä»ä¸€ä¸ªéå¸¸é«˜çº§çš„æ¦‚è¿°æ¥çœ‹ï¼ŒEnvoy å¯ä»¥è¢«è§†ä¸ºä¸€å †ç®¡é“ã€‚ç®¡é“ä»ä¾¦å¬å™¨å¼€å§‹ï¼Œç„¶åé€šè¿‡ä¸€ç»„è¿‡æ»¤å™¨è¿æ¥åˆ°ä¸€å®šæ•°é‡çš„é›†ç¾¤ï¼Œå…¶ä¸­é›†ç¾¤åªæ˜¯ç½‘ç»œç«¯ç‚¹çš„é€»è¾‘ç»„ã€‚å°½é‡ä¸é‚£ä¹ˆæŠ½è±¡ï¼š

```
# Ingress
listener 0.0.0.0:80
       |
http_connection_manager (filter)
       |
http_router (filter)
       |
local_service (cluster) [127.0.0.1:8000]

# Egress
listener 127.0.0.1:9001
       |
http_connection_manager (filter)
       |
http_router (filter)
       |
remote_service_b (cluster) [b.service:80]

```


_Click here to see envoy.yaml file._

_ç‚¹å‡»è¿™é‡ŒæŸ¥çœ‹ envoy.yaml æ–‡ä»¶ã€‚_

```yaml
static_resources:
listeners:
# Ingress
  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 80
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          codec_type: auto
          stat_prefix: ingress_http
          access_log:
          - name: envoy.access_loggers.file
            typed_config:
              "@type": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog
              path: /dev/stdout
          route_config:
            name: ingress_route
            virtual_hosts:
            - name: local_service
              domains:
              - "*"
              routes:
              - match:
                  prefix: "/"
                route:
                  cluster: local_service
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}
# Egress
  - address:
      socket_address:
        address: 127.0.0.1
        port_value: 9001
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          codec_type: auto
          stat_prefix: egress_http
          access_log:
          - name: envoy.access_loggers.file
            typed_config:
              "@type": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog
              path: /dev/stdout
          route_config:
            name: egress_route
            virtual_hosts:
            - name: remote_service_b
              domains:
              - "b.local.service"
              - "b.local.service:*"
              retry_policy:
                retry_on: 5xx
                num_retries: 2
              routes:
              - match:
                  prefix: "/"
                route:
                  cluster: remote_service_b
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}

clusters:
  - name: local_service
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    load_assignment:
      cluster_name: local_service
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: 127.0.0.1
                port_value: 8000
  - name: remote_service_b
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    load_assignment:
      cluster_name: remote_service_b
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: b.service
                port_value: 80

admin:
access_log_path: "/dev/stdout"
address:
    socket_address:
      # Beware: it's insecure to expose admin interface publicly
      address: 0.0.0.0
      port_value: 9901

```

[source on GitHub](https://github.com/iximiuz/envoy-playground/blob/master/basics/service-a/envoy.yaml)

Envoy is famous for its observability capabilities. It exposes various statistic information and luckily for us, it supports the prometheus metrics format out of the box. We can extend the prometheus scrape configs adding the following section:

Envoy ä»¥å…¶å¯è§‚å¯Ÿæ€§èƒ½åŠ›è€Œé—»åã€‚å®ƒå…¬å¼€äº†å„ç§ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¸è¿çš„æ˜¯ï¼Œå®ƒæ”¯æŒå¼€ç®±å³ç”¨çš„æ™®ç½—ç±³ä¿®æ–¯æŒ‡æ ‡æ ¼å¼ã€‚æˆ‘ä»¬å¯ä»¥æ‰©å±• prometheus æŠ“å–é…ç½®ï¼Œæ·»åŠ ä»¥ä¸‹éƒ¨åˆ†ï¼š

```bash
# prometheus/prometheus.yml

  - job_name: service-a-envoy
    scrape_interval: 1s
    metrics_path: /stats/prometheus
    static_configs:
      - targets: ['a.service:9901']

```


To build the envoy sidecar image we can run:

è¦æ„å»º envoy sidecar é•œåƒï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œï¼š

```bash
$ sudo podman build -t service-a-envoy -f service-a/Dockerfile-envoy

```


We don't need to rebuild service images since they've been made configurable via environment variables. However, we need to recreate the service _A_ to make it listening on the pod's `localhost:8000`.

æˆ‘ä»¬ä¸éœ€è¦é‡å»ºæœåŠ¡æ˜ åƒï¼Œå› ä¸ºå®ƒä»¬å·²ç»é€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œäº†é…ç½®ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åˆ›å»ºæœåŠ¡ _A_ ä»¥ä½¿å…¶ä¾¦å¬ pod çš„â€œlocalhost:8000â€ã€‚

```bash
# Stop existing container and remove pod
$ sudo podman kill service-a
$ sudo podman pod rm service-a-pod

$ sudo podman pod create --name service-a-pod \
   --add-host b.local.service:127.0.0.1 \
   --add-host b.service:$POD_B_IP \
   --publish 8080:80

$ sudo podman run --name service-a -d --rm --pod service-a-pod \
   -e SERVICE_HOST=127.0.0.1 \
   -e SERVICE_PORT=8000 \
   -e UPSTREAM_SERVICE=http://b.local.service:9001 \
service-a

$ sudo podman run --name service-a-envoy -d --rm --pod service-a-pod \
   -e ENVOY_UID=0 -e ENVOY_GID=0 service-a-envoy

$ POD_A_IP=$(sudo podman inspect -f "{{.NetworkSettings.IPAddress}}" \
    $(sudo podman pod inspect -f "{{.InfraContainerID}}" service-a-pod))

```


Let's see what happens if we pour some more traffic:

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬æ³¨å…¥æ›´å¤šæµé‡ä¼šå‘ç”Ÿä»€ä¹ˆï¼š

```bash
$ for _ in {1..1000};do curl --silent localhost:8080;done |sort |uniq -w 24 -c
>    1000
>       9 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1263663296
>     991 Service A: upstream responded with: Service B: Yay!nounce 1003014939

```


Hooray! ğŸ‰ Seems like the success rate of the service _A_ jumped from 80% to 99%! Well, that's great, but also as expected. The original probability to get HTTP 500 from the service _A_ was equal to the probability of the service _B_ to fail a request since the network conditions are kind of ideal here. But since the introduction of the envoy sidecar, service _A_ got a superpower of retries. The probability to fail 3 consequent requests with a 20% chance of a single attempt failure is `0.2 * 0.2 * 0.2 = 0.008`, i.e very close to 1%. Thus, we theoretically confirmed the observed 99% success rate.

ä¸‡å²ï¼ ğŸ‰ å¥½åƒæœåŠ¡_A_çš„æˆåŠŸç‡ä»80%è·³åˆ°äº†99%ï¼å—¯ï¼Œè¿™å¾ˆæ£’ï¼Œä½†ä¹Ÿç¬¦åˆé¢„æœŸã€‚ä»æœåŠ¡ _A_ è·å¾— HTTP 500 çš„åŸå§‹æ¦‚ç‡ç­‰äºæœåŠ¡ _B_ è¯·æ±‚å¤±è´¥çš„æ¦‚ç‡ï¼Œå› ä¸ºè¿™é‡Œçš„ç½‘ç»œæ¡ä»¶æœ‰ç‚¹ç†æƒ³ã€‚ä½†æ˜¯è‡ªä»å¼•å…¥äº† envoy sidecarï¼ŒæœåŠ¡ _A_ è·å¾—äº†é‡è¯•çš„è¶…çº§èƒ½åŠ›ã€‚å¤±è´¥ 3 ä¸ªåç»­è¯·æ±‚çš„æ¦‚ç‡ä¸º 20% çš„å•æ¬¡å°è¯•å¤±è´¥æ¦‚ç‡ä¸ºâ€œ0.2 * 0.2 * 0.2 = 0.008â€ï¼Œå³éå¸¸æ¥è¿‘ 1%ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯å®äº†è§‚å¯Ÿåˆ°çš„ 99% æˆåŠŸç‡ã€‚

Last but not least, let's check out the metrics. We will start from the familiar `service_a_requests_total` counter:

æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æŒ‡æ ‡ã€‚æˆ‘ä»¬å°†ä»ç†Ÿæ‚‰çš„â€œservice_a_requests_totalâ€è®¡æ•°å™¨å¼€å§‹ï¼š

![Service A - only 1% of outgoing requests failed](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-service-a-envoy.png)

_`service_a_requests_total`_

Well, seems like service _A_ again got 1000 requests, but this time it failed only a tiny fraction of it. What's up with service _B_?

å¥½å§ï¼Œä¼¼ä¹ service _A_ å†æ¬¡æ”¶åˆ°äº† 1000 ä¸ªè¯·æ±‚ï¼Œä½†è¿™æ¬¡å®ƒåªå¤±è´¥äº†å…¶ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚æœåŠ¡_B_æ€ä¹ˆäº†ï¼Ÿ

![Service B - still 20% of incoming requests failed](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-service-b-envoy.png)

_`service_b_requests_total`_



Here we definitely can see the change. Instead of the original 1000, this time service _B_ got about 1250 requests in total. However, only about 1000 have been served successfully.

åœ¨è¿™é‡Œæˆ‘ä»¬ç»å¯¹å¯ä»¥çœ‹åˆ°å˜åŒ–ã€‚è¿™æ¬¡æœåŠ¡ _B_ æ€»å…±æ”¶åˆ°äº†å¤§çº¦ 1250 ä¸ªè¯·æ±‚ï¼Œè€Œä¸æ˜¯åŸæ¥çš„ 1000 ä¸ªã€‚ä½†æ˜¯ï¼Œåªæœ‰å¤§çº¦ 1000 ä¸ªæˆåŠŸé€è¾¾ã€‚

What can the envoy sidecar tell us?

ç‰¹ä½¿è¾¹è½¦å¯ä»¥å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ

![Envoy local cluster stats](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-envoy-local-service.png)

_`envoy_cluster_upstream_rq{envoy_cluster_name="local_service"}`_



![Envoy remote cluster stats](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-envoy-remote-service.png)

_`envoy_cluster_upstream_rq{envoy_cluster_name="remote_service_b"}`_



While both `local_service` and `remote_service_b` clusters don't shed much light on the actual number of retries that were made, there is another metric we can check:

è™½ç„¶â€œlocal_serviceâ€å’Œâ€œremote_service_bâ€é›†ç¾¤éƒ½æ²¡æœ‰è¯´æ˜å®é™…é‡è¯•æ¬¡æ•°ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å¦ä¸€ä¸ªæŒ‡æ ‡ï¼š

![Envoy retries stats](http://iximiuz.com/service-proxy-pod-sidecar-oh-my/prom-envoy-retries.png)

_`envoy_cluster_retry_upstream_rq{envoy_cluster_name="remote_service_b"}`_



Perfect, we managed to confirm that all those ~250 extra requests the service _B_ received are actually retries originated by the envoy sidecar!

å®Œç¾ï¼Œæˆ‘ä»¬è®¾æ³•ç¡®è®¤æœåŠ¡ _B_ æ”¶åˆ°çš„æ‰€æœ‰å¤§çº¦ 250 ä¸ªé¢å¤–è¯·æ±‚å®é™…ä¸Šæ˜¯ç”±ç‰¹ä½¿ sidecar å‘èµ·çš„é‡è¯•ï¼

## Instead of conclusion 

## è€Œä¸æ˜¯ç»“è®º

I hope you enjoyed playing with all these pods and sidecars as much as I did. It's always beneficial to build such demos from time to time because often the amount of insights you're going to get while working on it is underestimated. So, I encourage everyone to get the hands dirty and share your findings! See you next time!

æˆ‘å¸Œæœ›ä½ å’Œæˆ‘ä¸€æ ·å–œæ¬¢ç©æ‰€æœ‰è¿™äº›åŠèˆ±å’Œè¾¹è½¦ã€‚æ—¶ä¸æ—¶åœ°æ„å»ºè¿™æ ·çš„æ¼”ç¤ºæ€»æ˜¯æœ‰ç›Šçš„ï¼Œå› ä¸ºé€šå¸¸ä¼šä½ä¼°æ‚¨åœ¨å·¥ä½œæ—¶è·å¾—çš„æ´å¯ŸåŠ›ã€‚å› æ­¤ï¼Œæˆ‘é¼“åŠ±æ¯ä¸ªäººéƒ½åŠ¨æ‰‹å¹¶åˆ†äº«æ‚¨çš„å‘ç°ï¼ä¸‹æ¬¡è§ï¼

Make code, not war!

ç¼–å†™ä»£ç ï¼Œè€Œä¸æ˜¯æˆ˜äº‰ï¼

### Other posts you may like

### æ‚¨å¯èƒ½å–œæ¬¢çš„å…¶ä»–å¸–å­

- [Service Discovery in Kubernetes - Combining the Best of Two Worlds](http://iximiuz.com/en/posts/service-discovery-in-kubernetes/)
- [Traefik: canary deployments with weighted load balancing](http://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/)

- [Kubernetes ä¸­çš„æœåŠ¡å‘ç° - ç»“åˆä¸¤å…¨å…¶ç¾](http://iximiuz.com/en/posts/service-discovery-in-kubernetes/)
- [Traefikï¼šå…·æœ‰åŠ æƒè´Ÿè½½å¹³è¡¡çš„é‡‘ä¸é›€éƒ¨ç½²](http://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/)

[envoy,](javascript: void 0) [microservices,](javascript: void 0) [architecture](javascript: void 0)

[envoy,](javascript: void 0) [å¾®æœåŠ¡,](javascript: void 0) [æ¶æ„](javascript: void 0)

#### Written by Ivan Velichko

#### ç”±ä¼Šä¸‡Â·ç»´åˆ©å¥‡ç§‘ (Ivan Velichko) æ’°å†™

_Follow me on twitter [@iximiuz](https://twitter.com/iximiuz)_

_åœ¨æ¨ç‰¹ä¸Šå…³æ³¨æˆ‘ [@iximiuz](https://twitter.com/iximiuz)_

Liked this article? Let it be the beginning of a great friendship. Leave your email so I could notify you about new articles or any other interesting happenings around the topics of this blog. No spam whatsoever, I promise!

å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿè®©å®ƒæˆä¸ºä¸€æ®µä¼Ÿå¤§å‹è°Šçš„å¼€å§‹ã€‚ç•™ä¸‹æ‚¨çš„ç”µå­é‚®ä»¶ï¼Œä»¥ä¾¿æˆ‘å¯ä»¥é€šçŸ¥æ‚¨æœ‰å…³æ­¤åšå®¢ä¸»é¢˜çš„æ–°æ–‡ç« æˆ–ä»»ä½•å…¶ä»–æœ‰è¶£çš„äº‹ä»¶ã€‚æ²¡æœ‰ä»»ä½•åƒåœ¾é‚®ä»¶ï¼Œæˆ‘ä¿è¯ï¼

Copyright Ivan Velichko Â© 2021Â FeedÂ [Atom](http://iximiuz.com/feed.atom)[RSS](http://iximiuz.com/feed.rss) 

ç‰ˆæƒæ‰€æœ‰ Ivan Velichko Â© 2021 Feed [Atom](http://iximiuz.com/feed.atom)[RSS](http://iximiuz.com/feed.rss)

